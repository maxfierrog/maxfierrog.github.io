<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Max Fierro</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Max Fierro</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 13 Oct 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    
    
    
    
    <item>
      <title>Poem 9. &#34;Reflexiones&#34;</title>
      <link>http://localhost:1313/poem-9.-reflexiones/</link>
      <pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-9.-reflexiones/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;soledades entre columnas de piedra
vacíos entre las personas del mundo
las cumbres de la tierra, sus árboles 
venas de fuego que mueren en el mar
un silencio eterno en la cima del cielo
pensamientos perdidos bajo palabras
instantes que tocan la piel del futuro
otras manos, sentidas por última vez
las nubes se desmoronan y son lluvia
espacios vivos en volúmenes sin nada
bosques de noche, un verde sin ocaso 
la luz de una estrella que ya no existe
verdades absolutas entre los detalles
las miradas y su amor incandescente
desvíos de vida, la soledad inminente
ley del sol en el desierto, y un charco
libre misericordia de un océano cruel
el fruto sanguíneo de labios efímeros
sueño fugaz, los ríos ya desembocan
las hojas mojadas se surten de brillo
el suelo se nutre y las raíces respiran
la gente vive y se abraza con fuerza
ceñido, todo está aquí y es completo
de día se siente en todos lados calor
ya con compañía, fosforece la noche
nuestra piel está cubierta de energía 
somos indistinguibles entre nosotros
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Poem 8. &#34;Cáliz&#34;</title>
      <link>http://localhost:1313/poem-8.-c%C3%A1liz/</link>
      <pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-8.-c%C3%A1liz/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;te recuerdo en mi sangre,
calidez que fluye por mi cuello
  derriba mi cabeza;
te recuerdo en mis lágrimas,
  cierro mis ojos, y te veo
  entre formas familiares;
siento que te abrazo,
  existes entre mis brazos
  abrazándose a sí mismos,
      siento 
    que te cubro de lágrimas,
    siento la calidez 
      de tu sangre;
en mi mente
  te recojo y te defiendo 
    del tiempo,
  te junto en un cáliz
    hecho con mis manos,
    que poco a poco 
  te derrama:
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Poem 7. &#34;Absoluto&#34;</title>
      <link>http://localhost:1313/poem-7.-absoluto/</link>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-7.-absoluto/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;desde ya distantes astros
por un volumen transparente 
    se derrama 
  un cuerpo que mi visión sostiene;
mi mirada atraviesa el cielo 
    y se filtra por un medio invisible;
se somete a su estructura,
  y se concreta,
    se cristaliza:

    si lo busco,
todo existe en ese cuerpo;
  cada cosa
    es una de sus caras,
  cada perspectiva
    es una de sus esquinas,
  y dentro de su totalidad
se encuentra a si misma
    mi mirada:

ser que se yergue 
  entre patrones que yo elijo,
  tu cara lo es todo
    y existe en la nada,
   tu mirada me ve 
  y me concreta,
    me cristaliza:
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Perspectives into Tensors, Signals, and Kernel Methods</title>
      <link>http://localhost:1313/perspectives-into-tensors-signals-and-kernel-methods/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/perspectives-into-tensors-signals-and-kernel-methods/</guid>
      <description>&lt;aside id=&#34;toc&#34;&gt;
    &lt;details&gt;
        &lt;summary&gt;&amp;nbsp;&lt;strong&gt; Table of contents&lt;/strong&gt;&lt;/summary&gt;
        &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#vector-spaces&#34;&gt;Vector Spaces&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#tensor-spaces&#34;&gt;Tensor Spaces&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#signals-and-systems&#34;&gt;Signals and Systems&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#kernel-methods&#34;&gt;Kernel Methods&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
    &lt;/details&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Linear algebra, signal processing, and machine learning methods are (one of many groups of) topics that enjoy beautiful relationships enclosed in a dense shell of mathematics. Within the shell, one finds a kernel of surprisingly diverse perspectives which are simply a pleasure to entertain. This article hopes to give the reader a romantic and thematic glimpse of the truths in this particular group of topics.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;I follow an essay-like structure including an introduction, body, and conclusion. The introduction sets the stage building from &amp;ldquo;class-style&amp;rdquo; knowledge, assuming a first course in linear algebra, signal processing, and machine learning. For readers lacking this background, I leave pointers to free resources.&lt;/p&gt;
&lt;p&gt;The introduction arrives at the &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_map&#34;&gt;linear operator&lt;/a&gt; perspective of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multidimensional_system&#34;&gt;systems of many dimensions&lt;/a&gt;, defining system properties like &lt;a href=&#34;https://en.wikipedia.org/wiki/Time-invariant_system&#34;&gt;time-invariance&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Causal_system&#34;&gt;causality&lt;/a&gt; in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor&#34;&gt;tensor&lt;/a&gt; representations. This results in the marriage of many linear algebra and signal processing concepts. These perspectives then help the body, where I present a cross-cutting perspective of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolution&#34;&gt;convolution kernel&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#:~:text=then%20called%20the-,reproducing%20kernel,-%2C%20and%20it%20reproduces&#34;&gt;reproducing kernel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the conclusion provides some subjective thematization to the concepts emphasized in the rest of the piece, summarizing mathematical details and indulging in a little bit of sensationalism. An attempt is made of providing pointers to further reading on adjacent concepts.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;2.1. Clarification&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The word &amp;ldquo;kernel&amp;rdquo; is criminally polysemous in mathematics, and it will be used a lot in this piece. I mostly use the word under two semantics: The convolution kernel from signal processing and the reproducing kernel from machine learning. All other instances of the word should simply refer to its english meaning.&lt;/p&gt;
&lt;p&gt;However, a primary objective of this piece is to reach a perspective of the convolution and reproducing kernels that allows them to be seen through a common lense. So an acute reader may interpret this as an explanation of why they are both called &amp;ldquo;kernel,&amp;rdquo; assuming that they were both named that way because they represent the same kind of mathematical object in some profound way.&lt;/p&gt;
&lt;p&gt;What is interesting is that they both received their names for a superficial reason &amp;ndash; because the symbols that represent them show up inside other symbols. One could imagine that people started calling them &amp;ldquo;kernel&amp;rdquo; independently just to avoid saying the phrase &amp;ldquo;that term in in the middle&amp;rdquo; while pointing at a blackboard.&lt;/p&gt;
&lt;p&gt;As such, the connecting view of the convolution and reproducing kernels that we will work towards only applies to these two kernels (convolution and reproducing). If these were the only two kernels in mathematics, maybe one could now say that they are named the same for a profound reason. But there are numerous other kinds of kernels for which the constructions in this piece simply do not apply.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This section provides not much more than a definition-based refresher on select topics from first courses in linear algebra, signal processing, and machine learning. For readers in need of comprehensive review or first-time coverage, I leave these free resources on said topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kernel methods.&lt;/strong&gt; &lt;a href=&#34;https://cs.nyu.edu/~mohri/mlbook/&#34;&gt;&lt;em&gt;Foundations of Machine Learning&lt;/em&gt;&lt;/a&gt; by Mohri, Rostamizadeh, and Talwalkar.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Signals and Systems.&lt;/strong&gt; &lt;a href=&#34;https://ss2-2e.eecs.umich.edu/&#34;&gt;&lt;em&gt;Signals &amp;amp; Systems: Theory and Applications&lt;/em&gt;&lt;/a&gt; by Ulaby and Yagle.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear algebra.&lt;/strong&gt; &lt;a href=&#34;https://linear.axler.net/&#34;&gt;&lt;em&gt;Linear Algebra Done Right&lt;/em&gt;&lt;/a&gt; by Axler.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vector-spaces&#34;&gt;Vector Spaces&lt;/h3&gt;
&lt;p&gt;In what is nowadays close to being a canon of linear algebra education, Sheldon Axler opens with the statement below to set the stage for the rest of &lt;em&gt;Linear Algebra Done Right&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Linear algebra is the study of linear maps on finite-dimensional vector spaces.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Here, the restriction of vector spaces to the finite-dimensional case was one the most mathematically respectful ways to negotiate generality with practical pedagogy. However, the spirit of linear algebra is alive way beyond the finite-dimensional case.&lt;/p&gt;
&lt;h4 id=&#34;hamel-bases&#34;&gt;Hamel Bases&lt;/h4&gt;
&lt;p&gt;Most engineers are familiar with the concept of a (Hamel) basis of a vector space. If we have a vector space $V$ over a field $\mathbb{F}$ and a Hamel basis $\mathcal{B}$, then &amp;ldquo;$\mathcal{B}$ spans $V$&amp;rdquo; translates to&lt;/p&gt;
$$
\begin{equation}
    \forall v \in V, \; v = \sum_{i \, = \, 0}^{k} c_i b_i \;\; \text{s.t.} \;\; c_i \in \mathbb{F}, \, b_i \in \mathcal{B}, \, k \in \mathbb{N}.
\end{equation}
$$&lt;p&gt;Importantly, for $B$ to be a Hamel basis, the sum in $(1)$ must have finite terms. Note that this is allowed even in cases where $\mathcal{B}$ is infinite (or in other words, where $V$ is infinite-dimensional), as one does not necessarily assign nonzero coefficients $c_i$ to each element of $\mathcal{B}$.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.1. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The vector space of polynomials (of finite terms) with coefficients in a field $\mathbb{F}$,&lt;/p&gt;
$$
\mathbb{F}[x] = \left\{ \sum_{i \, = \, 0}^n a_i x^i \;\Big|\; n \in \mathbb{N},\ a_i \in \mathbb{F} \right\},
$$&lt;p&gt;has the infinite basis $\mathcal{B}_{\mathbb{F}[x]} = \{1, x, x^2, x^3, \dots \}$. Each of its elements, however, is the linear combination of a finite number of basis elements. For example, the polynomial&lt;/p&gt;
$$ 
p(x) = 3 + 4x^2 + x^3
$$&lt;p&gt;can be expressed as a (finite) linear combination of basis elements,&lt;/p&gt;
$$
p(x) = 3
\begin{bmatrix}
 1 \\
 0 \\
 0 \\
 0 \\
 \vdots
\end{bmatrix} + 4  
\begin{bmatrix}
 0 \\
 0 \\
 1 \\
 0 \\
 \vdots
\end{bmatrix} + 1 
\begin{bmatrix}
 0 \\
 0 \\
 0 \\
 1 \\
 \vdots
\end{bmatrix}.
$$&lt;p&gt;Here we imposed a (canonical) representation such that, for example, $x^2 = \left[ 0, \, 0, \, 1, \, 0, \, {\dots} \right]^\top$. We see that, despite each basis vector being infinite-dimensional, all polynomials are determined by a finite number of them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;vera-molnar-molndrian-1974.png&#34;
         alt=&#34;Vera Molnár, &amp;lsquo;Molndrian&amp;rsquo; (1974)&#34; width=&#34;512&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Vera Molnár, &amp;lsquo;Molndrian&amp;rsquo; (1974)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h4 id=&#34;schauder-bases&#34;&gt;Schauder Bases&lt;/h4&gt;
&lt;p&gt;Interpreting Axler strictly, $\mathbb{F}[x]$ is already beyond linear algebra because it is of &lt;a href=&#34;https://en.wikipedia.org/wiki/Semi-infinite&#34;&gt;semi-infinite&lt;/a&gt; dimension. But definitionally, it is a perfectly valid vector space. Just as finite dimensionality is not necessary in order to access the theorems of linear algebra, having a countable Hamel basis is also not necessary; all vector spaces do have a Hamel basis&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, but not all of them have a countable one.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.2. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Countable bases are desireable not for being countable per se, but rather that, in most cases where a vector space does not have a countable Hamel basis, the uncountable Hamel basis is unconstructive and unutterable. Put another way, the most useful fact about an uncountable Hamel basis, in many cases, is that it exists.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For some vector spaces that do not have a countable Hamel basis, one can relax the definition of a basis itself to obtain one that is countable. Specifically, we redefine the phrase &amp;ldquo;$\mathcal{B}$ spans $V$&amp;rdquo; to&lt;/p&gt;
$$
\begin{equation}
    \forall v \in V, \; v = \lim_{ n \to \infty } \, \sum_{i \, = \, 0}^{n} c_i b_i \;\; \text{s.t.} \;\; c_i \in \mathbb{F}, \, b_i \in \mathcal{B}.
\end{equation}
$$&lt;p&gt;If the above is true for a vector space $V$ over $\mathbb{F}$, then $\mathcal{B}$ is a Schauder basis of said space. The critical difference to a Hamel basis is of course the generality afforded by the possiblity of infinite terms for the sum in $(2)$, giving us a new countably-infinite flavor of linear combination.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.3. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The vector space of square-summable sequences of real numbers,&lt;/p&gt;
$$
\ell^2 = \left\{ (x_1, x_2, x_3, \dots) \;:\; \sum_{n \, = \, 1}^\infty |x_n|^2 &lt; \infty \right\},
$$&lt;p&gt;has no Hamel basis because, no matter how you define one, you can come up with an element of $\ell^2$ which requires a decomposition into an infinite number of basis elements (which is not allowed). However, it does have the countably-infinite Schauder basis&lt;/p&gt;
$$
\mathcal{B}_{\ell^2} = \left\{ 
\left(
 1, \,
 0, \,
 0, \,
 \dots
\right), \,
\left(
 0, \,
 1, \,
 0, \,
 \dots
\right), \,
\left(
 0, \,
 0, \,
 1, \,
 \dots
\right), \,
{\dots}
\right\}.
$$&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;taxonomy-of-spaces&#34;&gt;Taxonomy of Spaces&lt;/h4&gt;
&lt;p&gt;Hidden in $(2)$ is the requirement that all such sums over basis elements converge. But the definition of a vector space does not include any operation that computes the &amp;ldquo;closeness&amp;rdquo; of two vectors, so additional concepts are needed to make sense of convergence. Abstractly, one needs to equip the vector space of interest with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Topological_space#topology&#34;&gt;topology&lt;/a&gt;. The way of doing so that we will consider is by assuming a &lt;a href=&#34;https://en.wikipedia.org/wiki/Norm_(mathematics)&#34;&gt;norm&lt;/a&gt; over the space, such that we can concretely declare the definition of an infinite series&lt;/p&gt;
$$
\begin{equation}
    \lim_{n \to \infty} \left\| x - \sum_{k \, = \, 1}^n a_k \right\| = 0 \iff x = \sum_{k \, = \, 1}^{\infty} a_k.
\end{equation}
$$&lt;p&gt;Vector spaces that have a norm are called normed vector spaces. If a normed vector space is &lt;a href=&#34;https://en.wikipedia.org/wiki/Complete_metric_space&#34;&gt;complete&lt;/a&gt; under the norm-induced &lt;a href=&#34;https://en.wikipedia.org/wiki/Metric_space&#34;&gt;metric&lt;/a&gt; $d : (x_1, \, x_2) \mapsto ||x_1 - x_2||$, then it is also called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Banach_space&#34;&gt;Banach space&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.4. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Banach spaces do not necessarily have a Schauder basis. The reason for this is technical and out of scope. Additionally, not all normed vector spaces that have a Schauder basis are Banach spaces, because they may not be complete. But for the remainder of this piece, completeness can be comfortably assumed. Indeed, most of the time anyone talks about a Schauder basis in a practical context, it spans a complete space (such as $\ell^2$).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;If a Banach space is also equipped with an &lt;a href=&#34;https://en.wikipedia.org/wiki/Inner_product_space&#34;&gt;inner product&lt;/a&gt; in such a way that $\langle x, \, x \rangle = ||x||^2$, then it is also called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Inner_product_space&#34;&gt;Hilbert space&lt;/a&gt;. With the understanding that a vector space with an inner product defined is called an inner-product space, we arrive at the following:&lt;/p&gt;
$$
\begin{equation}
\begin{aligned}
\text{Vector} &amp; \text{ spaces} \\[0.2em]
&amp;\supset
\Bigg\{
  \begin{aligned}
  &amp; \text{Normed vector spaces} \;\supset\; \text{Banach spaces} \\
  &amp; \text{Inner-product vector spaces}
  \end{aligned}\\[1.2em]
&amp;\supset \text{Hilbert spaces} = (\text{Banach spaces} \, \cap \, \text{Inner-product spaces}).
\end{aligned}
\end{equation}
$$&lt;h4 id=&#34;continuous-bases&#34;&gt;Continuous Bases&lt;/h4&gt;
&lt;p&gt;The last thing we will consider are vector spaces of uncountably-infinite dimension. So far, we have been comfortable in using syntax such as &amp;ldquo;$x = [ 1, \, 2, \, \ldots ]^\top$&amp;rdquo; to refer to vectors of countably-infinite dimension. This will no longer be possible with uncountably-infinite dimensions, so we must revisit our notation.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.5. Reminder&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;A vector $v$ is an abstract object, and is independent of a choice of basis. To write $v$ as a tuple $[c_1, \dots, c_n]$, one must choose a basis ${b_1, \, \dots, \, b_n}$ and expand (in the finite-dimensional case)&lt;/p&gt;
$$
v = \sum_{i \, = \, 1}^n c_i b_i.
$$&lt;p&gt;Hence, $[1, \, 2, \, 3]^\top$ means “the coefficients of $v$ in this basis.”
Changing the basis changes the coefficients, but not the vector itself. This also applies to infinite-dimensional cases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Observing that, once a basis is chosen, a vector in a vector space $V$ over a field $\mathbb{F}$ is determined by the coefficients in its representation as a linear combination of basis vectors, we can introduce a new type of linear combination to evolve the translation of &amp;ldquo;$\mathcal{B}$ spans $V$&amp;rdquo; to involve a &lt;a href=&#34;https://en.wikipedia.org/wiki/Lebesgue_integral&#34;&gt;Lebesgue integral&lt;/a&gt;,&lt;/p&gt;
$$
\begin{equation}
    \forall v \in V, \; v = \int_{\mathcal{\Omega}} c(\omega) b(\omega) \, d\mu(\omega) \;\; \text{s.t.} \;\; c : \Omega \to \mathbb{F}, \, b : \Omega \to \mathcal{B},
\end{equation}
$$&lt;p&gt;where a measure ${\mu}$ over $\Omega$ is provided. Here, the index $\omega$ intuitively replaces the index $i$ from $(2)$, where there is a map $c$ &amp;ldquo;choosing&amp;rdquo; a coefficient $c(\omega)$ and a map $b$ &amp;ldquo;choosing&amp;rdquo; a basis element $b(\omega)$ per index $\omega$. This way, once a map $b : \Omega \to \mathcal{B}$ and a measure $\mu$ over $\Omega$ have been agreed upon, a vector can still be represented by &amp;ldquo;its coefficients,&amp;rdquo; which is just the map $c$.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.6. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;We can consider the Hilbert space of all square-integrable functions&lt;/p&gt;
$$
L^2(\mathbb{R}) = 
\Bigl\{\, f:\mathbb{R} \to \mathbb{C} \; \big| \; \int_{-\infty}^{\infty} |f(t)|^2 \, dt &lt; \infty \,\Bigr\},
$$&lt;p&gt;with the inner product $\langle f_1(t), \, f_2(t) \rangle = \int_{\mathbb{R}} f_1(t) \overline{f_2(t)} \, dt$. A natural choice of &amp;ldquo;continuous basis&amp;rdquo; for $L^2(\mathbb{R})$ is the family of complex exponentials indexed by frequency, $b_\omega(t) = e^{2 \pi i \omega t}$ with $\omega \in \mathbb{R}$. For all $f \in L^2(\mathbb{R})$,&lt;/p&gt;
$$
f(t) = \int_{\mathbb{R}} c(\omega)b(\omega) \, d\mu(\omega) = \int_{-\infty}^{\infty} c(\omega)\, b_\omega(t)\, d\omega.
$$&lt;p&gt;The measure $d\omega$ is the typical Lebesgue measure on $\mathbb{R}$. Here, we see that any $f \in L^2(\mathbb{R})$ can be expressed as a continuous linear combination of basis elements in the form $e^{2 \pi i \omega t}$ which, to reiterate, are other functions parameterized by $t$ and indexed by $\omega \in \mathbb{R}$. In this case, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_transform&#34;&gt;Fourier transform&lt;/a&gt; of $f(t)$ provides $c(\omega)$:&lt;/p&gt;
$$
c(\omega) = \int_{-\infty}^{\infty} f(t) \, \overline{b_\omega(t)} \, dt
= \int_{-\infty}^{\infty} f(t)\, e^{-2 \pi i \omega t} \, dt.
$$&lt;p&gt;In a finite-dimensional case, we would compute $\langle v, \, e_n \rangle$ to observe the &amp;ldquo;contribution&amp;rdquo; of the basis element $e_n$ in the vector $v$, obtaining the coefficient it would be assigned in its decomposition as a linear combination of basis elements. The Fourier transform does exactly the same thing per $\omega$, where $v = f(t)$ and $e_n = b_\omega(t)$:&lt;/p&gt;
$$
c(\omega) = \langle f(t), \, b_\omega(t) \rangle.
$$&lt;p&gt;It is not difficult to show algebraically that the Fourier transform $\mathcal{F} : L^2(\mathbb{R}) \to L^2(\mathbb{R})$ is a linear operator over this Hilbert space. (So if it were countably-infinite dimensional, it would have a matrix representation.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;overview-1&#34;&gt;Overview&lt;/h4&gt;
&lt;p&gt;We have expanded a finite-dimensional view of vector spaces to potentially allow those with countably- or uncountably-infinite dimensions. To do so, we had to slowly relax our concept of a linear combination from $(1)$ (which already spanned certain infinite-dimensional spaces like $\mathbb{F}[x]$), to $(2)$ (which was able to span a more countably-infinite-dimensional spaces like $\ell^2$), and finally $(5)$ (which can span uncountably-infinite dimensional spaces like $L^2(\mathbb{R})$). We have also encountered the conepts of Banach and Hilbert spaces.&lt;/p&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;david-hilbert.jpg&#34;
         alt=&#34;David Hilbert (January 23, 1862 – February 14, 1943)&#34; width=&#34;256&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;David Hilbert (January 23, 1862 – February 14, 1943)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.7. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;In fact, $(1)$ is a special case of $(2)$, which is a special case of $(5)$. Therefore, it is truly a relaxation of the linear combination; at no point did we lock ourselves out of any vector spaces we could already span. Namely, in the case of $(5)$ and $(2)$ for a given vector space $V$,&lt;/p&gt;
$$
\Omega = \mathbb{N} \iff \forall v \in V, \; v = \int_{\Omega} c(\omega)b(\omega) \, d{\mu}(\omega) = \sum_{i \, = \, 0}^{\infty} c_i b_i.
$$&lt;p&gt;In the case of $(2)$ and $(5)$, when all vectors in $V$ are a linear combination of a finite number of basis elements (as is the case for $\mathbb{F}[x]$),&lt;/p&gt;
$$
\forall v \in V, \; v = \sum_{i \, = \, 0}^{\infty} c_i b_i = \sum_{i \, = \, 0}^{k} c_i b_i \;\; \text{s.t.} \;\; k \in \mathbb{N}.
$$&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;tensor-spaces&#34;&gt;Tensor Spaces&lt;/h3&gt;
&lt;p&gt;Let us revisit the opening scene of &lt;em&gt;Linear Algebra Done Right&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Linear algebra is the study of linear maps on finite-dimensional vector spaces.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Another thing to note (apart from the restriction to finite dimensions) is the lack of matrices and vectors in this description. This section will live up to this witholding; we will first look for a linear-map-centric view of the objects in linear algebra and, afterwards, gain an understanding of tensor spaces.&lt;/p&gt;
&lt;h4 id=&#34;vectors-and-matrices&#34;&gt;Vectors and Matrices&lt;/h4&gt;
&lt;p&gt;A regrettable aspect of a typical introduction to linear algebra is the marriage of syntax to abstract objects. Most of us were told in a first impression that a vector $v$ and a matrix $M$ may look something like this:&lt;/p&gt;
$$
\begin{equation}
v = 
\begin{bmatrix}
1 \\
2
\end{bmatrix}, \quad 
M = 
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}.
\end{equation}
$$&lt;p&gt;I assert that $v$ and $M$ are &lt;em&gt;both&lt;/em&gt; matrices, each of which simultaneously identifies a vector and linear map. For richer support, I will establish three resources below and explain their relationship afterward.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.8. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The set of linear maps from a vector space $V$ over the field $\mathbb{F}$ to another vector space $W$ (over the same field) forms a vector space over $\mathbb{F}$. That is,&lt;/p&gt;
$$
\mathcal{L}(V, W) = \left\{ \, T : V \to W \; | \; T \text{ is linear} \right\}
$$&lt;p&gt;is a vector space over $\mathbb{F}$. We denote the case of linear operators on $V$, which is simply $\mathcal{L}(V, V)$, as $\mathcal{L}(V)$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.9. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;There is a bijection between $\mathcal{L}(V, W)$ and $\mathbb{F}^{(\dim V) \times (\dim W)}$, where $V$ and $W$ are vector spaces on the same field $\mathbb{F}$ and are finite-dimensional. In other words, for each linear map $T$ from a vector space of dimension $n$ to another of dimension $m$, there is exactly one $m$-by-$n$ matrix with entries in $\mathbb{F}$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.10. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;A vector $v$ in a space $V$ over $\mathbb{F}$ can be regarded as a linear map from the space $\mathbb{F}^1$ into $V$, via&lt;/p&gt;
$$
\psi_v : \mathbb{F}^1 \to V, \;\; \psi_v(\lambda) = v \lambda.
$$&lt;p&gt;When a basis for $V$ is fixed, the map $\psi_v$ is represented by an $n \times 1$ matrix (as an instance of theorem 3.9). This matrix is the familiar column of &amp;ldquo;coordinates&amp;rdquo; of $v$. In particular, observe that scalar multiplication can be seen as matrix multiplication with a single-dimensional vector,&lt;/p&gt;
$$
\psi_v(\lambda) = 
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{(\dim V)}
\end{bmatrix}
\begin{bmatrix}
\lambda_1
\end{bmatrix} =
\begin{bmatrix}
\lambda_1 v_1 \\
\lambda_1 v_2 \\
\vdots \\
\lambda_1 v_{(\dim V)}
\end{bmatrix} = 
\lambda_1 v.
$$&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This sets up a clear organization of vectors, matrices, and linear maps. With fixed bases, we see that the set of linear maps from $V$ to $W$ is already a vector space (via 3.8) and also that all vectors in a space $U$ over a field $\mathbb{F}$ are canonically a linear map from $\mathbb{F}^1$ into $U$ (via 3.10), hence&lt;/p&gt;
$$
\text{Linear maps} \cong \text{Vectors}.
$$&lt;p&gt;Also, we see that there is a one-to-one correspondence between linear maps and matrices (via 3.9) by the bijection $\mathcal{L}(V, W) \leftrightarrow \mathbb{F}^{(\dim W) \times (\dim V)}$, so in finite dimensions ($\cong^!$)&lt;/p&gt;
$$
\text{Matrices} \cong^! \text{Linear maps}.
$$&lt;p&gt;Indeed, a vector can be turned into a matrix solely through its identifiability as a linear map, turning the linear map into the central object of our understanding of linear algebra. In summary,&lt;/p&gt;
$$
\begin{equation}
    \text{Matrices} \cong^! \text{Linear maps} \cong \text{Vectors}.
\end{equation}
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.11. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;We have started to use the symbol $\cong$. In this context, stating $A \cong B$ implies that there is a linear &lt;a href=&#34;https://en.wikipedia.org/wiki/Isomorphism&#34;&gt;isomorphism&lt;/a&gt; between $A$ and $B$. Exactly, this means that there exists some linear map $\phi : A \to B$ that is a bijection.&lt;/p&gt;
&lt;p&gt;This is complete as a definition of the symbol $\cong$. However, its use in the rest of this piece will often reference a choice of $\phi$ that is &lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_map&#34;&gt;canonical&lt;/a&gt;. This means that if you see $A \cong B$, there is probably an &amp;ldquo;standard&amp;rdquo; way to obtain a $b \in B$ from one unique $a \in A$ (and vice versa) &amp;ndash; here, we can informally say that if $\phi(a) = b$ then $a$ &amp;ldquo;is&amp;rdquo; $b$, but it is more precise to say that $a$ &amp;ldquo;identifies&amp;rdquo; $b$. Oftentimes, $\phi$ will not be made explicit.&lt;/p&gt;
&lt;p&gt;For example, choosing a basis $\mathcal{B}$ for a space $V$ gives the canonical isomorphism $\mathbb{F}^{(\dim V)\times(\dim V)} \cong \mathcal{L}(V)$. In this case, it makes sense to say that &amp;ldquo;a matrix is a linear transformation.&amp;rdquo; But without a choice of basis there is no &amp;ldquo;standard&amp;rdquo; bijection $\Phi_\mathcal{B} : \mathcal{L}(V) \to \mathbb{F}^{(\dim V) \times (\dim V)}$ (no way to bijectively identify maps from matrices).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;vera-molnar-untitled-square-1974.png&#34;
         alt=&#34;Vera Molnár, Untitled (1974)&#34; width=&#34;512&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Vera Molnár, Untitled (1974)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h4 id=&#34;vector-translation&#34;&gt;Vector Translation&lt;/h4&gt;
&lt;p&gt;The statement of 3.10 is neuanced. Consider the case of a linear map $T \in \mathcal{L}(V, W)$ represented by a matrix $M$ (under fixed bases). According to 3.8 that map is a vector, but according to 3.10 it is identified by some &lt;em&gt;other&lt;/em&gt; linear map $\psi_v$, which is identified by some &lt;em&gt;other&lt;/em&gt; column matrix $M^\prime$,&lt;/p&gt;
$$
M 
\xrightarrow{\displaystyle\Phi_\mathcal{B}^{-1}} T 
\xrightarrow{\displaystyle\Psi_{\mathcal{L}(V, W)}} \psi_v 
\xrightarrow{\displaystyle\Phi_\mathcal{B}} M^\prime.
$$&lt;p&gt;In this diagram, the basis-conscious bijection $\Phi_\mathcal{B} : \mathcal{L}(V, W) \to \mathbb{F}^{(\dim W) \times (\dim V)}$ lives up to 3.9, but we silently adopted the canonical translation $\Psi_U$ of arbitrary vectors into linear maps in 3.10,&lt;/p&gt;
$$
\Psi_U : U \to \mathcal{L}(\mathbb{F}^1, U) \;\; \text{s.t.} \;\; \Psi_U(u) = \psi_u \; \forall \, u \in U.
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.12. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;This makes sense when the vector space $U$ is finite-dimensional, as is the case whenever $U = \mathcal{L}(V, W)$ for finite-dimensional $V$ and $W$; the fact that we always interpret finite-dimensional vectors as column matrices is what makes this case of $\Psi_U$ &amp;ldquo;canonical.&amp;rdquo; In other cases where matrix representations make no sense (e.g. the linear map of the Fourier transform $\mathcal{F}$ from 3.6), the choice of $\Psi_U$ will have to be more conscientious.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;linear-forms&#34;&gt;Linear Forms&lt;/h4&gt;
&lt;p&gt;In a way that is &amp;ldquo;dual&amp;rdquo; to the statement 3.10, one could look at another representation which, while not as standard as mapping onto linear maps of column-matrix form (and hence non-canonical), can be seen as equally valid. Namely, one could map vectors to linear maps of row-matrix form,&lt;/p&gt;
$$
\Psi_U^* : U \to \mathcal{L}(U, \mathbb{F}^1) \;\; \text{s.t.} \;\; \Psi_U^*(u) = \varphi_u \; \forall \, u \in U.
$$&lt;p&gt;That is, for each vector $u \in U$, we can choose to represent it as a linear map $\varphi_u : U \to \mathbb{F}^1$. Back to the line of thought in 3.10, we see that when a basis is fixed, any such $\varphi_u$ can be identified by a $1 \times n$ matrix (again by 3.8). This can be illustrated quite simply for finite dimensions:&lt;/p&gt;
$$
\varphi_u(v) = 
\begin{bmatrix}
u_1 \quad
u_2 \quad
\dots \quad
u_{(\dim U)}
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{(\dim U)}
\end{bmatrix} =
\begin{bmatrix}
\sum_{i \, = \, 1}^{\dim U} u_i v_i
\end{bmatrix}.
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.13. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Just as stated in 3.12 for $\Psi_U$, motivating the form $U \to \mathbb{F}^1$ from row matrices in the case of vector spaces $U$ of infinite dimension does not always work, as matrix representations sometimes make no sense there. Hence, the concrete choice of $\Psi_U^*$ may require deeper consideration (e.g. in uncountably-infinite dimensions).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;dual-spaces&#34;&gt;Dual Spaces&lt;/h4&gt;
&lt;p&gt;All linear maps $\varphi_u : U \to \mathbb{F}^1$ receive the name of &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_form&#34;&gt;linear forms&lt;/a&gt; on the space $U$. After 3.8, this is no more than the vector space&lt;/p&gt;
$$
 U^* = \{ \, \varphi : U \to \mathbb{F}^1 \; | \; \varphi \text{ is linear} \}.
$$&lt;p&gt;This is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dual_space&#34;&gt;dual vector space&lt;/a&gt; of $U$, which receives the special notation $U^*$ due to how naturally it arises. Much like elements of $U$ can be represented by column matrices, the elements of $U^*$ (which are called covectors) can be represented by row matrices (wherever matrices make sense).&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.14. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;In any Hilbert space $H$, for every continuous linear form $\varphi \in H^*$, there is a unique vector $u \in H$ with&lt;/p&gt;
$$
 \forall \, v \in H, \; \varphi(v) = \langle u, v \rangle.
$$&lt;p&gt;Furthermore, $||u|| = ||\varphi||$. This gives a natural identification between elements of $H$ and $H^*$ by the canonical bijection $J : u \mapsto \varphi$. Here, we say &amp;ldquo;canonical&amp;rdquo; because it is provided uniquely by the inner product. This is the statement of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Riesz_representation_theorem&#34;&gt;Riesz representation theorem&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is not difficult to see that for any vector space $V$ over a field $\mathbb{F}$, in fact, $\Psi_V$ and $\Psi_V^*$ are both bijections. Since they are canonical (in the sense that they are naturally defined), it is only right to assert that&lt;/p&gt;
$$
V \cong \mathcal{L}(\mathbb{F}^1, V) \;\; \text{and} \;\; V^* \cong \mathcal{L}(V, \mathbb{F}^1).
$$&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;frigyes-riesz.jpg&#34;
         alt=&#34;Frigyes Riesz (January 22, 1880 – February 28, 1956)&#34; width=&#34;256&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Frigyes Riesz (January 22, 1880 – February 28, 1956)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h4 id=&#34;multilinearity&#34;&gt;Multilinearity&lt;/h4&gt;
&lt;p&gt;We can continue talking about linearity in maps even when they have multiple arguments. For a map $T$ from multiple vector spaces $V_i$ into another $W$ (all over some field $\mathbb{F}$) where&lt;/p&gt;
$$
T : V_1 \times V_2 \times \dots \times V_n \to W \;\; \text{s.t.} \;\; T(v_1, \, v_2, \, \ldots, \, v_n) = w,
$$&lt;p&gt;we say that $T$ is linear in an argument $v_i$ if, for all other arguments $v_j \neq v_i$, fixing $v_j$ makes the altered map $T^\prime : V_i \to W$ linear. If such a map $T$ is linear in all of its $n$ arguments it is called $n$-linear, and all maps like this are called &lt;a href=&#34;https://en.wikipedia.org/wiki/Multilinear_map&#34;&gt;multilinear maps&lt;/a&gt;. The set of multilinear maps of this form is denoted&lt;/p&gt;
$$
\mathcal{L}(V_1, \, \ldots, \, V_n; \, W) = \{\, T : V_1 \times \dots \times V_n \to W \; | \; \text{ T is linear} \}.
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.15. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Matrix-vector multiplication can be seen as a bilinear ($2$-linear) map,&lt;/p&gt;
$$
B : M_{m \times n}(\mathbb{F}) \times \mathbb{F}^n \to \mathbb{F}^m.
$$&lt;p&gt;This is with the understanding that $m$-by-$n$ matrices with entries in $\mathbb{F}$, in other words $M_{m \times n}(\mathbb{F})$, indeed form a vector space over the same field $\mathbb{F}$. One can verify that fixing either argument (the vector or the matrix) forms a linear map with the other argument.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We may also extend linear forms with the same treatment that led us to multilinear maps. In particular, if an $n$-linear map over a field $\mathbb{F}$ has the codomain of $\mathbb{F}$ itself, it is called an $n$-linear form (where all maps like this are called &lt;a href=&#34;https://en.wikipedia.org/wiki/Multilinear_form&#34;&gt;multilinear forms&lt;/a&gt;).&lt;/p&gt;
&lt;h4 id=&#34;tensor-product&#34;&gt;Tensor Product&lt;/h4&gt;
&lt;p&gt;If one has two vector spaces $V$ and $W$ over the same field, one can naturally talk about their cartesian product $V \times W$ (as we have been doing in the case of multilinear maps). But instead of doing that, one can talk about a third vector space $V \otimes W$ (called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor_product&#34;&gt;tensor product&lt;/a&gt; of $V$ and $W$) which, while having just as much expressiveness as $V \times W$, of course has the added benefit of being a vector space itself.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.16. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Let $\varphi : V \times W \to V \otimes W$ be a bilinear map. Then, for each bilinear map $h : V \times W \to Z$ (into another vector space $Z$), there is a unique linear map $\tilde h : V \otimes W \to Z$ such that $h = \tilde h \circ \varphi$. This is referred to as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Universal_property&#34;&gt;universal property&lt;/a&gt; of the tensor product, which justifies the phrase &amp;ldquo;just as much expressiveness.&amp;rdquo;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Every tensor product $V \otimes W$ is equipped with such a bilinear map $\varphi : V \times W \to V \otimes W$ that allows the construction of vectors in $V \otimes W$. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Outer_product&#34;&gt;outer product&lt;/a&gt; is an example of this in finite dimensions, but in other cases one must be more creative. Confusingly, this map $\varphi$ is also called a tensor product, and $\otimes$ is predominantly used instead of $\varphi$ in notation. Summarizing,&lt;/p&gt;
$$
\forall (v, w) \in V \times W, \; \varphi(v, w) = v \otimes w \quad \text{where} \quad v \otimes w \in V \otimes W.
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.17. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Even if the tensor product among vectors is not strictly commutative, there are canonical isomorphisms among permutations of tensor products of vector spaces. That is, for any permutation $\sigma$,&lt;/p&gt;
$$
V_1 \otimes \cdots \otimes V_n \;\cong\; V_{\sigma(1)} \otimes \cdots \otimes V_{\sigma(n)}.
$$&lt;p&gt;Due to this symmetry, we often write tensor products as if they were commutative without loss of generality. But when one talks about actual computation or representations, order will probably matter.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Totally, through 3.16 and 3.17, the tensor product is precisely designed to &amp;ldquo;linearize&amp;rdquo; multilinear maps. To elaborate, for any multilinear map $h \in \mathcal{L}(V_1, \, \ldots, \, V_n; \, W)$, there exists a unique linear map&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
$$
\tilde h : \bigotimes_i V_i \to W \;\; \text{s.t.} \;\; \tilde h(v_1 \otimes \cdots \otimes v_n) = h(v_1, \ldots, v_n).
$$&lt;h4 id=&#34;more-matrices&#34;&gt;More Matrices&lt;/h4&gt;
&lt;p&gt;Being now able to identify every multilinear map with a unique linear map over a tensor product space, it is possible to assert that 3.8, 3.9, and 3.10 also apply to tensor product spaces. I will reiterate the notes, dressing them up specifically for the case of tensor product spaces.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.18. Specialization of 3.8&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The set of linear maps from a tensor product space $\bigotimes_i V_i$ over the field $\mathbb{F}$ to another vector space $W$ over $\mathbb{F}$ forms a vector space over $\mathbb{F}$. Symbolically,&lt;/p&gt;
$$
\mathcal{L}({\textstyle\bigotimes}_i V_i, W) = \left\{ \, T : \bigotimes_i V_i \to W \;\; \bigg| \;\; T \text{ is linear}  \right\}
$$&lt;p&gt;is a vector space over $\mathbb{F}$. We denote the operator case as $\mathcal{L}(\bigotimes_i V_i) = \mathcal{L}(\bigotimes_i V_i, \, \bigotimes_i V_i)$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.19. Specialization of 3.9&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;There is a bijection between $\mathcal{L}(\bigotimes_i V_i, W)$ and $\mathbb{F}^{\times_i (\dim V_i)} \times \mathbb{F}^{(\dim W)}$ when $V$ and $W$ are finite-dimensional vector spaces over $\mathbb{F}$. That is, for each linear map from a tensor product over spaces $V_1, \, \ldots, \, V_n$ and into $W$ (all over a field $\mathbb{F}$), there is one matrix with axis lengths $(\dim V_1, \, \ldots, \, \dim V_n, \, \dim W)$ with entries in $\mathbb{F}$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Often, $(\dim V_1, \, \ldots, \, \dim V_n, \, \dim W)$ from 3.19 is referred to as the shape of the matrix. Each entry of the shape tuple can be viewed as the sidelength of a pictographical embedding of the matrix in $\mathbb{R}^{n + 1}$. For example, the matrix in $\mathbb{R}^{2 \times 3}$&lt;/p&gt;
$$
M = 
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
\end{bmatrix}
$$&lt;p&gt;is said to have shape $(2, 3)$ &amp;ndash; once it is &amp;ldquo;drawn on paper,&amp;rdquo; its &amp;ldquo;sidelengths&amp;rdquo; are $2$ and $3$. Keeping the spirit of &amp;ldquo;pictographical&amp;rdquo; representation, people often call this a $2$-dimensional array, as it can be neatly &amp;ldquo;drawn&amp;rdquo; on two dimensions. But concretely, this matrix corresponds to a $6$-dimensional linear map. This ambiguity is often resolved by calling each shape entry an &amp;ldquo;axis&amp;rdquo; instead of a dimension, understanding that it refers to the visual axis of $\mathbb{R}^n$ where we would pictographically embed it.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.20. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Software libraries that represent matrices often make the choice of calling them either $n$-dimensional arrays or simply tensors in an effort to maintain generality. Arguably, these terms are both misnomers. Personally, I think they should have just called them all matrices.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.21. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Appending a trailing \(1\) to the shape of a matrix does not change the underlying object. Concretely, a matrix of shape \((\alpha_1, \, \ldots, \, \alpha_n)\) can be naturally identified with one of shape \((\beta_1, \, \ldots, \, \beta_n)\) when $\prod_i \alpha_i = \prod_i \beta_i$. This is a result of the isomorphisms included in the scope of&lt;/p&gt;
$$
\prod_i \dim \Alpha_i = \prod_i \dim \Beta_i \iff \bigotimes_i \Alpha_i \;\cong\; \bigotimes_i \Beta_i \, .
$$&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Some bijections of the form $f : \bigotimes_i \Alpha_i \to \bigotimes_i \Beta_i$ are often referred to as &amp;ldquo;reshapings.&amp;rdquo; This concept is hard to formalize, but pictographically graspable. For example, this is one way to reshape $M$:&lt;/p&gt;
$$
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
\end{bmatrix}
\xrightarrow{f}
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\
\end{bmatrix}
\xrightarrow{g}
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
5 &amp; 6 \\
\end{bmatrix}.
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.22. Specialization of 3.10&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Every vector $v$ in a tensor product space $\bigotimes_i V_i$ can be seen as a linear map from the sapce $\mathbb{F}^1$ into $\bigotimes_i V_i$ through the definition&lt;/p&gt;
$$
\psi_v : \mathbb{F}^1 \to \bigotimes_i V_i \, , \;\; \psi_v(\lambda) = v \lambda.
$$&lt;p&gt;With a basis for $\bigotimes_i V_i$ fixed, the map $\psi_v$ can be represented by a matrix of shape $(1, \, \bigotimes_i V_i)$ (as an instance of note 3.19). However, in the context of tensor product spaces, it is common to represent $\psi_v$ using a matrix of shape $(\dim V_1, \, \ldots, \, \dim V_n)$ (invoking 3.21). This is done to facilitate descriptions of computations involving the vector in question.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;homogeneous-tensors&#34;&gt;Homogeneous Tensors&lt;/h4&gt;
&lt;p&gt;Many people refer to vectors in tensor product spaces as tensors, especially in computationally-oriented scientific disciplines. This population has recently gained numerosity (and maybe even majority) thanks to the increasing availability of efficient computers and their applications. But traditionally, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)&#34;&gt;tensor&lt;/a&gt; is a linear map associated with a single vector space $V$ over $\mathbb{F}$ of the form&lt;/p&gt;
$$
\begin{equation}
 T_{n}^{\, m} : (\times^m \, V^*) \times (\times^n \, V) \to \mathbb{F}.
\end{equation}
$$&lt;p&gt;Here, $(m, \, n)$ is called the &amp;ldquo;type&amp;rdquo; of the tensor $T$. This makes a map like $m : \mathbb{R}^2 \times \mathbb{R}^4 \to \mathbb{R}$ strictly not interpretable as a tensor, as $m$ itself is not of tensor form, and there is no standard bijection that can help us view it as a tensor. That is, we cannot always identify a tensor with a multilinear map.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.23. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The tensor space of type $(m, \, n)$ defined over a vector space $V$ is denoted&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
$$
T_{n}^{\, m}(V) = \mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, \mathbb{F}).
$$&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.24. Examples&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Let us observe the types of some known tensors of form $T_{n}^{, 0} : (\times^n \, V) \to \mathbb{F}$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Any scalar $\lambda : a \mapsto \lambda a$ is a tensor of type $(0, \, 0)$.&lt;/li&gt;
&lt;li&gt;Any linear form $\varphi : v \mapsto \varphi(v)$ is a tensor of type $(0, \, 1)$.&lt;/li&gt;
&lt;li&gt;Any quadratic form $q : (v, w) \mapsto v^\top A w$ is a tensor of type $(0, \, 2)$.&lt;/li&gt;
&lt;li&gt;Any inner product $\langle \cdot, \cdot \rangle : (v, w) \mapsto \mathbb{F}$ is a tensor of type $(0, \, 2)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;vera-molnar-structures-of-squares-1974.png&#34;
         alt=&#34;Vera Molnár, &amp;lsquo;Structures of Squares&amp;rsquo; (1974)&#34; width=&#34;512&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Vera Molnár, &amp;lsquo;Structures of Squares&amp;rsquo; (1974)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h4 id=&#34;tensor-identification&#34;&gt;Tensor Identification&lt;/h4&gt;
&lt;p&gt;When the definition of a multilinear map involves many vector spaces, there will continue to be a lack of such a canonical isomorphism (at least using the insights we currently have). But if we consider arbitrary multilinear maps defined over a single vector space $V$ over a field $\mathbb{F}$ (even those without a codomain $\mathbb{F}$ ), we will see that canonically&lt;/p&gt;
$$
\begin{equation}
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* ))
\cong
\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right),
\end{equation}
$$&lt;p&gt;where we always interpret $\times^0 \, V = \mathbb{F}$ and $\otimes^0 \, V = \mathbb{F}^1$ (as always, with $\mathbb{F} \cong \mathbb{F}^1$). This essentially claims we can uniquely identify vector-valued multilinear maps defined over a single vector space $V$ with vectors in a tensor product space defined only over $V$.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.25. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;As a result of 3.16, for all vector spaces $V$, $W$, and $Z$,&lt;/p&gt;
$$
\mathcal{L}(V \otimes W, \, Z) \cong\ \mathcal{L}(V, \, W; \, Z).
$$&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.26. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Any finite-dimensional vector space is canonically isomorphic to its own dual due the standard bijection&lt;/p&gt;
$$
\Phi : V \to V^* \;\; \text{s.t.} \;\; \Phi[v](w) = \langle v, w \rangle,
$$&lt;p&gt;where $u, w \in V$ (when $V$ is not an inner-product space one defaults to the standard dot product). However, many infinite-dimensional inner-product spaces have duals which cannot be spanned using this strategy. One exception is the set of Hilbert spaces, where 3.14 provides the bijection $J$ (still through the inner product).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.27. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;For any Hilbert space or finite-dimensional vector space $V$, we have that $V^* \otimes V^* \cong (V \otimes V)^*$ canonically. This is supported by the following standard choice of bijection&lt;/p&gt;
$$
\Phi : V^* \otimes V^* \to (V \otimes V)^* \;\; \text{s.t.} \;\; \Phi[\psi \otimes \varphi](v \otimes w) \mapsto \psi(v) \varphi(w),
$$&lt;p&gt;where $\psi, \varphi \in V^*$ and $v, w \in V$. To be clear, $\Phi[\psi \otimes \varphi]$ is in $(V \otimes V)^*$ and has the form $V \otimes V \to \mathbb{F}$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To show $(9)$, our strategy will be to consider an arbitrary multilinear map $T$ in the set&lt;/p&gt;
$$
\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V) = 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* )).
$$&lt;p&gt;We will introduce a bijection $\tilde \Gamma$ that identifies a unique tensor of type $(m + a, \, n + b)$ for each $T$. Then, we will introduce another bijection $\hat \Gamma$ to show that $T_{n + b}^{\, m + a}(V)$ is isomorphic to $\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right)$. Summarizing, we will construct an invertible trip&lt;/p&gt;
$$
\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V) 
\xrightarrow{\displaystyle \tilde \Gamma} 
T_{n + b}^{\, m + a}(V) 
\xrightarrow{\displaystyle \hat \Gamma} 
\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right).
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.28. Demonstration&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Let us first consider $\tilde \Gamma$. Given a $T \in \mathcal{L}_{(n, \, m)}^{\, (a, \, b)}(V)$, define $\tilde T^\prime$ where&lt;/p&gt;
$$
\begin{align*}
\tilde T^\prime : \; &amp; (\times^m \, V^*) \times (\times^n \, V) \times (\left( \otimes^a \, V \right) \otimes \left( \otimes^b \, V^* \right))^* \to \mathbb{F} \\
&amp; \text{s.t.} \;\; \tilde T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n,  \, \varphi) = \varphi(T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n)),
\end{align*}
$$&lt;p&gt;where $v_i \in V^*$, $w_i \in V$, and $\varphi \in (\left( \otimes^c \, V \right) \otimes \left( \otimes^d \, V^* \right))^*$ is the covector of $T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n)$ (which can be canonically determined by 3.14). Then, observe that&lt;/p&gt;
$$
\begin{align*}
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, &amp; \,\, V_{(1)}, \, \dots, \, V_{(n)}, \, (( \otimes^a \, V ) \otimes ( \otimes^b \, V^* ))^*; \, \mathbb{F}) \\
&amp;\overset{\text{3.27}}{\cong} 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \,\, V_{(1)}, \, \dots, \, V_{(n)}, \, ( \otimes^a \, V^* ) \otimes ( \otimes^b \, V ); \, \mathbb{F}) \\
&amp;\overset{\text{3.25}}{\cong} 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m + a)}^*, \,\, V_{(1)}, \, \dots, \, V_{(n + b)}; \, \mathbb{F}).
\end{align*}
$$&lt;p&gt;Using this, a map $\tilde T$ can be uniquely constructed from $\tilde T^\prime$ (invoking 3.25 and 3.17 for argument order), where&lt;/p&gt;
$$
\tilde T : (\times^{m + a} \, V^*) \times (\times^{n + b} \, V)\to \mathbb{F}.
$$&lt;p&gt;This finalizes the definition of $\tilde \Gamma : T \mapsto \tilde T$. Each step of this process is bijective, therefore $\tilde \Gamma$ is bijective.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.29. Demonstration&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Now we can consider $\hat \Gamma$. Define the tensor $T \in T_{n + b}^{, m + a}(V)$ such that by definition&lt;/p&gt;
$$
T : (\times^{m + a} \, V^*) \times (\times^{n + b} \, V) \to \mathbb{F}.
$$&lt;p&gt;By repeated application of 3.25, we can always use $T$ to construct a unique&lt;/p&gt;
$$
\hat T^\prime : (\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V) \to \mathbb{F}.
$$&lt;p&gt;This is clearly a linear form. In other words, $\hat T^\prime \in ((\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V))^*$. But by 3.27, there is a unique&lt;/p&gt;
$$
\hat T \in (\otimes^{m + a} \, V) \otimes (\otimes^{n + b} \, V^*)
$$&lt;p&gt;for each $\hat T^\prime$ we could construct. This finalizes the definition of $\hat \Gamma : T \mapsto \hat T$. Each step above is bijective, so $\hat \Gamma$ is itself a bijection.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We have shown that vector-valued multilinear maps defined on a single vector space (such as operators) do identify tensors uniquely, despite not being of tensor form. Also, we have shown how tensors uniquely identify elements of tensor product spaces defined on a single vector space. This is why it is normal refer to all of these objects as tensors.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.30. Examples&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Equation $(9)$ provides a formula for identifying the tensor type of elements in $\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V)$. Indeed, tensor type acts like an algebraic signature for these objects.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Linear operators $T : V \to V$ identify type $(1, 1)$ tensors.&lt;/li&gt;
&lt;li&gt;The tensor product $\otimes : V \times V \to V \otimes V$ identifies a type $(2, 2)$ tensor.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For linear operators (represented by square matrices), we can apply $(9)$ and observe they identify elements of $V \otimes V^*$ as they are of type $(1, 1)$. Then, square matrix multiplication represents the map&lt;/p&gt;
$$
T : (V \otimes V^*) \times (V \otimes V^*) \to (V \otimes V^*) \;\; \text{s.t.} \;\; T(A, \, B) = AB,
$$&lt;p&gt;where $A, B \in \mathcal{L}(V)$. But due to 3.25, we can reform $T : (\times^2 \, V^*) \times (\times^2 \, V) \to V \otimes V^*$. Applying $(9)$ one more time, we observe that this is a type $(3, 3)$ tensor. This illustrates how the tensor type of a multilinear map defined over other tensor product spaces can be computed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;heterogeneous-tensors&#34;&gt;Heterogeneous Tensors&lt;/h4&gt;
&lt;p&gt;We have used algebraic pathways to extend the idea of a tensor beyond scalar-valued multilinear maps. Our primary tool was the canonical isomorphism, which allowed us to ignore many specifics in all cases. Now, we will widen the concept of a tensor to involve many vector spaces with linear maps of form&lt;/p&gt;
$$
\begin{equation}
T_{B}^{\, A} : (\times_A \, V_i^*) \times (\times_B \, V_i) \to \mathbb{F}.
\end{equation}
$$&lt;p&gt;Here, we reference two indexed collections of vector spaces (over the same field $\mathbb{F}$), $\langle V_i \rangle_A$ and $\langle V_i \rangle_B$. We differentiate these tensors by calling them heterogeneous, since they involve different vector spaces. They share much theory with the homogeneous case, resulting in the isomorphism&lt;/p&gt;
$$
\begin{equation}
\mathcal{L}(\langle V_i^* \rangle_{A}, \, \langle V_i \rangle_{B}; \, ( \otimes_{C} \, V_i^* ) \otimes ( \otimes_{D} \, V_i ))
\cong
\left( \otimes_{A \cup D} \, V_i \right) \otimes \left( \otimes_{B \cup C} \, V_i^* \right).
\end{equation}
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.31. Explanation&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;For the sake of brevity, this will be hand-wavy. The main insight is to quotient $\mathcal{V} = A \cup B \cup C \cup D$ by simple equality. For each partition in $\mathcal{V}_k \in (\mathcal{V}/=)$ of vector spaces associated with a map&lt;/p&gt;
$$
T \in \mathcal{L}(\langle V_i^* \rangle_{A}, \, \langle V_i \rangle_{B}; \, ( \otimes_{C} \, V_i ) \otimes ( \otimes_{D} \, V_i )),
$$&lt;p&gt;fix an argument for all vector spaces in $\mathcal{V} \setminus \mathcal{V}_k$, creating a new multilinear map&lt;/p&gt;
$$
\tilde T_k \in \mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* )),
$$&lt;p&gt;where $\mathcal{V}_k = [V]$ in $(\mathcal{V} / =)$ and $|\mathcal{V}_k| = a + b + m + n$ (skipping additional bookeeping). By 3.28 and 3.29, $\tilde T_k$ identifies a homogeneous tensor. Since all $\tilde T_k$ can be uniquely transformed this way, we can define a new $\bar T$ that has the cartesian product of their domains as its own domain with codomain $\mathbb{F}$ (by multilinearity). Finally, 3.25 gives the desired form (the RHS of $(11)$). These steps were through isomorphisms, hence $(11)$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.32. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Much like $(9)$ relies on finite-dimensionality of the vector space in question for homogeneous tensors (or that it be a Hilbert space), the same restriction is needed in $(11)$ for all vector spaces in heterogeneous tensor forms.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is still possible to consider tensor type in the heterogeneous case &amp;ndash; one simply has to keep track of one type tuple per $[V] \in (\mathcal{V}/=)$ (see 3.31). If there are $K$ such equivalence classes, a tensor type may be&lt;/p&gt;
$$
(m_1, \, \ldots, \, m_K, \, n_1, \, \ldots, \, n_K).
$$&lt;p&gt;With heterogeneous tensors, one must also carry a mapping of type index to corresponding vector space. Without this, we would not know which vector space each type tuple $(m_i, \, n_i)$ corresponds to. So when it is not clear from context, we simply say that the type is $(A, \, B)$ as implied from the syntax of $(10)$.&lt;/p&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;vera-molnar-untitled-1974.png&#34;
         alt=&#34;Vera Molnár, Untitled (1974)&#34; width=&#34;512&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Vera Molnár, Untitled (1974)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h4 id=&#34;tensor-contractions&#34;&gt;Tensor Contractions&lt;/h4&gt;
&lt;p&gt;The statements of $(9)$ and $(11)$ may initially seem like a cryptic justification of our choice of vocabulary; they justify why we use the word &amp;ldquo;tensor&amp;rdquo; so liberally, with the most general use being in reference to an element of a heterogeneous tensor product space (up to isomorphism).&lt;/p&gt;
&lt;p&gt;But beyond justifying use of language, $(9)$ and $(11)$ also provide a clear perspective on computation with tensors. These isomorphisms specify an &amp;ldquo;exchange rate&amp;rdquo; between inputs and outputs of homogeneus and heterogeneous tensors. Concretely, one may algebraically &amp;ldquo;trade&amp;rdquo; a tensor input in $V$ for a tensor product evaluation with a canonical element of $V^*$ in the output as many times as desired while maintaining type.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.33. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Consider a vector $v \in V$. Earlier, 3.10 showed that this can be seen as a linear map $\psi_v : \mathbb{F} \to V$. Indeed, we can say that $v$ is a vector of type $(1, 0)$ by application of $(9)$ (which helps identify $\psi_v$ with a map in the form of $(8)$, providing its unique tensor type). Informally, we traded an application of $\cdot \otimes V$ in the codomain $V$ (turning it into $\mathbb{F}$ via $\otimes^0 \, V \cong \mathbb{F}$) for an argument in $V^*$ to the domain $\mathbb{F}$ (recall $\times^0 \, V \cong \mathbb{F}$) to finally identify&lt;/p&gt;
$$
\hat \psi_v : V^* \to \mathbb{F} \;\; \left( \, \text{s.t.} \;\; \hat \psi_v :  \times^{(0 \, + \, 1)} \, V^* \to \otimes^{(1 \, - \, 1)} \, V \, \right).
$$&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This becomes especially powerful in the context of composition. Indeed, we can legally do &amp;ldquo;trades&amp;rdquo; of this kind (even disregarding argument order by 3.17) to reorganize and compose tensors as needed. In other words, we may be able to compose the same two tensors in surprisingly many different ways after we use these &amp;ldquo;trades&amp;rdquo; to view each of them as one of many different linear maps they can represent.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.34. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Consider two linear operators $f, \, g \in \mathcal{L}(V)$. They are of form $V \to V$ and have type $(1, 1)$. Without loss of generality, apply $(9)$ to $f$ and $g$ to identify&lt;/p&gt;
$$
\hat g : \mathbb{F} \to V^* \otimes V \;\; \text{and} \;\; \hat f : V^* \otimes V \to \mathbb{F}.
$$&lt;p&gt;(Recall that 3.25 allows us to identify $\hat f$ from the form $V^* \times V \to \mathbb{F}$.) Then, we may compose $\hat f \circ \hat g$, which is a tensor of type $(0, 0)$ (the type of a scalar). But we could have just as easily identified&lt;/p&gt;
$$
\tilde g : V^* \otimes V \to \mathbb{F} \;\; \text{and} \;\; \tilde f : \mathbb{F} \to V^* \otimes V,
$$&lt;p&gt;in which case $\tilde f \circ \tilde g$ would be a tensor of type $(2, 2)$. As a final case, we could compose $f$ and $g$ as defined to obtain $f \circ g$, a tensor of type $(1, 1)$. Hence, can obtain tensors of type $(0, 0)$, $(1, 1)$, and $(2, 2)$ from $f$ and $g$ via canonical identification and simple composition. We can even continue doing &amp;ldquo;trades&amp;rdquo; in these three tensors (without composition), allowing us to reach the types $(a, b)$ where $a + b \in \{0, \, 2, \, 4 \}$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In the example of 3.34, the map $\otimes : (f, \, g) \mapsto \tilde f \circ \tilde g$ receives the special name of tensor outer product. It is defined for any two tensors, just as the tensor product (which it is a special case of) is defined on any two tensor product spaces. Taking the outer product of two tensors of type $(a, b)$ and $(c, d)$ results in one more of type $(a + c, \, b + d)$.&lt;/p&gt;
&lt;p&gt;Likewise, the map $\langle \cdot, \, \cdot \rangle : (f, \, g) \mapsto \hat f \circ \hat g$ is simply a special case of an inner product. As such, it only made sense in 3.34 as it admitted two tensors that live in the same tensor product space. In such cases, the inner product of two tensors of (necessarily equal) type $(a, b)$ is a scalar of type $(0, 0)$.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.35. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Let us take inspiration in the extreme effect that the outer and inner products have in the types of their outputs, using the same tensors $f$ and $g$ as in the example of 3.34. We know from $(9)$ that we can identify&lt;/p&gt;
$$
T_g, \, T_f \in V \otimes V^*
$$&lt;p&gt;from $f$ and $g$ (canonically). We will see that we can obtain the types $(2, 2)$, $(1, 1)$, and $(0, 0)$ without invoking composition, giving us a new perspective on tensor operations. First, the outer product identifies $\hat f \circ \hat g$ with the tensor of type $(2, 2)$ obtained by $T_f \otimes T_g \in V \otimes V^* \otimes V \otimes V^*$.&lt;/p&gt;
&lt;p&gt;The next key concept is the evaluation map, which is made canonical by convention. It is defined as the tensor $\text{ev}_U : U^* \otimes U \to \mathbb{F}$ of type $(1, 1)$ such that $\text{ev}_U(\varphi \otimes u) = \varphi(u)$. We can use it to obtain another map&lt;/p&gt;
$$
\begin{align*}
(\text{id}_V \otimes \text{ev}_V \otimes \text{id}_{V^*}) &amp; : V \otimes V^* \otimes V \otimes V^* \to V^* \otimes V \\
&amp; \text{s.t.} \;\; (\text{id}_V \otimes \text{ev}_V \otimes \text{id}_{V^*})(v \otimes \varphi \otimes w \otimes \phi) = \varphi(w) (v \otimes \phi),
\end{align*}
$$&lt;p&gt;where the tensor $(\text{id}_U \otimes \text{ev}_U \otimes \text{id}_U)(T_f \otimes T_g)$ corresponds exactly to $f \circ g$ and is of type $(1, 1)$. Similarly, applying the evaluation map a second time decreases tensor type uniformly, where we can use&lt;/p&gt;
$$
\begin{align*}
(\text{ev}_V \otimes \text{ev}_V) : V^* &amp; \otimes V \otimes V^* \otimes V \to \mathbb{F} \\
&amp; \text{s.t.} \;\; (\text{ev}_V \otimes \text{ev}_V)(v \otimes \varphi \otimes w \otimes \phi) = \varphi(w) \phi(v)
\end{align*}
$$&lt;p&gt;with 3.17 (to disregard argument order) to get $(\text{ev}_V \otimes \text{ev}_V)(T_f \otimes T_g)$, corresponding exactly to $\tilde f \circ \tilde g$, whose type is $(0, 0)$ (a scalar). The pattern is becomes clear &amp;ndash; the evaluation map provides a canonical way to obtain a tensor of type $(a - 1, \, b - 1)$ from another of type $(a, \, b)$, annihilating one vector-covector argument pair of our choosing (when the input tensor is viewed as a multilinear map). After, we may still perform &amp;ldquo;trades&amp;rdquo; on the resulting tensors (independently of any idea of composition, as described in 3.33).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The process of &amp;ldquo;evaluation&amp;rdquo; (perhaps done over many vector-covector argument pairs simultaneously) as described in 3.35 is known as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor_contraction&#34;&gt;tensor contraction&lt;/a&gt;. Note that it can involve any number of tensors, as the atomic step is the evaluation of $\text{ev}_V$ with respect to a single vector-covector pair of arguments involved in the group of tensors. The collection of tensors involved in a contraction is referred to as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor_network&#34;&gt;tensor network&lt;/a&gt;. The result of a contraction is of course a single tensor, which can be seen to &amp;ldquo;compose&amp;rdquo; the tensors in the network in arbitrarily complex ways (through the perspective in 3.34).&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.36. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Remember that $(9)$ and $(11)$ rely on the assumption that $V^* \cong V$ exists and is canonical, as with any Hilbert space (per 3.14) or finite-dimensional vector space. This is what underlies 3.34 (the perspective of &amp;ldquo;trades and compositions&amp;rdquo;) and 3.35 (the perspective of &amp;ldquo;contractions through evaluations&amp;rdquo;). The idea of &amp;ldquo;trading&amp;rdquo; described in 3.33 was central in these contexts.&lt;/p&gt;
&lt;p&gt;While assuming a canonical $V^* \cong V$ provides a way to perform &amp;ldquo;trades&amp;rdquo; which we semantically interpret to be uniquely correct (the bijection underlying the isomorphism), many applications introduce special tensors of type $(2, 0)$ or $(0, 2)$ with the sole purpose of using them as adapters, enabling &amp;ldquo;trades&amp;rdquo; via contractions with them, for example, to contract a tensor of type $(0, 7)$ with an adapter of type $(2, 0)$ to obtain another of type $(1, 6)$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;heterogeneous-contractions&#34;&gt;Heterogeneous Contractions&lt;/h4&gt;
&lt;p&gt;We have mostly ignored heterogeneous tensor spaces. Now, the framework of contractions offers a great opportunity to pull them back onto our train of thought. We have implied two stages for finding the type of a tensor contraction. First, consider the tensor product of all the spaces involved. Then, repeatedly utilize the canonical map $\text{ev}_V$, once per pair of dual spaces involved in the contraction.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.37. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;For a homogeneous tensor network involving tensors $T_1, \, \ldots, \, T_n$ with $T_i$ of shape $(a_i, b_i)$, the tensor product of all the tensors in the network is&lt;/p&gt;
$$
\otimes_i \, T_i \in \otimes_i \, ((\otimes^{a_i} V) \otimes (\otimes^{b_i} V^*)) \;\; \text{s.t.} \;\; T_i \in T_{b_i}^{\, a_i}(V)
$$&lt;p&gt;thanks to $(9)$. Applying 3.17 to reorganize, we see that $\otimes_i \, T_i$ is of type $(\Sigma_i a_i , \, \Sigma_i b_i)$. We then continue to use 3.17 and compose $\text{ev}_V$ with $\text{id}_V$ to construct mappings that perform arbitrary contractions just as done in 3.35, where together with &amp;ldquo;trades,&amp;rdquo; we may achieve a contraction with any type in&lt;/p&gt;
$$
\{ \, (a, b) \; : \; a + b = 2k \;\; \text{s.t.} \;\; 0 \leq k \leq \min(\Sigma_i a_i , \, \Sigma_i b_i) \, \}.
$$&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here, we notice that we can use specialized maps $\text{ev}_X$ for appearances of each different vector space $X$ in a heterogeneous tensor contraction. While I will not formulate a heterogeneous equivalent of 3.37 (as it would be exceedingly verbose), we can see that for a tensor of type&lt;/p&gt;
$$
(\{A_{(1)}^*, \, \ldots, \, B_{(1)}^*, \ldots \}, \{A_{(1)}, \, \ldots, \, B_{(1)}, \ldots \}),
$$&lt;p&gt;a contraction is determined by a collection of pairs $\{ (X_{(i)}^*, \, X_{(j)}), \, \ldots \}$ where maps $\text{ev}_X$ are to be used in the manner of 3.35. It requires a significant amount of bookeeping, but its soundness is visible through an argument completely analogous to 3.31. Likewise, we can apply &amp;ldquo;trading&amp;rdquo; as in 3.33 for &amp;ldquo;like terms&amp;rdquo; in their domain and codomain (pairs of vector spaces dual to each other among their factors).&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.38. Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Bookeeping heterogeneous tensor contractions is a big practical problem. In particular, many machine learning workloads which consider heterogeneous tensors (often typed over vector spaces $\langle \mathbb{R}^d \rangle_{d \, \in \, D}$) are made difficult from the need of ensuring that tensor contractions are well-formed before their coordinates can be computed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;syntax-standards&#34;&gt;Syntax Standards&lt;/h4&gt;
&lt;p&gt;An appealing model of tensor operations was offered by &lt;a href=&#34;https://en.wikipedia.org/wiki/Roger_Penrose&#34;&gt;Sir Roger Penrose&lt;/a&gt; in 1971 within the illustrated writeup &lt;a href=&#34;https://www.mscs.dal.ca/%7Eselinger/papers/graphical-bib/public/Penrose-applications-of-negative-dimensional-tensors.pdf&#34;&gt;Applications of Negative-Dimensional Tensors&lt;/a&gt;. There, he provided a first theory of abstract tensor networks which he called Abstract Tensor Systems (ATS), which came with a coordinate-free system for representing homogeneous tensors and contractions. This system became known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Penrose_graphical_notation&#34;&gt;Penrose graphical notation&lt;/a&gt;. It is delightful for any abstract treatment of tensors (like our own so far).&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;3.39. Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;In Penrose graphical notation, individual tensors are represented as nodes in a graph sometimes distinguished by geometric shapes for ease of reference. The type of the tensor being represented is indicated by its number of outgoing edges. The system differentiates a &amp;ldquo;cartesian&amp;rdquo; case by the availability of a bijection $\Phi : V \to V^*$. (We have been assuming this &amp;ndash; see 3.36). In the cartesian case, edge direction does not matter. Otherwise, a tensor of type $(a, b)$ will have $a$ upwards pointing edges and $b$ downward pointing edges. Contractions are set by connecting corresponding edges.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;roger-penrose.png&#34;
         alt=&#34;Sir Roger Penrose (born August 8, 1931)&#34; width=&#34;256&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Sir Roger Penrose (born August 8, 1931)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;h4 id=&#34;overview-2&#34;&gt;Overview&lt;/h4&gt;
&lt;h3 id=&#34;signals-and-systems&#34;&gt;Signals and Systems&lt;/h3&gt;
&lt;h3 id=&#34;kernel-methods&#34;&gt;Kernel Methods&lt;/h3&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;When considering infinite-dimensional vector spaces, this statement is true if and only if one admits the axiom of choice. Perhaps this was another motivation of Axler&amp;rsquo;s restriction to finite-dimensional vector spaces.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Non-parenthesized sub-indices imply that the item is part of an indexed set &amp;ndash; $a_i$ may not be equal to $a_j$.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Parenthesized sub-indices are only used to indicate argument index. That is, $a_{(i)} = a_{(j)}$ for all $i$ and $j$.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Poem 6. &#34;Lena&#34;</title>
      <link>http://localhost:1313/poem-6.-lena/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-6.-lena/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;entrevista criatura echada en la baldosa,
la luz penetraba el follaje 
    del helecho donde te resguardabas:
en ese entonces, los dos éramos niños y 
  nos ceñía la juventud de nuestra familia,
ese hogar llevaba tu color, 
        y en ese jardín
  nos vestía el espacio de una piel
            más radiante y transparente 

criatura entrevista entre las décadas,
  tiempo tenaz que fluye a mi costado:
  todavía soy un niño, pero sin ti
             ya no soy joven,
   mi familia ya no es joven
    ahora que tú no estás, 
        te resguardo dentro de mí,
      y en ti siempre se encontrarán 
    aquellas joyas de nuestra juventud
  dentro de las cuales centellea 
la luz que compartimos
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Poem 5. &#34;Zoe&#34;</title>
      <link>http://localhost:1313/poem-5.-zoe/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-5.-zoe/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;caudaloso río de la memoria;
sacrificio, derrota clavada,
memoria total, pensamiento
que me abruma,
               te vas
para regresar una vez más,
siempre una vez más, nunca
te vas,
        pero tampoco llegas;
ya te fuiste, y aún sigues
aquí, sigues en el instante,
la hora te produce siempre,
ya no sueño, recordar tu luz
me despierta, me carcome,
lo pagué todo para pensarte,
símbolo del olvido,
                    cara
atorada siempre en el pasado,
sólo te pienso en el pasado,
palabras que escribo pensando
en el pasado; 
        ahí perteneces,
que el instante es una nada;
el futuro toca ya la puerta
de mi alma, y no lo escucho;
     silencio que me abruma,
pensamientos que me gritan,
   memorias que me ahogan
en su caudal, río perdido
       que te busca siempre
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Poem 4. &#34;Consuelo&#34;</title>
      <link>http://localhost:1313/poem-4.-consuelo/</link>
      <pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-4.-consuelo/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Ay, mijo,
que te bendiga Dios.
No eres de ule ni de fierro.
Te amo con todo mi ser;
eres mi corazón.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Poem 3. &#34;Lucy&#34;</title>
      <link>http://localhost:1313/poem-3.-lucy/</link>
      <pubDate>Sat, 12 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-3.-lucy/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Marsupial madre de un amor tenaz
que se resguarda entre mis costillas;
me llena de un sentimiento líquido
que se construye al cerrar mis ojos.

Llueves gotas de ideas inmarcesibles.
Toda la noche llueves sobre mi rostro;
gotas de lluvia que veo ya sobre mi piel
son transparentes, por tu transparencia.

Cómo te extraño, querida amiga;
que convertiste nuestra vivienda en hogar,
y cuidas siempre de mí, así cómo yo de ti.
Pronto nos veremos otra vez.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>Poem 2. &#34;Occidental&#34;</title>
      <link>http://localhost:1313/poem-2.-occidental/</link>
      <pubDate>Wed, 30 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-2.-occidental/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Son trayectorias que trazan entre los átomos,
son curvas que desnudan misterios paralelos.
Es hipótesis de tiempo escrita en el espacio,
es información que se filtra entre la materia.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>The Elo Rating System through Likelihood Gradient Ascent</title>
      <link>http://localhost:1313/the-elo-rating-system-through-likelihood-gradient-ascent/</link>
      <pubDate>Wed, 30 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/the-elo-rating-system-through-likelihood-gradient-ascent/</guid>
      <description>&lt;aside id=&#34;toc&#34;&gt;
    &lt;details&gt;
        &lt;summary&gt;&amp;nbsp;&lt;strong&gt; Table of contents&lt;/strong&gt;&lt;/summary&gt;
        &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#mathematical-orderings&#34;&gt;Mathematical Orderings&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#elo-ratings-and-updates&#34;&gt;Elo Ratings and Updates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#derivation&#34;&gt;Derivation&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#map-estimation-1&#34;&gt;MAP Estimation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#gaussian-convolution&#34;&gt;Gaussian Convolution&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
    &lt;/details&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Probability and optimization are strong monsters. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Elo_rating_system&#34;&gt;Elo rating system&lt;/a&gt;, used to estimate performance in &lt;a href=&#34;https://en.wikipedia.org/wiki/FIDE_rankings&#34;&gt;competitive chess&lt;/a&gt;, &lt;a href=&#34;https://www.vox.com/2019/2/7/18210998/tinder-algorithm-swiping-tips-dating-app-science&#34;&gt;online dating&lt;/a&gt;, and &lt;a href=&#34;https://lmsys.org/blog/2023-05-03-arena/&#34;&gt;AI agents&lt;/a&gt;, is an under-the-hood reminder of this fact that operates within many of the systems that need to establish comparative metrics. This piece is my contribution to the endless pile of explainers on the topic. I exercise an emphasis on bayesian statistics and optimization that should ring a bell for anyone familiar with the basics of machine learning.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;mathematical-orderings&#34;&gt;Mathematical Orderings&lt;/h3&gt;
&lt;p&gt;At the risk of including a needless dependency on the topic of this piece, I introduce you to the idea of an ordering.
Colloquially, we take this to mean an arrangement (i.e., &lt;a href=&#34;https://en.wikipedia.org/wiki/Permutation&#34;&gt;a permutation&lt;/a&gt;) of a set of things.
We will replace that with its formal meaning, which is a specific kind of &lt;a href=&#34;https://en.wikipedia.org/wiki/Binary_relation&#34;&gt;binary relation&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;A &lt;strong&gt;binary relation&lt;/strong&gt; $R$ from a set $X$ to another $Y$ is a subset of $X \times Y$, where it is possible that $X = Y$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This should seem odd, as a subset is in no obvious way reminiscent of a permutation. But introducing some new syntax to indicate membership in a relation,&lt;/p&gt;
$$
  (x, y) \in R \vdash xRy,
$$&lt;p&gt;we are an example away from making sense. In particular, consider $R = \, \leq$ (less-than). When we say things like &amp;ldquo;$x \leq y$,&amp;rdquo; we are in fact using syntactic sugar for &amp;ldquo;$(x, y) \in \, \leq$.&amp;rdquo; With this in mind, we can take a look at &lt;a href=&#34;https://en.wikipedia.org/wiki/Partially_ordered_set&#34;&gt;partial orders&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;A &lt;strong&gt;partial order&lt;/strong&gt; $R$ is a binary relation over a set $X$ and itself which satisfies the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reflexivity. This means that $\forall x \in X, \, xRx$.&lt;/li&gt;
&lt;li&gt;Antisymmetry. This means that $\forall (x, y) \in X^2, \, xRy \wedge yRx \implies x = y$.&lt;/li&gt;
&lt;li&gt;Transitivity. This means that $\forall (x, y, z) \in X^3, \, xRy \wedge yRz \implies xRz$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The canonical example of a partial order is $\subseteq$ over $\mathcal P(S)$. Importantly, a partial order over a set does not imply a permutation over it, because of the possibility for two elements $x$ and $y$ to be &lt;em&gt;unrelated&lt;/em&gt;, or in other words, for $\neg xRy$ and $\neg yRx$. In a &lt;a href=&#34;https://en.wikipedia.org/wiki/Total_order&#34;&gt;total order&lt;/a&gt;, we simply do not allow this.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;A &lt;strong&gt;total order&lt;/strong&gt; $R$ is a partial order that is also total, which means that $\forall (x, y) \in X^2, \, xRy \vee yRx$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The canonical example of a total order is $\leq$ over $\mathbb{R}$. With a total order, there is a single valid ordering $\bold{x}$ (i.e. arrangement or permutation) over its set $X$ such that $x_iRx_{i + 1}$ for all $i = 0, \ldots, |X| - 1$. One more variation we can make on the idea of an order is that of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Weak_ordering&#34;&gt;weak order&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;A &lt;strong&gt;weak order&lt;/strong&gt; $R$ is a total order that is not necessarily antisymmetric.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In other words, it is possible that for distinct elements $x$ and $y$, both $xRy$ and $yRx$. This is conceptually aligned with allowing &amp;ldquo;ties&amp;rdquo; in any resulting ordering, potentially sacrificing their uniqueness.&lt;/p&gt;
&lt;h3 id=&#34;elo-ratings-and-updates&#34;&gt;Elo Ratings and Updates&lt;/h3&gt;
&lt;p&gt;We can take a look at the question that &lt;a href=&#34;https://en.wikipedia.org/wiki/Arpad_Elo&#34;&gt;Arpad Elo&lt;/a&gt; (kind of) answered: How can you compare the skill level of two chess players?&lt;/p&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;arpad_elo.jpg&#34;
         alt=&#34;Arpad Elo (August 25, 1903 – November 5, 1992)&#34; width=&#34;256&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Arpad Elo (August 25, 1903 – November 5, 1992)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;p&gt;His proposed procedure is straightforward. Each player $i \in N$ will have a real-valued rating $r_i$, which will be a proxy for their skill level. These ratings will be initialized at some predetermined value for all players. Then, when there is a match between player $i$ and $j$, the following updates are made:&lt;/p&gt;
$$
\begin{align*}
&amp;r_j \gets r_j + k(s_j - e_j), \\
&amp;r_i \gets r_i + k(s_i - e_i),
\end{align*}
$$&lt;p&gt;where&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;/p&gt;
$$
e_p = \frac{1}{1 + e^{-(r_p - r_{\text{other}})}}, \;\;\;\;
s_p = 
\begin{cases}
1 &amp;\text{if \(p\) wins},\\
0.5 &amp;\text{if draw},\\
0 &amp;\text{if \(p\) loses},\\
\end{cases}
$$&lt;p&gt;and $k$ is a constant chosen arbitrarily. So, as players accrue matches with other players, their ratings are updated according to the above rules with the hope that they will eventually stabilize. Now, the difference between players&amp;rsquo; ratings can be used to compare their skill levels via the ordering $\leq$ on $\mathbb{R}$.&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;Maximum likelihood estimation (MLE)&lt;/a&gt; is a method used to fit distribution parameters to samples. The setup for MLE is a random variable $Y$ of known distribution $\mathcal{D_\theta}$ (parameterized by $\theta$), with access to &lt;a href=&#34;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&#34;&gt;IID&lt;/a&gt; samples $\langle y_i \rangle \sim \mathcal{D}_\theta$. The objective is to estimate $\hat\theta$ such that the &lt;a href=&#34;https://en.wikipedia.org/wiki/Likelihood_function&#34;&gt;likelihood function&lt;/a&gt; $\mathcal{L}$ is maximized,&lt;/p&gt;
$$
\begin{equation}
    \hat\theta_\mathrm{MLE} 
    = \argmax_\theta \, \mathcal{L}(\theta; \langle y_i \rangle) 
    = \argmax_\theta \, \prod_i \mathbb{P}_\theta[Y = y_i].
\end{equation}
$$&lt;p&gt;In other words, MLE is the optimization procedure associated with finding the distribution parameters that were most likely to generate observed data, provided that we know or assume its distribution.&lt;/p&gt;
&lt;h4 id=&#34;map-estimation&#34;&gt;MAP Estimation&lt;/h4&gt;
&lt;p&gt;When there is access to a (known or assumed) prior $p(\theta)$ on the distribution of parameters, we can fold it into our optimization process by doing MLE on the posterior distribution, which by &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34;&gt;Bayes&amp;rsquo; theorem&lt;/a&gt;,&lt;/p&gt;
$$
\begin{align*}
    p(\theta \mid \langle y_i\rangle) 
    &amp;= \frac{p(\langle y_i \rangle \mid \theta) p(\theta)}{p(\langle y_i \rangle)} 
    \; \propto \; \underbrace{p(\langle y_i \rangle \mid \theta)}_{\displaystyle{\mathcal{L}(\theta; \langle y_i \rangle)}} p(\theta).
\end{align*}
$$&lt;p&gt;The resulting parameters $\hat\theta_\mathrm{MAP}$ are then a &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&#34;&gt;maximum a posteriori (MAP)&lt;/a&gt; estimate,&lt;/p&gt;
$$
\begin{align*}
    \hat\theta_{\mathrm{MAP}}
    &amp;= \argmax_\theta\;p\bigl(\theta \mid \langle y_i\rangle\bigr) \\
    &amp;= \argmax_\theta\;\Bigl[\mathcal{L}\bigl(\theta;\langle y_i\rangle\bigr)\,p(\theta)\Bigr] \\
    &amp;= \argmax_\theta\;\Bigl[\prod_{i=1}^n \mathbb{P}_\theta\bigl[Y=y_i\bigr] \times p(\theta)\Bigr].
\end{align*}
$$&lt;h4 id=&#34;optimization&#34;&gt;Optimization&lt;/h4&gt;
&lt;p&gt;Sometimes, it is possible to find closed-form solutions for $\hat\theta_\mathrm{MAP}$ and $\hat\theta_\mathrm{MLE}$ through convex optimization. For example, samples with gaussian noise lead to the closed-form solution of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares&#34;&gt;OLS problem&lt;/a&gt; through the process of MLE.&lt;/p&gt;
&lt;p&gt;However, most of the time the resulting optimization objective of MLE (and hence also MAP estimation) is not convex. Here, gradient-based approaches (along with all other non-convex optimization techniques) are helpful for finding local maxima of the likelihood objective.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;derivation&#34;&gt;Derivation&lt;/h2&gt;
&lt;p&gt;The derivation presented here will depart from the usual in hopes of contributing some kind of novelty. We begin with the game-theory-native idea of payoff, which we will take to be a numeric value representing a player&amp;rsquo;s utility differential with respect to the start of a game $G$,&lt;/p&gt;
$$
    \text{payoff of player i} = p_i.
$$&lt;p&gt;Next, we will consider player performance. Just as Elo, we take the performance of a player $i$ on a game $G$ to be a real-valued random variable $X_i$, independent to other players.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Observation&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Perhaps Elo motivated this decision after noticing the variance of his own performance over the chess board.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Then, we will expand our setup by allowing players to outperform others, which we will present through the difference between the performance of two players during $G$ (which is another RV),&lt;/p&gt;
$$
    \delta_{i, \, j} = X_i - X_j.
$$&lt;p&gt;We will also establish a relationship between $\delta_{i , \, j}$ and $p_i$. For this purpose, we introduce a game-specific mapping $g$ with a noise term $\epsilon \sim \mathcal{N}(0, \sigma_\epsilon^2)$, which together form the generative process of payoffs:&lt;/p&gt;
$$
    p_i = g(\delta_{i, \, j}) + \epsilon.
$$&lt;p&gt;Finally, we will assert a prior on the distribution of $X_i$, which we will refer to as $\mathcal{D}(\theta_i)$ without yet deciding on a particular distribution (just that it is parameterized by $\theta_i$). This prior $\pi(x)$ will be global for all players, and its distribution parameters will be $\theta_\pi$.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;While this is a global prior, notice that none of the following breaks if it were player-specific from the start.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;So far, none of this has helped us answer the question that Elo answered. For that, we will introduce one last artifact on top of our setup; each player $i$ will have a &amp;ldquo;rating&amp;rdquo; $r_i$, which we will ultimately use to order players by skill in our system or organization:&lt;/p&gt;
$$
    r_i = \mathbb{E}[X_i].
$$&lt;h3 id=&#34;map-estimation-1&#34;&gt;MAP Estimation&lt;/h3&gt;
&lt;p&gt;Clearly, since our goal is to know players&amp;rsquo; ratings, the only additional information we will need to get them are the distribution parameters $\theta_i$. Of course, at a lack of observations, we can assert from our prior&lt;/p&gt;
$$
    r_i = \mathbb{E}_{X \, \sim \, \pi}[X].
$$&lt;p&gt;But what if at the end of a game $G$ between players $i$ and $j$, we observe &lt;a href=&#34;https://en.wikipedia.org/wiki/Without_loss_of_generality&#34;&gt;WLOG&lt;/a&gt; the payoff $p_i$? Here, we will be wishing that $g$ is neatly invertible. Assuming it is, we arrive at the following MLE for their difference in performance via application of $(1)$:&lt;/p&gt;
$$
\begin{equation}
    \hat\delta_{i, \, j}
    = \argmax_{\delta} \exp\!\Bigl(-\frac{(p_i-g(\delta))^2}{2\sigma_\epsilon^2}\Bigr)
    = g^{-1}(p_i).
\end{equation}
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Notice &lt;strong&gt;we did not use a prior&lt;/strong&gt; when estimating $\hat\delta_{i, \, j}$. This assumption is due to Elo; we will not use players&amp;rsquo; history when calculating their performance for a single game. This is the design decision that, by omission, accounts for sudden changes in player skill (as a result of learning, etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;We just estimated the difference between the performance of the players from the payoff of a single player. The invertibility of $g$ has the hidden implication that &lt;strong&gt;it is strictly monotonic&lt;/strong&gt;; no two differences in performance lead to the same payoff, and the greater the difference, the greater the payoff for the outperforming player.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Knowing this, we can perform a bayesian update to our prior through MAP estimation. Writing down the joint posterior of the parameters of $X_i$ and $X_j$,&lt;/p&gt;
$$
\begin{equation}
    p(\theta_i, \, \theta_j  \mid \hat\delta_{i, \, j}) 
    \propto 
    \underbrace{
        p(\hat\delta_{i, \, j} \mid \theta_i, \, \theta_j)
    }_{
        \displaystyle{p(\hat\delta_{i, \, j} \mid X_i - X_j)}
    }
    p(\theta_i)p(\theta_j).
\end{equation}
$$&lt;p&gt;Notice that we already have access to priors $p(\theta_i)$ and $p(\theta_j)$; those are quite simply $\pi(\theta_i)$ and $\pi(\theta_j)$, which we assume per our initial setup.&lt;/p&gt;
&lt;h4 id=&#34;gaussian-performance&#34;&gt;Gaussian Performance&lt;/h4&gt;
&lt;p&gt;We proceed by considering the case where $X_i \sim \mathcal{N}(r_i, \, \sigma_i^2)$, such that $\theta_i = (r_i, \, \sigma_i^2)$. That is, player performance is gaussian-distributed,&lt;/p&gt;
$$
\begin{equation}
    p_{X_i}(x_i \mid \theta_i)
    = \frac{1}{\sqrt{2\pi} \, \sigma_i}
      \exp\!\Bigl(-\frac{(x_i - r_i)^2}{2\,\sigma_i^2}\Bigr).
\end{equation}
$$&lt;p&gt;Our next goal is to set up an analytic function for the likelihood $p(\hat\delta_{i, \, j} \mid \theta_i, \, \theta_j)$. We observe that we have access to the conditional density of $\hat\delta_{i, \, j}$&lt;/p&gt;
$$
\begin{equation}
    p(\hat\delta_{i, \, j} \mid x_i, \, x_j)
    = \frac{1}{\sqrt{2\pi}\,\sigma_\varepsilon}
      \exp\!\Bigl(-\frac{\bigl(g(\hat\delta_{i, \, j}) - (x_i - x_j)\bigr)^2}{2\,\sigma_\varepsilon^2}\Bigr)
      \;|g&#39;(\hat\delta_{i, \, j})|
\end{equation}
$$&lt;p&gt;by using $(2)$ implicitly through the change of variables&lt;/p&gt;
$$
    p(\hat\delta_{i, \, j} \mid x_i, \, x_j)
    = p_{p_i}\bigl(g(\hat\delta_{i, \, j}) \mid x_i, \, x_j \bigr)
    \;\Bigl|\frac{d}{d\hat\delta_{i, \, j}}\,g(\hat\delta_{i, \, j})\Bigr|,
$$&lt;p&gt;where we take&lt;/p&gt;
$$
    p\bigl(p_i \mid x_i, \, x_j \bigr)
    = \frac{1}{\sqrt{2\pi}\,\sigma_\varepsilon}
      \exp\!\Bigl(-\frac{\bigl(p_i - g(x_i - x_j)\bigr)^2}{2\,\sigma_\varepsilon^2}\Bigr).
$$&lt;p&gt;Now, we can use $(4)$ and $(5)$ to derive the desired likelihood by marginalizing,&lt;/p&gt;
$$
\begin{align*}
    p\bigl(\hat\delta_{i, \, j} \mid \theta_i,\theta_j\bigr)
    &amp;= \iint
       p\bigl(\hat\delta_{i, \, j}\mid x_i,x_j\bigr)\;
       p_{X_i}(x_i \mid \theta_i)\;p_{X_j}(x_j \mid \theta_j)\,
       dx_i\,dx_j \\
    &amp;= \iint
       \frac{1}{\sqrt{2\pi}\,\sigma_\varepsilon}
       \exp\!\Bigl(-\frac{\bigl(g(\hat\delta_{i, \, j}) - (x_i - x_j)\bigr)^2}
                         {2\,\sigma_\varepsilon^2}\Bigr)\,
       \bigl|g&#39;(\hat\delta_{i, \, j})\bigr|
    \\[-2pt]
    &amp;\quad\;\times\,
       \frac{1}{\sqrt{2\pi}\,\sigma_i}
       \exp\!\Bigl(-\frac{(x_i - r_i)^2}{2\,\sigma_i^2}\Bigr)
       \;\frac{1}{\sqrt{2\pi}\,\sigma_j}
       \exp\!\Bigl(-\frac{(x_j - r_j)^2}{2\,\sigma_j^2}\Bigr)
    \,dx_i\,dx_j.
    \\[6pt]
\end{align*}
$$&lt;p&gt;After another cup of coffee, we arrive at the following version of our joint likelihood $p\bigl(\hat\delta_{i, , j} \mid \theta_i,\theta_j\bigr)$,&lt;/p&gt;
$$
\begin{aligned}
    &amp;= \int
       \frac{1}{\sqrt{2\pi}\,\sigma_\varepsilon}
       \exp\!\Bigl(-\frac{\bigl(g(\hat\delta_{i, \, j}) - d\bigr)^2}{2\,\sigma_\varepsilon^2}\Bigr)
       \;|g&#39;(\hat\delta_{i, \, j})|\; \\
    &amp;\quad\;\quad\;\times \frac{1}{\sqrt{2\pi(\sigma_i^2+\sigma_j^2)}}
       \exp\!\Bigl(-\frac{(d - (r_i - r_j))^2}{2(\sigma_i^2+\sigma_j^2)}\Bigr)
    \,dd
\end{aligned}
$$&lt;p&gt;where $d = x_i - x_j$ (hinted at in equation $(3)$) is possible because $X_i - X_j \sim \mathcal{N}(r_i - r_j, \sigma_i^2 + \sigma_j^2)$. Finally, we obtain the following after remembering an &lt;a href=&#34;#gaussian-convolution&#34;&gt;important fact&lt;/a&gt; from signal processing,&lt;/p&gt;
$$
\begin{align}
    \mathcal{J}_\mathrm{MLE}(\theta_i, \, \theta_j; \, \hat\delta_{i, \, j})
    = \frac{|g&#39;(\hat\delta_{i, \, j})|}{\sqrt{2\pi\,\bigl(\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2\bigr)}}\,
        \exp\!\Biggl(-\frac{\bigl(g(\hat\delta_{i, \, j}) - (r_i - r_j)\bigr)^2}
                             {2\,\bigl(\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2\bigr)}\Biggr).
\end{align}
$$&lt;p&gt;Wonderful. We then attend to the reflexes drilled into our brains from machine learning, and find the gradient of the log-likelihood with respect to learned&amp;hellip; ahem, the ratings $\bold{r} = [r_i, \, r_j]^\top$:&lt;/p&gt;
$$
\begin{equation}
    \nabla_\bold{r}\log\mathcal{J}_\mathrm{MLE}(\theta_i, \, \theta_j; \, \hat\delta_{i, \, j}) = 
    \begin{bmatrix}
        \displaystyle\frac{g(\hat\delta_{i, \, j}) - (r_i - r_j)}{\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2} \\[8pt]
        \\
        \displaystyle-\frac{g(\hat\delta_{i, \, j}) - (r_i - r_j)}{\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2}
    \end{bmatrix}.
\end{equation}
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;By taking the $\log$ of the joint likelihood we achieve nothing, but we respect a very important tradition&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Using $\nabla_\bold{r}\log\mathcal{J}(\theta_i, \, \theta_j; \, \hat\delta_{i, \, j})$ as it stands to adjust $\bold{r}$ would be tantamount to MLE on $\bold{r}$. To turn this into a proper MAP estimate we must also fold in our prior terms into $(6)$, which we assume to be gaussian:&lt;/p&gt;
$$
\begin{aligned}
    \mathcal{J}_\mathrm{MAP}(\theta_i, \, \theta_j; \, \hat\delta_{i, \, j})
    &amp;= \frac{\lvert g&#39;(\hat\delta_{i,j})\rvert}
           {\sqrt{2\pi\,\bigl(\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2\bigr)}}
      \exp\!\Bigl(-\frac{\bigl(g(\hat\delta_{i, \, j}) - (r_i - r_j)\bigr)^2}
                       {2\,\bigl(\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2\bigr)}\Bigr)\\
    &amp;\quad\;\times\;
      \frac{1}{\sqrt{2\pi}\,\sigma_\pi}
      \exp\!\Bigl(-\frac{(r_i - r_\pi)^2}{2\,\sigma_\pi^2}\Bigr)
      \;\times\;
      \frac{1}{\sqrt{2\pi}\,\sigma_\pi}
      \exp\!\Bigl(-\frac{(r_j - r_\pi)^2}{2\,\sigma_\pi^2}\Bigr).
\end{aligned}
$$&lt;p&gt;Being again unable to ignore our instincts,&lt;/p&gt;
$$
\begin{equation}
    \nabla_{\mathbf r}\log \mathcal{J}_{\mathrm{MAP}}(\theta_i,\theta_j;\hat\delta_{i, \, j})
    = \begin{bmatrix}
    \displaystyle
    \frac{g(\hat\delta_{i, \, j}) - (r_i - r_j)}{\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2}
    \\\\
    \displaystyle
    -\frac{g(\hat\delta_{i, \, j}) - (r_i - r_j)}{\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2}
    \end{bmatrix}
    -
    \begin{bmatrix}
    \displaystyle
\frac{r_i - r_\pi}{\sigma_\pi^2}
\\\\
    \displaystyle
\frac{r_j - r_\pi}{\sigma_\pi^2}
    \end{bmatrix}.
\end{equation}
$$&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Checkpoint&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Let us take a step back for a second, and roughly see what is on the table. Intuitively, we are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Observing&lt;/strong&gt; a materialized payoff $p_i$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inverting&lt;/strong&gt; $g$ to recover the latent skill gap $\hat\delta_{i, \, j}$ that was most likely to produce $p_i$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comparing&lt;/strong&gt; that inferred gap to our current belief of the skill gap $r_i - r_j$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deriving&lt;/strong&gt; the change to $r_i$ and $r_j$ would bring our belief closer to $\hat\delta_{i, \, j}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Then, the gradient-ascent update with step size $k$,&lt;/p&gt;
$$
\begin{equation}
    \bold{r}_{t + 1} 
    \gets 
    \bold{r}_{t} 
    + k\nabla_{\bold{r}}\log\mathcal{J}_\mathrm{MAP}(\theta_i, \, \theta_j; \, \hat\delta_{i, \, j}),
\end{equation}
$$&lt;p&gt;offers a complete recovery (and generalization) of the Elo update after an observed payoff $p_i$.&lt;/p&gt;
&lt;h3 id=&#34;discussion&#34;&gt;Discussion&lt;/h3&gt;
&lt;h4 id=&#34;procedural-discrepancy&#34;&gt;Procedural Discrepancy&lt;/h4&gt;
&lt;p&gt;Usually, implementations of Elo updates do not consider a prior. Instead, they simply initialize parameters at some default amount, then do MLE (as opposed to MAP estimation) to produce gradient updates. I decided to display the full MAP estimate because I think it is more principled; if you believe that ratings &amp;ldquo;start off&amp;rdquo; at some amount, that constitutes a bayesian prior in my eyes.&lt;/p&gt;
&lt;h4 id=&#34;distribution-discrepancy&#34;&gt;Distribution Discrepancy&lt;/h4&gt;
&lt;p&gt;The Elo rating system assumes a logistic distribution on player performance, not gaussian. However, the above procedure will invariantly recover Elo updates as presented in the &lt;a href=&#34;#elo-ratings-and-updates&#34;&gt;background section&lt;/a&gt; with both distributions (at least in form). I thought it would be somewhat interesting to make it gaussian.&lt;/p&gt;
&lt;h4 id=&#34;fixed-parameters&#34;&gt;Fixed Parameters&lt;/h4&gt;
&lt;p&gt;In theory, one could estimate the variance parameters using the exact same procedure, by taking the gradient of the joint likelihood with respect to them in addition to the means (the ratings). Surprisingly, people do things similar to this &amp;ndash; although not in this particular way. See the &lt;a href=&#34;https://en.wikipedia.org/wiki/Glicko_rating_system&#34;&gt;Glicko rating system&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;redundancy-with-g&#34;&gt;Redundancy with $g$&lt;/h4&gt;
&lt;p&gt;You may have noticed that throughout our derivations (most notably in equations $(7)$ and $(8)$) there are $g(\hat\delta_{i, \, j})$ terms that can be safely replaced with $p_i$ by definition, and can be therefore seen as redundant. This is a completely accurate observation.&lt;/p&gt;
&lt;p&gt;I decided to make $g$ explicit to make the fundamental link between payoffs and performance differentials also explicit, which is something I consider to be a lot more principled. In fact, $g$ does not need to be strictly monotonic, as we never explicitly evaluate $g^{-1}(\small\bullet)$. However, not satisfying this property may result in a lack of parameter identifiability, which is easy to forget if you discard the symbol early on.&lt;/p&gt;
&lt;h4 id=&#34;weak-ordering&#34;&gt;Weak Ordering&lt;/h4&gt;
&lt;p&gt;It is important to acknowledge that mapping player skill to $\mathbb{R}$ and then using $\leq$ to order players is a fundamentally misguided approach to how the world works. In doing so, we establish a weak ordering among players, but completely ignore that some players have qualities that make them strong against some players and weak against others (in a manner that is potentially cyclic).&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;To illustrate this, consider three players of rock-paper-scissors. One always plays rock, one always plays paper, and the other scissors. You will find that there is no way of assigning them a real number such that the player with the highest number beats both of the other players in expectation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Still, sometimes we are forced to make rankings which make sense &lt;em&gt;in expectation&lt;/em&gt;. In the real world, there is sufficient variance in player attributes that there are actors that can consistently beat some others. Here, systems such as Elo&amp;rsquo;s bring real utility. But as a human, you should trust your intuition more than some potentially senseless number.&lt;/p&gt;
&lt;h4 id=&#34;outcome-prediciton&#34;&gt;Outcome Prediciton&lt;/h4&gt;
&lt;p&gt;Further expanding on the inadequacy of ranking players via weak order, consider the very plausible machine learning task of outcome prediction, say, for the game of &lt;a href=&#34;https://en.wikipedia.org/wiki/Basketball&#34;&gt;Basketball&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is tempting to, for example, train a network $f_\theta : \mathbb{R}^n \to \mathbb{R}$ on $n$-dimensional encodings of teams to predict a scalar value, where you then train in tandem over historic game outcomes $\langle ((a, b), \, y)_i \rangle_{i \in D}$.&lt;/p&gt;
&lt;p&gt;Here, teams $a, \, b \in \mathbb{R}^n$ played each other and achieved outcome $y \in \{-1, 1\}$ for each match $i \in D$. One could optimize under the following loss,&lt;/p&gt;
$$
    \mathcal{L}((a, \, b), \, y; \, \theta) 
    = \log\exp\bigl( 1 + y(f_\theta(a) - f_\theta(b))\bigr) 
    + \lambda(f_\theta(a) + f_\theta(b)),
$$&lt;p&gt;where the regularization term helps with stability. Then, $f_\theta$ would essentially become a rating estimator. Whoever does this, however, will have the same fundamental problem as the Elo system; a weak order cannot capture the potentialy cyclic structure of actors&amp;rsquo; dominance on each other.&lt;/p&gt;
&lt;p&gt;The solution, of course, is to instead train another model $f_\theta : \mathbb{R}^{2n} \to \mathbb{R}$ that admits pairings as an input via concatentaion, and implements typical binary cross-entropy loss:&lt;/p&gt;
$$
    \mathcal{L}((a, \, b), \, y; \, \theta) 
    = -\bold{I}_y\,\log\bigl(\sigma(f_\theta(a \Vert b))\bigr) - (1 - \bold{I}_y)\,\log\bigl(1-\sigma(f_\theta(a \Vert b))\bigr).
$$&lt;p&gt;However, there is no free lunch &amp;ndash; when training over &lt;em&gt;pairs&lt;/em&gt; of teams in $T$, the sample space of the task grows with the size of $T \times T$, naturally increasing the amount of out-of-distribution data for your model quadratically. Of course, this problem was ignored by the first formulation too, just in a different way.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;gaussian-convolution&#34;&gt;Gaussian Convolution&lt;/h3&gt;
&lt;p&gt;Here, I justify equation $(6)$ by instantiating a proof of the fact that the convolution of two gaussians is another gaussian determined by the parameters of the original gaussians.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;This was made via ChatGPT with &lt;code&gt;o4-mini-high&lt;/code&gt; and adjusted by me, because you can probably find it in a textbook somewhere. Let&lt;/p&gt;
$$
    f(d)=\frac{1}{\sqrt{2\pi}\,\sigma_1}\exp\!\Bigl(-\frac{(d-\mu_1)^2}{2\,\sigma_1^2}\Bigr),
    \quad
    g(d)=\frac{1}{\sqrt{2\pi}\,\sigma_2}\exp\!\Bigl(-\frac{(d-\mu_2)^2}{2\,\sigma_2^2}\Bigr).
$$&lt;p&gt;We wish to show&lt;/p&gt;
$$
    \int_{-\infty}^{\infty} f(d)\,g(d)\,dd
    =\frac{1}{\sqrt{2\pi\,(\sigma_1^2+\sigma_2^2)}}\,
    \exp\!\Bigl(-\frac{(\mu_1-\mu_2)^2}{2\,(\sigma_1^2+\sigma_2^2)}\Bigr).
$$&lt;p&gt;Set $A=\sigma_1^2$ and $B=\sigma_2^2$. Then,&lt;/p&gt;
$$
    f(d)\,g(d)
    =\frac{1}{2\pi\sqrt{AB}}
    \exp\!\Bigl(-\tfrac12\bigl[\tfrac{(d-\mu_1)^2}{A}+\tfrac{(d-\mu_2)^2}{B}\bigr]\Bigr).
$$&lt;p&gt;Combine quadratic terms:&lt;/p&gt;
$$
    B(d-\mu_1)^2 + A(d-\mu_2)^2
    =(A+B)\Bigl(d-\frac{B\mu_1 + A\mu_2}{A+B}\Bigr)^2
    +\frac{AB}{A+B}(\mu_1-\mu_2)^2.
$$&lt;p&gt;Define&lt;/p&gt;
$$
    m=\frac{B\mu_1 + A\mu_2}{A+B},
    \quad
    C=\frac{AB}{A+B}.
$$&lt;p&gt;Then,&lt;/p&gt;
$$
    \int f(d)\,g(d)\,dd
    =\frac{1}{2\pi\sqrt{AB}}
    \int
    \exp\!\Bigl(-\tfrac12\bigl[\tfrac{(d-m)^2}{C}+\tfrac{(\mu_1-\mu_2)^2}{A+B}\bigr]\Bigr)
    \,dd.
$$&lt;p&gt;Factor out the constant term and use&lt;/p&gt;
$$
    \int \exp\Bigl(-\frac{(d-m)^2}{2C}\Bigr) \, dd
    =\sqrt{2\pi\,C}.
$$&lt;p&gt;Hence,&lt;/p&gt;
$$
\begin{aligned}
    \int f(d)\,g(d)\,dd
    &amp;=\frac{\sqrt{2\pi\,C}}{2\pi\sqrt{AB}}
    \exp\!\Bigl(-\frac{(\mu_1-\mu_2)^2}{2\,(A+B)}\Bigr)\\
    &amp;=\frac{1}{\sqrt{2\pi\,(A+B)}}
    \exp\!\Bigl(-\frac{(\mu_1-\mu_2)^2}{2\,(A+B)}\Bigr). \quad \square
\end{aligned}
$$&lt;h4 id=&#34;instantiation&#34;&gt;Instantiation&lt;/h4&gt;
&lt;p&gt;Consider the expression which $(6)$ was derived from,&lt;/p&gt;
$$
\begin{aligned}
    &amp;\int
       \frac{1}{\sqrt{2\pi}\,\sigma_\varepsilon}
       \exp\!
       \overbrace{
        \Bigl(-\frac{\bigl(g(\hat\delta_{i, \, j}) - d\bigr)^2}{2\,\sigma_\varepsilon^2}\Bigr)
       }^{
        \text{Quadratic term is symmetric.}
       }
       \;|g&#39;(\hat\delta_{i, \, j})|\; \\
    &amp;\quad\;\quad\;\times \frac{1}{\sqrt{2\pi(\sigma_i^2+\sigma_j^2)}}
       \exp\!\Bigl(-\frac{(d - (r_i - r_j))^2}{2(\sigma_i^2+\sigma_j^2)}\Bigr)
    \,dd.
\end{aligned}
$$&lt;p&gt;Now, use the substitutions&lt;/p&gt;
$$
    \mu_1 = g(\hat\delta_{i, \, j}),
    \quad
    \mu_2 = r_i - r_j,
    \quad
    A = \sigma_\varepsilon^2,
    \quad
    B = \sigma_i^2 + \sigma_j^2,
$$&lt;p&gt;and re-attach the Jacobian factor $|g^\prime(\hat\delta_{i, \, j})|$ to recover&lt;/p&gt;
$$
\mathcal{J}(\theta_i,\theta_j;\hat\delta_{i, \, j})
=\frac{|g&#39;(\hat\delta_{i, \, j})|}
      {\sqrt{2\pi\,(\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2)}}\,
  \exp\!\Bigl(-\frac{(g(\hat\delta_{i, \, j}) - (r_i - r_j))^2}
                       {2\,(\sigma_\varepsilon^2 + \sigma_i^2 + \sigma_j^2)}\Bigr).
$$&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;I modified a term in $e_p$ to exclude scaling factors, to make it look less crazy. These scaling factors make the resulting ratings quite practical by allowing one to make comparisons like &amp;ldquo;player $i$ is 10x better than $j$ if $i$&amp;rsquo;s rating is 400 points higher.&amp;rdquo;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Taking the $\log$ makes it easier to deal with multiple samples, as it turns the product in $(1)$ into a sum. But here, we only use one sample, so it is useless. However, tradition is important for learning.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Poem 1. &#34;Delia&#34;</title>
      <link>http://localhost:1313/poem-1.-delia/</link>
      <pubDate>Sun, 27 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/poem-1.-delia/</guid>
      <description>&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Madre que no me suelta el estandarte,
madre de selva que me cubre como yedra,
estás completamente marcada en mí;
me arrullas desde mi penumbra,
me susurras el nombre de Dios.

Ya quítame de aquí que me muero, madre,
dame las palabras que me corresponden,
sacude desde tu sigilo mi sangre
y hazme llegar tu amor.

Soledad eterna y vida corta,
nunca te me vayas a ir.

Madre que a ciegas todo lo ve;
corazón de parota, palabras de luz.
Eres la cumbre de este desierto,
autora del método mío;
eres madre de todo.

Águila de quinientas virtudes,
reguilete infinito de colores,
piedra de orgullo inexorable.

Árbol terrestre que toca el cielo;
sosiego inminente,
calor solar.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    
    
    <item>
      <title>N-Gram Model of Optimal Policy on Interpretable Abstractions</title>
      <link>http://localhost:1313/n-gram-model-of-optimal-policy-on-interpretable-abstractions/</link>
      <pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/n-gram-model-of-optimal-policy-on-interpretable-abstractions/</guid>
      <description>&lt;aside id=&#34;toc&#34;&gt;
    &lt;details&gt;
        &lt;summary&gt;&amp;nbsp;&lt;strong&gt; Table of contents&lt;/strong&gt;&lt;/summary&gt;
        &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#n-gram-modeling&#34;&gt;$N$-Gram Modeling&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#rules-of-thumb&#34;&gt;Rules of Thumb&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#markov-abstractions&#34;&gt;Markov Abstractions&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#model&#34;&gt;Model&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#training&#34;&gt;Training&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#sinks&#34;&gt;Sinks&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#remarks&#34;&gt;Remarks&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#explorations&#34;&gt;Explorations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
    &lt;/details&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Generally, the choice of functional form of a policy model and of the domain that it operates on forms the basis of interpretability. Domains that are the image of class-valued abstractions of the observable state space are desireable because humans excel at visual classification tasks that map onto (largely) discrete characteristics. Hence, we provide an interpretable functional form that is valid over multiclass spaces in the form of an $n$-gram model approximation of dynamics under optimal policy.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;n-gram-modeling&#34;&gt;$N$-Gram Modeling&lt;/h3&gt;
&lt;p&gt;$n$-gram models were developed as a rudimentary statistical model of language. Assuming an $n^{th}$-order &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_property&#34;&gt;Markov property&lt;/a&gt; on the probability of a word $w_{t + 1}$ at discrete time $t + 1$ given a history $\langle w_i \rangle_{i \in [1, \, t]}$,&lt;/p&gt;
$$
\begin{equation}
  P(w_1, \, \ldots, w_{t + 1}) =
  P(w_1, \dots, w_{t - n - 1}) \prod_{i = 0}^{n - 1} P(w_{t+1} \mid w_{t-n}, \dots, w_t),
\end{equation}
$$&lt;p&gt;straightforward &lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;&gt;maximum likelihood estimation&lt;/a&gt; shows that this probability is the proportion of times that the sequence $\langle w_{t-n}, \, \ldots, w_t \rangle$ appears before $w_{t + 1}$ in observations. This is can be seen as frequentist inference, making the probability measure intuitive.&lt;/p&gt;
&lt;p&gt;When applied to a set of symbols (words) $S$, such a model implies a Markov chain over the product $S^n = S \times \cdots \times S$. It follows that the chain&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_matrix&#34;&gt;stochastic matrix&lt;/a&gt; $\Pi$ is an element of $\mathbb{R}^{k \times k^n}$ with $k = |S|$, so the number of learnable parameters grows exponentially with the order of the model for a fixed $S$.&lt;/p&gt;
&lt;p&gt;As a result of upholding the Markov property, $n$-gram models are stationary&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This flaw makes them incompatible with natural language to any useful extent, and is directly addressed by modern language models through mechanisms like &lt;a href=&#34;https://en.wikipedia.org/wiki/Attention_(machine_learning)&#34;&gt;attention&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;rules-of-thumb&#34;&gt;Rules of Thumb&lt;/h3&gt;
&lt;p&gt;Many heuristics taught in strategic decision-making can be described to be conditionals on the result of classification exercises. For example, there is a rule of thumb in Chess which calls for protecting one&amp;rsquo;s own king if it is open.&lt;/p&gt;
&lt;p&gt;When implementing this heuristic, a player performs classification via a mapping $\phi : S \to \{\text{Yes}, \, \text{No}\}$ from the set of board states to an answer to the heuristic&amp;rsquo;s condition, where experience insists that if a player&amp;rsquo;s $\phi$ is sufficiently close to ground truth, they obtain a performance improvement in expectation.&lt;/p&gt;
&lt;p&gt;Naturally, the complexity involved in evaluating a classification $\phi_h(s)$ for some state $s \in S$ should be minimal so that its heuristic $h$ can be implemented without computer assistance. In many cases, their simplicity to humans (i.e., how intuitive they are) directly translates to the simplicity of implementing them in other models of computation. Put simply, it is generally easy to program such functions.&lt;/p&gt;
&lt;p&gt;However, humans can obtain an &lt;em&gt;unexplainable&lt;/em&gt; intuitive understanding of a game. In such cases, the classification exercises they carry out for their expert heuristics are mappings onto a set of abstract characteristics (e.g., area &amp;lsquo;crowdedness&amp;rsquo; in Chess). This can be seen as &lt;a href=&#34;https://en.wikipedia.org/wiki/Feature_learning&#34;&gt;representation learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But even in these cases, it is relatively simple to train a model which replicates a human&amp;rsquo;s capacity to perform classification for their own expert-level heuristics by having them label training datasets by hand. Hence, one can generally assume access to efficient classifiers for human-interpretable features.&lt;/p&gt;
&lt;h3 id=&#34;markov-abstractions&#34;&gt;Markov Abstractions&lt;/h3&gt;
&lt;p&gt;Given an abstraction $\phi : S \to Z$ over a state set $S$, the lack of an injectivity constraint could produce a situation where, for a policy $\pi_S : S \to S$ with $\pi(s) = a$ and $\pi(s^\prime) = b$ on distinct $a, \, b, \, s, \, s^\prime \in S$,&lt;/p&gt;
$$
\begin{equation}
  \phi(s) = \phi(s^\prime) \;\; \text{and} \;\; \phi(a) \neq \phi(b).
\end{equation}
$$&lt;p&gt;Hence, learning a counterpart $\pi_Z : Z \to Z$ which preserves the information in $\pi_S$ could be impossible, as $\pi_Z(\alpha(s)) = \pi_Z(\alpha(s^\prime))$ would have to &amp;lsquo;retain&amp;rsquo; the information of both $\pi_S(s) = a$ and $\pi_S(s^\prime) = b$. Such an abstraction $\phi$ is said to not be Markov, as its image contains insufficient information to induce a dynamics that corresponds to the behavior specified by $\pi_S$ in $S$.&lt;/p&gt;
&lt;p&gt;In the context of interpretable rules of thumb, reducing the state space $S$ to significantly smaller abstract spaces (e.g., taking decision in $\{\text{Yes}, \, \text{No}\}$ while implementing a heuristic) nearly guarantees that the abstraction which mediated the reduction is not Markov&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Let $\langle \phi^{(\alpha)} : S \to Z^{(\alpha)} \rangle_{\alpha \in \Alpha}$ be a collection of abstractions enumerated in $\Alpha$, and $\pi_S : S \to S$ a policy over $S$. We propose modeling class-conditional transition probability distributions,&lt;/p&gt;
$$
\begin{equation}
  P^{(\alpha)}_{t+1}(k) = P[\phi^{(\alpha)}(\pi^{t + 1}(s)) = k \; | \; \phi^{(\alpha)}(\pi^t(s)) = k_t, \, \ldots, \, \phi^{(\alpha)}(\pi^0(s)) = k_0],
\end{equation}
$$&lt;p&gt;of the elements $k_i \in Z^{(\alpha)}$ via an $n$-gram model. This effectively establishes sequences in $\phi^{(\alpha)}(S)$ via repeated aplication of $\pi$ within $S$ (following the dynamics of $\pi$), so that in the above equation, we allow $\pi^t(s) = \pi_t(\pi_{t-1}(\ldots\pi_1(s)))$.
This yields a collection of stochastic matrices $\langle  \Pi^{(\alpha)} \rangle_{\alpha \in \Alpha}$ with&lt;/p&gt;
$$
\Pi^{(\alpha)}_{i, j} = P[\, i \text{ is observed at time } t \; | \; j \text{ is observed immediately before}\,],
$$&lt;p&gt;where $i \in Z^{(\alpha)}$ and $j \in (Z^{(\alpha)})^n$. The amount of learnable parameters (i.e., the size) of such a model $M = \langle  \Pi^{(a)} \rangle_{\alpha \in \Alpha}$ is therefore&lt;/p&gt;
$$
\begin{equation}
  |M| = \sum_{\alpha \in \Alpha} |Z^{(\alpha)}|^n \, (|Z^{(\alpha)}| - 1).
\end{equation}
$$&lt;p&gt;The finalized abstract policy $\pi_Z$ would use this model to operate on $Z = \large{\times_{\alpha \in \Alpha}} Z^{(\alpha)}$ (see the &lt;a href=&#34;#inference&#34;&gt;inference section&lt;/a&gt; for a high-level overview of evaluation). By operating on the cross-product of multiple sufficiently independent heuristics, $\pi_Z$ could closely approximate $\pi_S$ while remaining interpretable.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;The parameter space for a model $M$ of order $n$ is precisely&lt;/p&gt;
$$
\begin{equation}
  \Theta =
  \large{\times_{\alpha \in \Alpha}}
  \large{\times_{k \in Z^{(\alpha)}}}
  \bold{S}^{|Z^{(\alpha)}|^n},
\end{equation}
$$&lt;p&gt;(where $\bold{S}^d$ denotes the $d$-dimensional unit sphere). Finding optimal parameters $\theta^* \in \Theta$ follows standard procedure as in any $n$-gram model. Hence, we simply provide the generic closed-form solution written in terms of the objects at hand,&lt;/p&gt;
$$
\begin{equation}
 \Pi^{(\alpha)}_{i, j} = \frac{1}{N}
 \sum_{s \in S}
  I^{(\alpha)}_{i,j}(\pi^n(s), \langle \pi^i(s) \rangle_{i \in [0, \, n)}),
\end{equation}
$$&lt;p&gt;where&lt;/p&gt;
$$
\begin{equation*}
 I^{(\alpha)}_{i,j}(a, \langle b_i \rangle_{i \in [0, \, n)}) =
 \begin{cases}
       1 &amp; \text{if } \; \phi^{(\alpha)}(a) = i \; \text{ and } \; \phi^{(\alpha)}(b) = j, \\
       0 &amp; \text{otherwise},
  \end{cases}
\end{equation*}
$$&lt;p&gt;and $N$ is the number of length-$(n + 1)$ contiguous subsequences in the dynamics of $\pi$, which can be easily sketched while computing the sum in $(5)$.&lt;/p&gt;
&lt;h3 id=&#34;sources&#34;&gt;Sources&lt;/h3&gt;
&lt;p&gt;The nature of the policy operator $\pi$ is such that there exists some $s \in S$ wihtout an $s^\prime$ with $\pi(s^\prime) = s$. Here, $s$ is called a source within the dynamics of $\pi$. This constitutes a problem, as the start $s_0$ of the game for which $S$ is a state space is necessarily a source (which may not be unique); therefore, an attempt to find an $n$-length sequence of moves leading up to a state less than $n$ applications of $\pi$ away from a source in its dynamics may fail.&lt;/p&gt;
&lt;p&gt;This is important because it is a step necessary to compute the $\Pi^{(\alpha)}_{i, j}$$^{\text{th}}$ parameter of the model, where $i$ is the parameter that is too close to a source to have a valid $n$-gram history. A solution which does not significantly alter transition distributions of $\Pi^{(\alpha)}$ is to sample missing elements of $n$-gram histories from a uniform distribution while computing its entries. If this measure is taken, $N$ can be set to $|S|$ in $(5)$, avoiding the need for sketching proportions.&lt;/p&gt;
&lt;h3 id=&#34;sinks&#34;&gt;Sinks&lt;/h3&gt;
&lt;p&gt;In many traditional definitions of a policy $\pi$, there may exist elements $s^\prime_i$ of $S$ over which $\pi$ is not defined, as they are terminal in the game under representation. These are sinks in the dynamics of $\pi$, and should never be considered as part of a history while computing model parameters.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;When at a state $s \in S$, a human player can consider the set of next possible states $t(s)$ (where the transition function $t : S \to \mathcal{P}(S)$ is set-valued). Optimally, combinatorial optimization would be done across all elements $s^\prime \in t(s)$ under the MLE objective of maximizing the probability that their action is observed across all abstract state space transitions.&lt;/p&gt;
&lt;p&gt;While this is possible to an extent due to the simplicity of the abstractions in consideration (which map onto small sets of classes, reducing maximization objectives during MLE), the true value of the model is in the subjective analysis of each $\Pi^{(\alpha)}$. Additionally, quantitative techniques (such as finding the static distribution and convergence rate of these matrices) may illustrate interpretable patterns in the dynamics of $\pi$, depending on $\langle \phi^{(\alpha)} \rangle$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;remarks&#34;&gt;Remarks&lt;/h2&gt;
&lt;p&gt;Establishing an approximation of optimal policy in the form of a Markov process provides an interpretable functional representation that is able to work with intuitive abstractions. Thus, it is a valid representation of a praxis, and the above methods effectively &amp;rsquo;translate&amp;rsquo; from policies of arbitrary form.&lt;/p&gt;
&lt;h3 id=&#34;explorations&#34;&gt;Explorations&lt;/h3&gt;
&lt;p&gt;The following are left as potential avenues of analysis relating to the model family.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smoothing techniques, and an analysis of their benefit in the context of optimal policy.&lt;/li&gt;
&lt;li&gt;Non-interpretability of $n$-gram model successors; in particular transformer attention.&lt;/li&gt;
&lt;li&gt;Skip-gram models as an extension of this family.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;credits&#34;&gt;Credits&lt;/h2&gt;
&lt;p&gt;Thank you to my good friend Humberto Gutierrez for spending late nights discussing the concept of policy abstraction with me, and helping me organize many ideas about policies over continuous abstractions.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;A stationary model&amp;rsquo;s probability assignments are invariant with respect to shifts in the time index.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Which is a way of saying that rules of thumb are not globally applicable.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Representation Concepts in Game-Theoretic Systems</title>
      <link>http://localhost:1313/representation-concepts-in-game-theoretic-systems/</link>
      <pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/representation-concepts-in-game-theoretic-systems/</guid>
      <description>&lt;aside id=&#34;toc&#34;&gt;
    &lt;details&gt;
        &lt;summary&gt;&amp;nbsp;&lt;strong&gt; Table of contents&lt;/strong&gt;&lt;/summary&gt;
        &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#materials&#34;&gt;Materials&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#errata&#34;&gt;Errata&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#game-theory&#34;&gt;Game theory&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#computer-science&#34;&gt;Computer science&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#representation&#34;&gt;Representation&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#rulesets&#34;&gt;Rulesets&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#abstraction&#34;&gt;Abstraction&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#design&#34;&gt;Design&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#interface-items&#34;&gt;Interface items&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#parallel-dp&#34;&gt;Parallel DP&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#meta-content&#34;&gt;Meta-content&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
    &lt;/details&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;I gave an introductory talk about how computer systems represent, compute, and store noteworthy attributes about a particular class of games. This was part of &lt;a href=&#34;http://kyleburke.info/sprouts/&#34;&gt;Sprouts &amp;lsquo;24&lt;/a&gt;, an undergraduate-oriented conference primarily dedicated to combinatorial game theory.&lt;/p&gt;
&lt;p&gt;Here, I share the materials I used during my presentation and share a longer-form (but very different) exploration of the topic I covered. Generically, it can be useful for all problems where one must run a domain-specific algorithm on a graph that is not materialized in memory, but can be traversed in linear time from a starting node and a set of functions that derive adjacent edges and nodes from existing ones (a so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/Implicit_graph&#34;&gt;implicit graph&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As a concrete case of this abstract class of problems, I present concepts that support the process of finding a Nash Equilibrium for a specific subclass of games through cousins of the minimax algorithm. However, these concepts are also applicable to other such problems (e.g., the membership problem&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; for decidable subclasses of context-free grammars).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;materials&#34;&gt;Materials&lt;/h2&gt;
&lt;p&gt;The slides I used during my talk can be found below. Anyone can use them without my permission.&lt;/p&gt;
&lt;div class=&#34;embed-pdf-container&#34; id=&#34;embed-pdf-container-de1ff50c&#34;&gt;
    &lt;div class=&#34;pdf-loadingWrapper&#34; id=&#34;pdf-loadingWrapper-de1ff50c&#34;&gt;
        &lt;div class=&#34;pdf-loading&#34; id=&#34;pdf-loading-de1ff50c&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;canvas class=&#34;pdf-canvas&#34; id=&#34;pdf-canvas-de1ff50c&#34;&gt;&lt;/canvas&gt;
&lt;/div&gt;

&lt;div class=&#34;pdf-paginator&#34; id=&#34;pdf-paginator-de1ff50c&#34;&gt;
    &lt;button id=&#34;pdf-prev-de1ff50c&#34;&gt;Previous&lt;/button&gt;
    &lt;button id=&#34;pdf-next-de1ff50c&#34;&gt;Next&lt;/button&gt; &amp;nbsp; &amp;nbsp;
    &lt;span&gt;
      &lt;span class=&#34;pdf-pagenum&#34; id=&#34;pdf-pagenum-de1ff50c&#34;&gt;&lt;/span&gt; / &lt;span class=&#34;pdf-pagecount&#34; id=&#34;pdf-pagecount-de1ff50c&#34;&gt;&lt;/span&gt;
    &lt;/span&gt;
    &lt;a class=&#34;pdf-source&#34; id=&#34;pdf-source-de1ff50c&#34; href=&#34;http://localhost:1313/pdf/slides-sprouts-2024.pdf&#34;&gt;[pdf]&lt;/a&gt;
&lt;/div&gt;

&lt;noscript&gt;
View the PDF file &lt;a class=&#34;pdf-source&#34; id=&#34;pdf-source-noscript-de1ff50c&#34; href=&#34;http://localhost:1313/pdf/slides-sprouts-2024.pdf&#34;&gt;here&lt;/a&gt;.
&lt;/noscript&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
    (function(){
    var url = &#39;\/pdf\/slides-sprouts-2024.pdf&#39;;

    var hidePaginator = &#34;&#34; === &#34;true&#34;;
    var hideLoader = &#34;&#34; === &#34;true&#34;;
    var selectedPageNum = parseInt(&#34;&#34;) || 1;

    
    var pdfjsLib = window[&#39;pdfjs-dist/build/pdf&#39;];

    
    if (pdfjsLib.GlobalWorkerOptions.workerSrc == &#39;&#39;)
      pdfjsLib.GlobalWorkerOptions.workerSrc = &#34;http:\/\/localhost:1313\/&#34; + &#39;js/pdf-js/build/pdf.worker.js&#39;;

    
    var pdfDoc = null,
        pageNum = selectedPageNum,
        pageRendering = false,
        pageNumPending = null,
        scale = 3,
        canvas = document.getElementById(&#39;pdf-canvas-de1ff50c&#39;),
        ctx = canvas.getContext(&#39;2d&#39;),
        paginator = document.getElementById(&#34;pdf-paginator-de1ff50c&#34;),
        loadingWrapper = document.getElementById(&#39;pdf-loadingWrapper-de1ff50c&#39;);


    
    showPaginator();
    showLoader();

    

    function renderPage(num) {
      pageRendering = true;
      
      pdfDoc.getPage(num).then(function(page) {
        var viewport = page.getViewport({scale: scale});
        canvas.height = viewport.height;
        canvas.width = viewport.width;

        
        var renderContext = {
          canvasContext: ctx,
          viewport: viewport
        };
        var renderTask = page.render(renderContext);

        
        renderTask.promise.then(function() {
          pageRendering = false;
          showContent();

          if (pageNumPending !== null) {
            
            renderPage(pageNumPending);
            pageNumPending = null;
          }
        });
      });

      
      document.getElementById(&#39;pdf-pagenum-de1ff50c&#39;).textContent = num;
    }

    

    function showContent() {
      loadingWrapper.style.display = &#39;none&#39;;
      canvas.style.display = &#39;block&#39;;
    }

    

    function showLoader() {
      if(hideLoader) return
      loadingWrapper.style.display = &#39;flex&#39;;
      canvas.style.display = &#39;none&#39;;
    }

    

    function showPaginator() {
      if(hidePaginator) return
      paginator.style.display = &#39;block&#39;;
    }

    

    function queueRenderPage(num) {
      if (pageRendering) {
        pageNumPending = num;
      } else {
        renderPage(num);
      }
    }

    

    function onPrevPage() {
      if (pageNum &lt;= 1) {
        return;
      }
      pageNum--;
      queueRenderPage(pageNum);
    }
    document.getElementById(&#39;pdf-prev-de1ff50c&#39;).addEventListener(&#39;click&#39;, onPrevPage);

    

    function onNextPage() {
      if (pageNum &gt;= pdfDoc.numPages) {
        return;
      }
      pageNum++;
      queueRenderPage(pageNum);
    }
    document.getElementById(&#39;pdf-next-de1ff50c&#39;).addEventListener(&#39;click&#39;, onNextPage);

    

    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
      pdfDoc = pdfDoc_;
      var numPages = pdfDoc.numPages;
      document.getElementById(&#39;pdf-pagecount-de1ff50c&#39;).textContent = numPages;

      
      if(pageNum &gt; numPages) {
        pageNum = numPages
      }

      
      renderPage(pageNum);
    });
    })();
&lt;/script&gt;

&lt;h3 id=&#34;errata&#34;&gt;Errata&lt;/h3&gt;
&lt;p&gt;Here are the mistakes I have found in the slides:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In slide 10, the first bullet point should also restrain the set of games under consideration to be extensive-form and non-collaborative, as implied by the subsequent definition in slides 11-15.&lt;/li&gt;
&lt;li&gt;In slide 20, the formulation $\langle N, S, p, u \rangle$ should also include a transition function $t : S \to \mathcal{P}(S)$, where if $s$ is a state corresponding to history $h$, then the history $h&amp;rsquo;$ corresponding to $s&amp;rsquo;$ should be the same as $h$ with an additional action appended for all $s&amp;rsquo; \in t(s)$.&lt;/li&gt;
&lt;li&gt;In slide 28, the partition should not necessarily minimize the sum of the conductance of all cuts that produce the partition. Instead, the ideal partition would be a solution to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_partition#:~:text=%5B4%5D-,Problem,-%5Bedit%5D&#34;&gt;balanced partition problem&lt;/a&gt;, where optimal parameters are determined from hardware-related constraints (such as the cost of inter-process communication). The goal is to balance parallelism with its own overhead.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In the interest of accessibility, I will briefly cover useful basics in game theory and computer science that seldom find their way into students&amp;rsquo; syllabi or are otherwise worth refreshing. If you think you can safely skip this, you are probably right.&lt;/p&gt;
&lt;h3 id=&#34;game-theory&#34;&gt;Game theory&lt;/h3&gt;
&lt;p&gt;The generic setup of a game is some amount of &amp;ldquo;players&amp;rdquo; taking actions according to their own interests or preferences, potentially affecting other players in the process. From the point of view of a single player, a game is an optimization problem that seeks to find an &amp;ldquo;optimal strategy&amp;rdquo; from the information available to them. This is an assumption known as &amp;ldquo;individual rationality&amp;rdquo; that pervades most of game theory.&lt;/p&gt;
&lt;p&gt;But from a global point of view, there is no obvious question to ask about a game. This is why games are not problems; they are situations that we can ask different questions about. But &lt;em&gt;per se&lt;/em&gt;, games are not aching to be solved. To ask specific questions with some hope of rigor, a lot of effort has been placed into defining classes of games that posses different characteristics.&lt;/p&gt;
&lt;h4 id=&#34;taxonomy-of-games&#34;&gt;Taxonomy of games&lt;/h4&gt;
&lt;p&gt;There are a lot of classes of games. They are separated by the mathematical properties of their setting and participants, among other factors.&lt;/p&gt;
&lt;p&gt;There is no global dictionary or atlas for game classes, as interpretations can become nuanced to the extent of opinion. Hence, anytime someone makes a statement in game theory they must specify the class of games it targets. In this article, we will target games that are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Perfect-information.&lt;/strong&gt; Here, all players know everything in the universe that could possibly help them make or avoid any decision, except the decisions that other players will make. Most forms of Poker are not perfect-information, as the exact location of the cards is unknown.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deterministic.&lt;/strong&gt; Here, if all players choose a strategy and never change it, there is only one possible outcome for the game. Chess is deterministic, because if players make the exact same moves in two different games they are guaranteed to achieve the same result.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sequential.&lt;/strong&gt; More intuitive superset of &lt;a href=&#34;https://cdn.nba.com/headshots/nba/latest/1040x760/2544.png&#34; style=&#34;color: inherit; text-decoration: none&#34;&gt;extensive-form games&lt;/a&gt;. Here, all actions that players can take are indivisible. Soccer is not discrete, because players&amp;rsquo; movements constitute their actions and it is possible to divide any movement into a shorter one.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Extensive-form.&lt;/strong&gt; The adjective &amp;ldquo;extensive-form&amp;rdquo; refers to games that can be written in extensive form, which is a kind of mathematical template. Here, games are defined in terms of the histories of actions that could be observed by the players, and the preferences each player has among them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;solution-concepts&#34;&gt;Solution concepts&lt;/h4&gt;
&lt;p&gt;There are some questions that are so broadly-applicable in terms of the classes of games they can target that they achieve the special status of a &lt;strong&gt;solution concept.&lt;/strong&gt; This is a term that refers to a characteristic can be observed in a useful set of game classes.&lt;/p&gt;
&lt;p&gt;A very human thing to ask about broad categories of games is who will win. As it turns out, there is no real answer to this question most of the time, because it can come across obstacles like chance, incomplete information, and lack of clarity around the word &amp;ldquo;win.&amp;rdquo; An equally important yet more applicable question, however, is what strategy each player should take to achieve the best possible result for themselves.&lt;/p&gt;
&lt;p&gt;In many cases, it is necessary to tack on additional nuances to this question to be able to answer it. One such refinement of the question (which revolutionized economics) asks which strategy each player should adopt so that no single player could change their own and benefit from it. A pairing of players to strategies is known as a &lt;strong&gt;strategy profile&lt;/strong&gt;, and those that satisfy the above property are known as &lt;strong&gt;Nash Equilibria&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The strategies and strategy profiles that allow players to act probabilistically are called &lt;strong&gt;mixed&lt;/strong&gt;. Mixed strategies are tantamount to sampling &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_distribution&#34;&gt;probability distributions&lt;/a&gt; of &lt;strong&gt;pure strategies&lt;/strong&gt;, which themselves specify deterministic actions. In 1950, John Nash defined the concept of a Nash Equilibrium (NE), additionally proving that there exists such a mixed strategy profile in all games &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Note&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Nash Equilibrium is an overloaded term, as it refers to both a solution concept and a strategy profile that satisfies the solution concept. You will need to tell which is which from context.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;computer-science&#34;&gt;Computer science&lt;/h3&gt;
&lt;p&gt;The possibility of players taking actions simultaneously (among other things) can make the existence of a pure-strategy NE impossible. But if sequential play is assumed, it is straightforward to show that there always exists a pure-strategy NE &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. As mentioned above, this article benefits from this assumption.&lt;/p&gt;
&lt;p&gt;Because finding a NE is such a popular desire, most of the discussion here will focus on the procedure of finding a pure-strategy NE in the class of games we specified previously. This is a costly process, which is why it calls for techniques that help minimize use of computational resources. However, you will notice that the things that make this an inherently costly process for some games are actually factors that have nothing to do with game theory.&lt;/p&gt;
&lt;p&gt;Hence, it is possible that the concepts I will discuss are applicable beyond the problem of finding a pure-strategy NE. To elaborate, a maximally generic yet snobby version of this article would perhaps be titled &lt;em&gt;Techniques for Implementing Solutions to Search Problems on Implicit Graphs&lt;/em&gt;. The meanings of these terms are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implementing.&lt;/strong&gt; Bring into the real world.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Solutions.&lt;/strong&gt; In this context, algorithms that solve problems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search Problem.&lt;/strong&gt; A problem that asks you to find something.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implicit Graph.&lt;/strong&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_(abstract_data_type)&#34;&gt;Graph&lt;/a&gt; representation in terms of an initial element and a collection of functions which allow you to perform a traversal. Useful but unconventional term.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In particular, the section titled &amp;ldquo;Representation&amp;rdquo; will explain the link between the definition of an extensive game and the representation of its structure as an implicit graph, and will introduce a trick that can be used to end up with a significantly simpler traversals. This trick is also applicable to problem domains other than games, but I only present it with regard to games because its implementation depends on the underlying problem. Everything else is applicable as soon as you have an implicit graph in your hands.&lt;/p&gt;
&lt;p&gt;While explaining these concepts, it will be useful to have access to ideas in complexity theory. Below are some domain-specific remarks and definitions introducing language that will be of relevance later.&lt;/p&gt;
&lt;h4 id=&#34;complexity-theory&#34;&gt;Complexity theory&lt;/h4&gt;
&lt;p&gt;In computer science, complexity is a measure&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; of the minimal number of elementary operations that must be composed to complete a target operation. The relevant elementary operations correspond to the kind of complexity in question:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Time complexity.&lt;/strong&gt; Here, elementary operations are other operations whose time is assumed to be a known constant. Elementary operations should always be specified.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Space complexity.&lt;/strong&gt; Here, the elementary operation is setting a bit. A more formal definition of space complexity depends on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Model_of_computation&#34;&gt;model of computation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the sake of expressibility, complexity is usually expressed in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/Asymptotic_analysis&#34;&gt;asymptotic characteristics&lt;/a&gt;. In particular, symbolisms like &lt;a href=&#34;https://web.mit.edu/16.070/www/lecture/big_o.pdf&#34;&gt;Big-O notation&lt;/a&gt; help compare the asymptotic complexity of different algorithms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Computational_problem&#34;&gt;Computational problems&lt;/a&gt; can be put into &lt;a href=&#34;https://en.wikipedia.org/wiki/Complexity_class&#34;&gt;complexity classes&lt;/a&gt;. For example, the problem of finding a mixed-strategy NE, known as $\text{N}\small{\text{ASH}}$, is in the time complexity class &lt;a href=&#34;https://en.wikipedia.org/wiki/FNP_(complexity)&#34;&gt;$\text{FNP}$&lt;/a&gt;, which has led many people to look for other solution concepts that are more computationally favorable&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. The equivalent problem for the class of games we are considering, however, is in &lt;a href=&#34;https://en.wikipedia.org/wiki/FP_(complexity)&#34;&gt;$FP$&lt;/a&gt;. In other words, $\text{N}\small{\text{ASH}}$ can be solved &lt;a href=&#34;https://en.wikipedia.org/wiki/Algorithmic_efficiency&#34;&gt;efficiently&lt;/a&gt; on this restricted domain of games&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representation&#34;&gt;Representation&lt;/h2&gt;
&lt;p&gt;One can find solutions to instances of many search and decision problems over games without incurring large computational expenses. This is possible by deriving a logical analysis on a case-by-case basis, using the mathematical properties of the components of the game in question.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;An &lt;strong&gt;extensive-form game&lt;/strong&gt; is a 4-tuple $\langle N, H, p, (\succsim_i) \rangle$, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N$ is a set of players, usually $\{1, \; \ldots, \; n\}$ for simplicity.&lt;/li&gt;
&lt;li&gt;$H$ is a set of strings of actions where $h \in H \implies h&amp;rsquo; \in H$ where $h&amp;rsquo;$ is the string resulting from removing the last action from the string $h$.&lt;/li&gt;
&lt;li&gt;$p : H \to N$ assigns a player to each non-terminal history.&lt;/li&gt;
&lt;li&gt;The player $i \in N$ has a preference relation $\succsim_i$ on the set $Z \subseteq H$ of terminal histories (which is reflexive and transitive).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For a game provided in the extensive form, its instantiation of the above abstractions can be logically leveraged to prove statements about the game. But given that it can be defined arbitrarily, sometimes it is impossible to achieve this solely through formal rewriting.&lt;/p&gt;
&lt;p&gt;In some of these cases it is possible to simply expand the component definitions into their explicit forms in order to later compute a solution using these expansions. But of course, doing this can also be extremely impractical. For example, it is common for $H$ to be of very high cardinality.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Consider a &lt;a href=&#34;https://en.wikipedia.org/wiki/Rubik%27s_Cube&#34;&gt;Rubik&amp;rsquo;s Cube&lt;/a&gt; that is initialized to specific starting colors, which can be set into the extensive form via the following instantiations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N = \{1\}$.&lt;/li&gt;
&lt;li&gt;$H = \{ h \; | \; h \text{ is a sequence of 90° rotations of a plane in the cube} \}$.&lt;/li&gt;
&lt;li&gt;$p : h \mapsto 1$ for all $h \in H$.&lt;/li&gt;
&lt;li&gt;$h_i \succsim_1 h_j$ for all $h_i$ that leave the cube solved.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, the set $H$ is countably infinite, so it is impossible to expand it to its elements in order to later compute a property of this puzzle. One such property is the smallest number of actions that can solve the cube&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;rulesets&#34;&gt;Rulesets&lt;/h3&gt;
&lt;p&gt;Before introducing tools to deal with this, there is another representation that is common when dealing with &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_strategy_game&#34;&gt;abstract strategy games&lt;/a&gt;. A ruleset, as it is known in &lt;a href=&#34;https://en.wikipedia.org/wiki/Combinatorial_game_theory&#34;&gt;combinatorial game theory&lt;/a&gt;, is the most familiar kind of representation of a game.&lt;/p&gt;
&lt;p&gt;In particular, a ruleset specifies exactly what actions are permitted to whom and when. It also explains the &lt;a href=&#34;https://en.wikipedia.org/wiki/Utility&#34;&gt;utility&lt;/a&gt; obtained by each participating player when no further actions are available. These characteristics are expressed in terms of the mutable state of a proxy (e.g., a board with pieces).&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;The game &lt;code&gt;10-to-0-by-1-or-2&lt;/code&gt; is generated by the following ruleset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a collection of 10 items.&lt;/li&gt;
&lt;li&gt;2 players take alternating turns removing either 1 or 2 items from the collection.&lt;/li&gt;
&lt;li&gt;Player 1 starts. The player who takes the last item from the collection wins.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this game, the collection of items that is mutated by players&amp;rsquo; actions is a proxy that allows players to judge what they are allowed to do and to determine who wins the game.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This way, the information contained in the state of a proxy is a representation of the history of actions that produced it. These representations are called &lt;strong&gt;game states&lt;/strong&gt;. This makes rulesets implicit graphs over the set of game states (denoted $S$). While they do not act directly over a set of histories, rulesets include enough information to generate an equivalent extensive-form representation. The resulting structure of generated action histories is hence intimately tied to the nature of the proxy.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Given a directed graph $G = \langle S, E \rangle$, the corresponding &lt;strong&gt;implicit graph&lt;/strong&gt; $G^I$ is a 3-tuple $\langle S, t, s_0 \rangle$, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S$ is a set of states.&lt;/li&gt;
&lt;li&gt;$s_0 \in S$ is a starting element.&lt;/li&gt;
&lt;li&gt;$t : S \to \mathcal{P}(S)$ is a transition dynamics function with $(s_i, s_j) \in E \iff s_j \in t(s_i)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A proof of the bijection between implicit and directed graphs is omitted.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Furthermore, because many actions could be globally or locally commutative with respect to proxies&amp;rsquo; mutable state, sets of histories in the extensive form are usually of much higher cardinality than sets of possible states for rulesets&amp;rsquo; proxies. This is of course computationally favorable, as finite ruelesets (whose proxies have a finite number of possible states) can generate even infinite extensive forms.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;

&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;style&gt;
    .book-column &gt; :first-child:not(.halign-container, .valign-container) {
        margin-top: 0px !important;
    }

    .book-column &gt; :last-child:not(.halign-container, .valign-container) {
        margin-bottom: 0px !important;
    }
&lt;/style&gt;
&lt;div style=&#34;display: flex; flex-wrap: flex&#34;&gt;
       
    &lt;div class=&#34;book-column&#34; style=&#34;flex: 9;&#34;&gt;
        &lt;p&gt;In the diagram to the right, let $S_0$, $S_1$, $S_2$, and $S_3$ be allowed states under a ruleset, and $A = \{a, b\}$ allowed actions. We have that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The set of histories is $H = \{ \epsilon, a, b, ab, ba \}$.&lt;/li&gt;
&lt;li&gt;The set of states is $S = \{ S_0, S_1, S_2, S_3 \}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see, $|H| &amp;gt; |S|$ despite how $|A| &amp;lt; |S|$. The difference in size between $H$ and $S$ scales rapidly in the general case.&lt;/p&gt;

    &lt;/div&gt;
     
    &lt;div class=&#34;book-column&#34; style=&#34;flex: 5;&#34;&gt;
        &lt;style&gt;
    .valign-container {
        display: flex;
        height: 100%;
        align-items: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;valign-container&#34;&gt;
&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;commutative_diagram.png&#34; width=&#34;128&#34;/&gt; 
&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;

    &lt;/div&gt;
    
&lt;/div&gt;


&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;abstraction&#34;&gt;Abstraction&lt;/h3&gt;
&lt;p&gt;In real life, games come mostly in the form of rulesets. We are usually aware of the environment they transpire in (the so-called intuitive proxy) and the laws that describe how it can change as a function of players&amp;rsquo; actions. Because of this, much of applied theory is centered around semantics that involve mutable state. For example, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_decision_process&#34;&gt;Markov Decision Process (MDP)&lt;/a&gt; from &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;reinforcement learning&lt;/a&gt; strongly reflects the nature of a ruleset.&lt;/p&gt;
&lt;p&gt;However, all of these constructs have a latent yet equivalent extensive form representation. This will motivate the technique of &lt;strong&gt;state abstraction&lt;/strong&gt;&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;: The necessity for action histories to be directly prefixed implies that they have a directed tree structure, and because all directed graphs have an equivalent implicit graph representation, the relationship between action histories and ruleset states maps an implicit graph to another one that retains important information about the original.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Definition&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;An &lt;strong&gt;abstraction map&lt;/strong&gt; $a : S_{\text{pre}} \to S_{\text{post}}$ maps the states in an implicit graph $G^I_{\text{pre}}$ to the states in another implicit graph $G^I_{\text{post}}$ with the structure-preserving condition&lt;/p&gt;
$$ s_j \in t_{\text{pre}}(s_i) \iff a(s_j) \in t_{\text{post}}(a(s_i)). $$&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As shown in the last example, the set of action histories is usually of much greater cardinality than its corresponding set of ruleset states, with the difference being possibly infinite. Altogether, this is a hint that, much like the implicit jump from action histories to ruleset states, it is possible to jump to more abstract state sets of smaller cardinality but equal representational power.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;

&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;

&lt;p&gt;An abstraction mapping between the action histories (left) and states (right) from the previous example. Notice how action histories always imply a tree, and the process of abstraction folds it into a potentially cyclic graph.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt; &lt;!-- This is for rendering the above as a &lt;p&gt; element --&gt;&lt;/p&gt;



&lt;style&gt;
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
&lt;/style&gt;
&lt;div class=&#34;halign-container&#34;&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;abstraction.png&#34; width=&#34;475&#34;/&gt; 
&lt;/figure&gt;

&lt;/div&gt;


&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Therefore, much like imposing a ruleset proxy causes many of the action histories to fold into equivalent states, we can determine for a problem $\text{P}$ which states would be $\text{P}$-equivalent under the ruleset&amp;rsquo;s laws. This way, we can create new abstractions $a_i : S_{i - 1} \to S_i$ that can be composed to obtain a reduced state space that is equivalent to the original under $\text{P}$ for a significant computational upside.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Consider the ruleset underlying the game of Tic-Tac-Toe. Denote $B_i$ a board state. Then, any algorithm that computes a NE for this game through its ruleset is invariant to the abstraction $a$, where&lt;/p&gt;
$$ a(B_i) = a(B_j) \iff B_i \text{ is symmetrical to } B_j. $$&lt;p&gt;Further, the number of board states this algorithm will need to visit is reduced by a factor $&amp;gt;5$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;design&#34;&gt;Design&lt;/h2&gt;
&lt;p&gt;So far, discussion has brought us to implicit graphs and abstractions through the lens of game theory. The objective of this section will be to motivate these concepts beyond game theory, while supplying references to concrete programming ideas.&lt;/p&gt;
&lt;p&gt;To do this, we will cover a representation of an implicit graph in a real programming language, apply it to a new problem domain, and make improvements that bring real-world utility. Examples will still be given in terms of games, as they also happen to fit under our new focus. In doing so, we will design a solution to a broad problem using our new toolset items.&lt;/p&gt;
&lt;h3 id=&#34;interface-items&#34;&gt;Interface items&lt;/h3&gt;
&lt;p&gt;There are a number of considerations to make when encoding an implicit graph, whose importance will vary depending on the object being represented. This section will introduce only the example of graphs of subproblems in the context of &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_programming&#34;&gt;dynamic programming&lt;/a&gt; (DP), and will iterate on the following &lt;a href=&#34;https://doc.rust-lang.org/book/ch10-02-traits.html&#34;&gt;Rust interface&lt;/a&gt; to eventually allow their solutions to be found in parallel through a special kind of abstraction.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;trait&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ImplicitGraph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;where&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;IntoIterator&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Item&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The elements of this interface declaration relate to the implicit graph $G^I = \langle S, t, s_0 \rangle$ as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The generic parameter &lt;code&gt;Start&lt;/code&gt; is the type of the elements in $S$.&lt;/li&gt;
&lt;li&gt;The generic parameter &lt;code&gt;C&lt;/code&gt; is the type of the elements in $\mathcal{P}(S)$.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transition&lt;/code&gt; is the template of $t$.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start&lt;/code&gt; simply returns $s_0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;Implementation of the game &lt;code&gt;10-to-0-by-1-or-2&lt;/code&gt; from the section on Rulesets as an implicit graph.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;/// The game `10-to-0-by-1-or-2`.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;ZeroBy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;impl&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ImplicitGraph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ZeroBy&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// Tuple of (items, turn).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// Returns (10 items left, player 0&amp;#39;s turn).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// Returns states with one and two less items on the opposing player&amp;#39;s turn.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;u32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;turn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;let&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;mut&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Vec&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;new&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;turn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;turn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;turn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;parallel-dp&#34;&gt;Parallel DP&lt;/h3&gt;
&lt;p&gt;A popular characterization of DP is to establish dependency &lt;a href=&#34;https://en.wikipedia.org/wiki/Relation_(mathematics)&#34;&gt;relations&lt;/a&gt; on sets of subproblems, defining a &lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34;&gt;directed acyclic graph&lt;/a&gt; (DAG) for any properly formulated subproblem definition. A natural link to implicit graphs exists through their bijection with general graphs.&lt;/p&gt;
&lt;p&gt;Having established that a DP problem can be characterized as an implicit graph of subproblems, it is also worth mentioning that most unorganized solution implementations (i.e., that do not organize solutions or information about subproblems in a tensor) make use of stack-like data structures to aid traversals of subproblems in &lt;a href=&#34;https://en.wikipedia.org/wiki/Tree_traversal&#34;&gt;postorder&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;DP algorithm to compute who will win a game of &lt;code&gt;10-to-0-by-1-or-2&lt;/code&gt;, with the below subproblem relation&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
$$ W(s) = \max_{s&#39; \in \\, t(s)} \min_{s&#39; \in \\, t(s)} W(s&#39;). $$&lt;p&gt;Here, $W : S \to \{ 0, \, 1 \} $ maps state information (including the number of items remaining and player turn) to whether the player whose turn it is at $s$ would win under optimal play, with $t$ being the transition function of the implicit graph over this game&amp;rsquo;s states. This uses the implementation from the previous example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;procedure:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  temp ← empty stack
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  stack ← empty stack
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  visited ← empty set
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  stack.push(ZeroBy::start())
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  while stack is not empty:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    current ← stack.pop()
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    if current is not in visited:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      visited.add(current)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      temp_stack.push(current)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      for each state in ZeroBy::transition(current):
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        if state is not in visited:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          stack.push(state)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  solution ← empty map
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  while temp_stack is not empty:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    current ← temp_stack.pop()
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    solution[current] = W(current)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  return solution[ZeroBy::start()]&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Note the use of $W$ in line 19. Also note that the subproblem relation does not necessarily generalize to other games and, for the sake of brevity, is not defined for base cases (where $t(s) = \varnothing$).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Attempts to parallelize this setup must first identify a method to partition the subproblem graph in a way optimizes the tradeoff between parallelism and its own overhead. Of course, this depends significantly on the specific resources that will be used to execute the resulting program.&lt;/p&gt;
&lt;p&gt;I will introduce one way of doing this that follows naturally from the use of the implicit graph interface. Concretely, a carefully chosen abstraction $\pi : S \to \mathbb{N}$ that connects the graph of subproblems to a DAG of enumerated sets of states will provide a parallelization scheme.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;trait&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ImplicitGraph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;where&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;IntoIterator&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Item&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;kt&#34;&gt;u64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// &amp;lt;-- NEW
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here, &lt;code&gt;partition&lt;/code&gt; is the template of $\pi$. The big idea is that during a traversal, we can observe a change in the value of &lt;code&gt;*::partition(current)&lt;/code&gt;, where &lt;code&gt;current&lt;/code&gt; is the current state in the traversal. This way, we can build a graph of the outputs of this function based on their adjacency in the subproblem graph. Finally, we analyze the resulting graph to find sets of states that can be traversed simultaneously.&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Example&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;p&gt;On the left, a graph over a set of states $S = \{ s_0, \, \ldots, \, s_{11} \}$. On the right, a graph over a set $\Pi \subset \mathbb{N}$ with four elements. They are related by an abstraction $\pi : S \to \Pi$ that is special in that the resulting graph over the elements of $\Pi$ is acyclic. Hence, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fiber_(mathematics)&#34;&gt;fibers&lt;/a&gt; $\pi^{-1}(\{\pi_i\})$ of certain distinct elements $(\pi_i)$ of $\Pi$ could possibly be traversed in parallel. An example of groupings of elements in  $\Pi$ whose fibers under $\pi$ could be traversed in parallel is provided in dotted boxes on the graph of $\Pi$.&lt;/p&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;partition.png&#34; width=&#34;550&#34;/&gt; 
&lt;/figure&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Finding a suitable $\pi$ depends on the chosen state (subproblem) representation and is of course highly problem-specific. However, a general strategy is to ensure that $\pi$ outputs a different label if and only if an irreversible change is made to some form of mutable state. As a high-level criterion, this helps construct an abstraction that assuredly maps onto a DAG.&lt;/p&gt;
&lt;p&gt;Having found such an abstraction, the next perplexity of parallelizing a postorder traversal of subproblems is managing efficient and clear use of shared data structures. Here, a zoo of approaches with varyingly personable tradeoffs are available. I will introduce only one, which involves an additional interface item in the form of a function $t&amp;rsquo; : S \to \mathcal{P}(S)$ called &lt;code&gt;retrograde&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-rust&#34; data-lang=&#34;rust&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;trait&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ImplicitGraph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;where&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;: &lt;span class=&#34;nb&#34;&gt;IntoIterator&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Item&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;kt&#34;&gt;u64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;fn&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;retrograde&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;: &lt;span class=&#34;nc&#34;&gt;Self&lt;/span&gt;::&lt;span class=&#34;n&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&amp;gt; &lt;span class=&#34;nc&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// &amp;lt;-- NEW
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In an ideal world, &lt;code&gt;retrograde&lt;/code&gt; is the equivalent of &lt;code&gt;transition&lt;/code&gt; for the &lt;a href=&#34;https://en.wikipedia.org/wiki/Transpose_graph&#34;&gt;transpose&lt;/a&gt; of the graph being represented. Unfortunately, it is not generally tractable to have a perfect implementation of &lt;code&gt;retrograde&lt;/code&gt; without first materializing the entire graph (defeating the purpose of an implicit graph representation). Thus, we only make sure that &lt;code&gt;retrograde&lt;/code&gt; returns a superset of what &lt;code&gt;transition&lt;/code&gt; would return for the transpose of the graph, which is easy to ensure in many useful cases.&lt;/p&gt;
&lt;p&gt;With $\pi$ and $t&amp;rsquo;$ in our hands, we can provision the following procedure for parallelizing the execution of an unorganized dynamic programming algorithm over an implicit graph $G^I$ of subproblems:&lt;/p&gt;
&lt;style&gt;
    .box-body &gt; :last-child {
        margin-bottom: 0 !important;
    }

    .box-body &gt; :first-child {
        margin-top: 0 !important;
    }
&lt;/style&gt;
&lt;div
    class=&#34;hint-box&#34;
    style=&#34;
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    &#34;
&gt;
    &lt;strong style=&#34;display: block; margin-bottom: 5px&#34;
        &gt;Procedure&lt;/strong
    &gt;
    &lt;hr
        style=&#34;
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        &#34;
    /&gt;
    &lt;div style=&#34;font-size: 0.92em&#34; class=&#34;box-body&#34;&gt;
&lt;ol&gt;
&lt;li&gt;Traverse $G^I$ to obtain the set of subproblems $S$ in $\mathcal{\Theta}(|G^I|)$ time and $\mathcal{\Theta}(|S|)$ space.&lt;/li&gt;
&lt;li&gt;During (1), track the subset of subproblems $S_{base} = \{ s \; | \; t(s) = \varnothing \}$ at no additional cost.&lt;/li&gt;
&lt;li&gt;During (1), construct a graph $G_\Pi$ over a set of partition labels $\Pi$ using $\pi$, at a $\pi$-dependent cost.&lt;/li&gt;
&lt;li&gt;Use $G_\Pi$ to generate a plan&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; of labeled parallel tasks in $\mathcal{\Theta}(|G_\Pi|)$ time and $\mathcal{\Theta}(|\Pi|)$ space.&lt;/li&gt;
&lt;li&gt;Delegate tasks, starting exploration from popped elements of $S_{base}$ with the task&amp;rsquo;s label.&lt;/li&gt;
&lt;li&gt;Use $t&amp;rsquo;$ for backward intra-partition traversal (using $S$ for existence checks) in $t&amp;rsquo;$-dependent time.&lt;/li&gt;
&lt;li&gt;When a change in $\pi(s)$ is observed on the current state $s$, add $s$ to $S_{base}$, leaving it unexplored.&lt;/li&gt;
&lt;li&gt;When a partition is completely explored, finish its task and free the parallel unit.&lt;/li&gt;
&lt;li&gt;Repeat from (5) on new elements of $S_{base}$ until there are no tasks remaining.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;These general steps skip some details, but they present an arbitrarily parallel stack-free traversal that can be implemented over a single shared data structure whose size scales in the order of $\mathcal{\Theta}(|S|)$ (ignoring structures related to partitions, whose size is assumed to be negligible). This kind of map-like &lt;a href=&#34;https://en.wikipedia.org/wiki/Thread_safety&#34;&gt;thread-safe&lt;/a&gt; functionality is available in many database implementations which automatically bring the added benefit of disk usage, making this method applicable to &amp;ldquo;bigger&amp;rdquo; problems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;meta-content&#34;&gt;Meta-content&lt;/h2&gt;
&lt;p&gt;The section titled &amp;ldquo;Representation&amp;rdquo; got us to stumble across the new concepts of implicit graphs and abstractions by looking at different forms for game representations. The following section extrapolated these ideas to the domain of dynamic programming, and showed how it is possible to incorporate them into the design of solutions to real-world problems.&lt;/p&gt;
&lt;p&gt;Something interesting is that games made their way into the second section, despite being decidedly out of scope at that point. In a dying hope of getting this article back on track, I will point out that the particular example of parallelizing DP algorithms was conveniently chosen because it is used to &lt;a href=&#34;https://en.wikipedia.org/wiki/Solved_game&#34;&gt;solve&lt;/a&gt; bigger games faster than was previously possible (through DP algorithms that consume representations of them).&lt;/p&gt;
&lt;p&gt;This way, I can say that this whole article was in fact about game-theoretic systems. But we both know that it was really about implicit graphs and abstractions. Maybe, if we squint our eyes, it can be about both topics. Either way, I hope the lack of clarity was more stimulating than it was confusing.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;The problem of deciding whether or not a string is in the language of a context-free grammar.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;For his doctoral dissertation, &lt;a href=&#34;https://gametheory.online/projects/documents/1521541344.pdf&#34;&gt;&lt;em&gt;Non-Cooperative Games&lt;/em&gt;&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;This fact is known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Zermelo%27s_theorem_(game_theory)&#34;&gt;Zermelo&amp;rsquo;s theorem&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;In both &lt;a href=&#34;https://en.wikipedia.org/wiki/Measure_(mathematics)&#34;&gt;the mathematical&lt;/a&gt; and informal sense.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;For example, &lt;a href=&#34;https://www.youtube.com/watch?v=-aSBlRhpwVc&#34;&gt;Christos Papadimitrou on replicator dynamics&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;$\text{N}\small{\text{ASH}}$ asks to find a mixed-strategy NE. A pure-strategy NE is a case of a mixed-strategy NE. &lt;a href=&#34;https://en.wikipedia.org/wiki/Zermelo%27s_theorem_(game_theory)&#34;&gt;Zermelo&amp;rsquo;s theorem&lt;/a&gt; shows a pure-strategy NE always exists for the class of games in question. Then, &lt;a href=&#34;https://en.wikipedia.org/wiki/Backward_induction&#34;&gt;backward induction&lt;/a&gt; algorithms can find one in linear time for finite representations of games.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;It is possible to obtain this number for a specific starting configuration of a Rubik&amp;rsquo;s Cube, but no one knows the length of the longest minimal sequence of moves necessary to solve it across all starting configurations. This is somewhat dramatically known as &lt;a href=&#34;https://web.archive.org/web/20141109174500/http://digitaleditions.walsworthprintgroup.com/article/The_Quest_For_God%E2%80%99s_Number/532775/50242/article.html&#34;&gt;God&amp;rsquo;s Number&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;Term generalized from &lt;a href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2001/CSD-01-1156.pdf&#34;&gt;its use in reinforcement learning&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;This subproblem relation monomorphizes the generic formulation of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Minimax&#34;&gt;minimax algorithm&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;A parallelization plan is a data structure that can dispense information on which task must be worked on next when a parallel unit becomes available for work. It may also communicate the need to wait until another unit completes its work.&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    
    
    
  </channel>
</rss>
