<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
    <meta name="description" content="Max&#39;s personal site">
    
     
    <link rel="icon" type="image/x-icon" href="/favicon.ico" media="(prefers-color-scheme: light)">
    <link rel="icon" type="image/x-icon" href="/favicon-dark.ico" media="(prefers-color-scheme: dark)"> 
     
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


    

    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/rich-perspectives-on-tensors-signals-and-kernel-methods/" />
    <title>Rich Perspectives on Tensors, Signals, and Kernel Methods</title>
</head>
<body><header id="banner">
    <a href="http://localhost:1313/"
        ><img src="/logo.svg" alt="Logo" class="site-logo"
    /></a>
    <h2><a href="http://localhost:1313/">Max Fierro</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/about/" title="about"
                    >about</a
                >
            </li><li>
                <a href="/resume/" title="resume"
                    >resume</a
                >
            </li><li>
                <a href="/index.xml" title=""
                    >rss</a
                >
            </li>
        </ul>
    </nav>
</header>
<main id="content">
<article>
    <header id="post-header">
        <h1>Rich Perspectives on Tensors, Signals, and Kernel Methods</h1>
        <div>
                <time>September 8, 2025</time>
            </div>
    </header><aside id="toc">
    <details>
        <summary>&nbsp;<strong> Table of contents</strong></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#linear-combinations">Linear Combinations</a></li>
        <li><a href="#abstract-tensor-spaces">Abstract Tensor Spaces</a></li>
        <li><a href="#signals-and-systems">Signals and Systems</a></li>
        <li><a href="#kernel-methods">Kernel Methods</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </details>
</aside>

<h2 id="abstract">Abstract</h2>
<p>Tensor algebra, signal processing, and machine learning methods are (one of many groups of) topics that enjoy natural relationships enclosed in a dense shell of mathematics. This article provides initiated readers satisfying new perspectives on ideas in this group of topics, while providing a challenging but rewarding introduction to those new to the topics.</p>
<hr>
<h2 id="overview">Overview</h2>
<p>I follow an essay-like structure including an introduction, body, and conclusion. , I assume a first course in linear algebra, signal processing, and machine learning methods. For readers lacking this background, I leave pointers to free resources.</p>
<p>The introduction arrives at the <a href="https://en.wikipedia.org/wiki/Linear_map">linear operator</a> perspective of <a href="https://en.wikipedia.org/wiki/Multidimensional_system">systems of many dimensions</a>, taking system properties like <a href="https://en.wikipedia.org/wiki/Time-invariant_system">time-invariance</a> and <a href="https://en.wikipedia.org/wiki/Causal_system">causality</a> in terms of <a href="https://en.wikipedia.org/wiki/Tensor">tensor</a> representations. These perspectives then help the body (a cross-cutting perspective of the <a href="https://en.wikipedia.org/wiki/Convolution">convolution kernel</a> and the <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#:~:text=then%20called%20the-,reproducing%20kernel,-%2C%20and%20it%20reproduces">reproducing kernel</a>).</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >2.1. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The word &ldquo;kernel&rdquo; is criminally polysemous in mathematics, and it will be used a lot in this piece. I mostly use the word under two semantics: The convolution kernel from signal processing and the reproducing kernel from machine learning. All other instances of the word should simply refer to its english meaning.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</div>
</div>
<hr>
<h2 id="introduction">Introduction</h2>
<p>This section provides not much more than a definition-based refresher on select topics from first courses in linear algebra, signal processing, and machine learning. For readers in need of comprehensive review or first-time coverage, I leave these free resources on said topics:</p>
<ul>
<li><strong>Kernel methods.</strong> <a href="https://cs.nyu.edu/~mohri/mlbook/"><em>Foundations of Machine Learning</em></a> by Mohri, Rostamizadeh, and Talwalkar.</li>
<li><strong>Signals and Systems.</strong> <a href="https://ss2-2e.eecs.umich.edu/"><em>Signals &amp; Systems: Theory and Applications</em></a> by Ulaby and Yagle.</li>
<li><strong>Linear algebra.</strong> <a href="https://linear.axler.net/"><em>Linear Algebra Done Right</em></a> by Axler.</li>
</ul>
<h3 id="linear-combinations">Linear Combinations</h3>
<p>In what is nowadays close to being a canon of linear algebra education, Sheldon Axler opens with the statement below to set the stage for the rest of <em>Linear Algebra Done Right</em>:</p>
<blockquote>
<p>Linear algebra is the study of linear maps on finite-dimensional vector spaces.</p></blockquote>
<p>Here, the restriction of vector spaces to the finite-dimensional case was one the most mathematically respectful ways to negotiate generality with practical pedagogy. However, the spirit of linear algebra is alive way beyond the finite-dimensional case.</p>
<h4 id="hamel-bases">Hamel Bases</h4>
<p>Most engineers are familiar with the concept of a (Hamel) basis of a vector space. If we have a vector space $V$ over a field $\mathbb{F}$ and a Hamel basis $\mathcal{B}$, then &ldquo;$\mathcal{B}$ spans $V$&rdquo; translates to</p>
$$
\begin{equation}
    \forall v \in V, \; v = \sum_{i \, = \, 0}^{k} c_i b_i \;\; \text{s.t.} \;\; c_i \in \mathbb{F}, \, b_i \in \mathcal{B}, \, k \in \mathbb{N}.
\end{equation}
$$<p>Importantly, for $B$ to be a Hamel basis, the sum in $(1)$ must have finite terms. Note that this is allowed even in cases where $\mathcal{B}$ is infinite (or in other words, where $V$ is infinite-dimensional), as one does not necessarily assign nonzero coefficients $c_i$ to each element of $\mathcal{B}$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.1. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The vector space of polynomials (of finite terms) with coefficients in a field $\mathbb{F}$,</p>
$$
\mathbb{F}[x] = \left\{ \sum_{i \, = \, 0}^n a_i x^i \;\Big|\; n \in \mathbb{N},\ a_i \in \mathbb{F} \right\},
$$<p>has the infinite basis $\mathcal{B}_{\mathbb{F}[x]} = \{1, x, x^2, x^3, \dots \}$. Each of its elements, however, is the linear combination of a finite number of basis elements. For example, $p(x) = 3 + 4x^2 + x^3$ can be expressed as the combination</p>
$$
p(x) = 3
\begin{bmatrix}
 1 \\
 0 \\
 0 \\
 0 \\
 \vdots
\end{bmatrix} + 4  
\begin{bmatrix}
 0 \\
 0 \\
 1 \\
 0 \\
 \vdots
\end{bmatrix} + 1 
\begin{bmatrix}
 0 \\
 0 \\
 0 \\
 1 \\
 \vdots
\end{bmatrix}.
$$<p>Here we imposed a standard representation such that, for example, $x^2 = \left[ 0, \, 0, \, 1, \, 0, \, {\dots} \right]^\top$. We see that, despite each basis vector being infinite-dimensional, all polynomials are determined by a finite number of them.</p>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-molndrian-1974.png"
         alt="Vera Molnár, &lsquo;Molndrian&rsquo; (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, &lsquo;Molndrian&rsquo; (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="schauder-bases">Schauder Bases</h4>
<p>Interpreting Axler strictly, $\mathbb{F}[x]$ is already beyond linear algebra because it is of <a href="https://en.wikipedia.org/wiki/Semi-infinite">semi-infinite</a> dimension. But definitionally, it is a perfectly valid vector space. Just as finite dimensionality is not necessary in order to access the theorems of linear algebra, having a countable Hamel basis is also not necessary; all vector spaces do have a Hamel basis<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, but not all of them have a countable one.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.2. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Countable bases are desireable not for being countable per se, but rather that, in most cases where a vector space does not have a countable Hamel basis, the uncountable Hamel basis is unconstructive and unutterable. Put another way, the most useful fact about an uncountable Hamel basis, in many cases, is that it exists.</p>
</div>
</div>
<p>For some vector spaces that do not have a countable Hamel basis, one can relax the definition of a basis itself to obtain one that is countable. Specifically, we redefine the phrase &ldquo;$\mathcal{B}$ spans $V$&rdquo; to</p>
$$
\begin{equation}
    \forall v \in V, \; v = \lim_{ n \to \infty } \, \sum_{i \, = \, 0}^{n} c_i b_i \;\; \text{s.t.} \;\; c_i \in \mathbb{F}, \, b_i \in \mathcal{B}.
\end{equation}
$$<p>If the above is true for a vector space $V$ over $\mathbb{F}$, then $\mathcal{B}$ is a Schauder basis of said space. The critical difference to a Hamel basis is of course the generality afforded by the possiblity of infinite terms for the sum in $(2)$, giving us a new countably-infinite flavor of linear combination.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.3. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The vector space of $p$-summable (with $p &gt; 0$) sequences of real numbers,</p>
$$
\ell^p = \left\{ (x_1, x_2, x_3, \dots) \;:\; \sum_{n \, = \, 1}^\infty |x_n|^p < \infty \right\},
$$<p>has no Hamel basis because, no matter how you define one, there is an element of $\ell^p$ requiring decomposition into an infinite number of basis elements (which is not allowed). But it does have the following countably-infinite Schauder basis:</p>
$$
\mathcal{B}_{\ell^p} = \left\{ 
\left(
 1, \,
 0, \,
 0, \,
 \dots
\right), \,
\left(
 0, \,
 1, \,
 0, \,
 \dots
\right), \,
\left(
 0, \,
 0, \,
 1, \,
 \dots
\right), \,
{\dots}
\right\}.
$$</div>
</div>
<h4 id="taxonomy-of-spaces">Taxonomy of Spaces</h4>
<p>Hidden in $(2)$ is the requirement that all such sums over basis elements converge. But the definition of a vector space does not include any operation that computes the &ldquo;closeness&rdquo; of two vectors, so additional concepts are needed to make sense of convergence. Abstractly, one needs to equip the vector space of interest with a <a href="https://en.wikipedia.org/wiki/Topological_space#topology">topology</a>. The way of doing so that we will consider is by assuming a <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> over the space, such that we can concretely declare the definition of an infinite series</p>
$$
\begin{equation}
    \lim_{n \to \infty} \left\| x - \sum_{k \, = \, 1}^n a_k \right\| = 0 \iff x = \sum_{k \, = \, 1}^{\infty} a_k.
\end{equation}
$$<p>Vector spaces that have a norm are called normed vector spaces. If a normed vector space is <a href="https://en.wikipedia.org/wiki/Complete_metric_space">complete</a> under the norm-induced <a href="https://en.wikipedia.org/wiki/Metric_space">metric</a> $d : (x_1, \, x_2) \mapsto \| x_1 - x_2 \|$, then it is also called a <a href="https://en.wikipedia.org/wiki/Banach_space">Banach space</a>. With the example of $\ell^p$, we have completeness through the conventional $\ell^p$-norm, defined with</p>
$$
\|x\|_p =
\begin{cases}
\; \left( \displaystyle\sum_{n=1}^\infty |x_n|^p \right)^{1/p} & \text{if} \;\; 1 \le p < \infty, \\[1.8em]
\;\; \sup_{n} |x_n| & \text{if} \;\; p = \infty.
\end{cases}
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.4. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Banach spaces do not necessarily have a Schauder basis. The reason for this is technical and out of scope. Additionally, not all normed vector spaces that have a Schauder basis are Banach spaces, because they may not be complete. But for the remainder of this piece, completeness can be comfortably assumed. Indeed, most of the time anyone talks about a Schauder basis in a practical context, it spans a complete space (such as $\ell^p$).</p>
</div>
</div>
<p>If a Banach space is also equipped with an <a href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a> in such a way that $\langle x, \, x \rangle = \| x \|^2$, then it is also called a <a href="https://en.wikipedia.org/wiki/Inner_product_space">Hilbert space</a>. With the understanding that a vector space with an inner product defined is called an inner-product space, we arrive at the following:</p>
$$
\begin{equation}
\begin{aligned}
\text{Vector} & \text{ spaces} \\[0.2em]
&\supset
\Bigg\{
  \begin{aligned}
  & \text{Normed vector spaces} \;\supset\; \text{Banach spaces} \\
  & \text{Inner-product vector spaces}
  \end{aligned}\\[1.2em]
&\supset \text{Hilbert spaces} = (\text{Banach spaces} \, \cap \, \text{Inner-product spaces}).
\end{aligned}
\end{equation}
$$<h4 id="continuous-bases">Continuous Bases</h4>
<p>The last thing we will consider are vector spaces of uncountably-infinite dimension. So far, we have been comfortable in using syntax such as &ldquo;$x = [ 1, \, 2, \, \ldots ]^\top$&rdquo; to refer to vectors of countably-infinite dimension. This will no longer be possible with uncountably-infinite dimensions, so we must revisit our notation.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.5. Reminder</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>A vector $v$ is an abstract object, and is independent of a choice of basis. To write $v$ as a tuple $[c_1, \dots, c_n]$, one must choose a basis ${b_1, \, \dots, \, b_n}$ and expand (in the finite-dimensional case)</p>
$$
v = \sum_{i \, = \, 1}^n c_i b_i.
$$<p>Hence, $[1, \, 2, \, 3]^\top$ means “the coefficients of $v$ in this basis.”
Changing the basis changes the coefficients, but not the vector itself. This also applies to infinite-dimensional cases.</p>
</div>
</div>
<p>Observing that, once a basis is chosen, a vector in a vector space $V$ over a field $\mathbb{F}$ is determined by the coefficients in its representation as a linear combination of basis vectors, we can introduce a new type of linear combination to evolve the translation of &ldquo;$\mathcal{B}$ spans $V$&rdquo; to involve a <a href="https://en.wikipedia.org/wiki/Lebesgue_integral">Lebesgue integral</a>,</p>
$$
\begin{equation}
    \forall v \in V, \; v = \int_{\mathcal{\Omega}} c(\omega) b(\omega) \, d\mu(\omega) \;\; \text{s.t.} \;\; c : \Omega \to \mathbb{F}, \, b : \Omega \to \mathcal{B},
\end{equation}
$$<p>where a measure ${\mu}$ over $\Omega$ is provided. Here, the index $\omega$ intuitively replaces the index $i$ from $(2)$, where there is a map $c$ &ldquo;choosing&rdquo; a coefficient $c(\omega)$ and a map $b$ &ldquo;choosing&rdquo; a basis element $b(\omega)$ per index $\omega$. This way, once a map $b : \Omega \to \mathcal{B}$ and a measure $\mu$ over $\Omega$ have been agreed upon, a vector can still be represented by &ldquo;its coefficients,&rdquo; which is just the map $c$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.6. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>We can consider the Banach space of all $p$-integrable (with $p &gt; 0$) functions on a measurable set $X$</p>
$$
L^p(X, \, \mu) = 
\Bigl\{\, f: X \to \mathbb{C} \; \text{measurable} \; \big| \; \int_{X} |f(x)|^p \, d\mu(x) < \infty \,\Bigr\}.
$$<p>Much like the $\ell^p$-norm is standard for $\ell^p$, the $L^p$-norm is standard for $L^p$. It is defined in the same way, with the only difference being the promotion of the sum to a Lebesgue integral on $X$ using the measure $\mu$</p>
$$
\|f\|_p =
\begin{cases}
\; \left( \displaystyle\int_{X} |f(x)|^p \, d\mu(x) \right)^{1/p} & \text{if} \;\; 1 \le p < \infty, \\[1.4em]
\;\; \sup_x |f(x)| & \text{if} \;\; p = \infty.
\end{cases}
$$<p>Notice that if $X = \mathbb{N}$ and $\mu$ is the counting measure, $L^p(X, \, \mu)$ becomes $\ell^p$ and the $L^p$-norm is the $\ell^p$-norm.
In the important case of $L^2(\mathbb{R})$, the inner product defined by</p>
$$
\langle f_1(x), \, f_2(x) \rangle = \int_{\mathbb{R}} f_1(x) \overline{f_2(x)} \, dx
$$<p>actually ensures that $\langle f, f \rangle = \| f \|_2$, making $L^2(\mathbb{R})$ a Hilbert space. A natural choice of &ldquo;continuous basis&rdquo; for $L^2(\mathbb{R})$ is the complex exponentials indexed by frequency, $b_\omega(t) = e^{2 \pi i \omega t}$ with $\omega \in \mathbb{R}$. For all $f \in L^2(\mathbb{R})$,</p>
$$
f(t) = \int_{\mathbb{R}} c(\omega)b(\omega) \, d\mu(\omega) = \int_{-\infty}^{\infty} c(\omega)\, b_\omega(t)\, d\omega.
$$<p>The measure $d\omega$ is the typical Lebesgue measure on $\mathbb{R}$. Here, we see that any $f \in L^2(\mathbb{R})$ can be expressed as a continuous linear combination of basis elements in the form $e^{2 \pi i \omega t}$ which, to reiterate, are other functions parameterized by $t$ and indexed by $\omega \in \mathbb{R}$. In this case, the <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> of $f(t)$ is in fact $c(\omega)$:</p>
$$
c(\omega) =
\int_{-\infty}^{\infty} f(t) \, \overline{b_\omega(t)} \, dt
= \int_{-\infty}^{\infty} f(t)\, e^{-2 \pi i \omega t} \, dt.
$$<p>In a finite-dimensional case, we would compute $\langle v, \, e_n \rangle$ to observe the &ldquo;contribution&rdquo; of the basis element $e_n$ in the vector $v$, obtaining the coefficient it would be assigned in its decomposition as a linear combination of basis elements. The Fourier transform does exactly the same thing per $\omega$, where $v = f(t)$ and $e_n = b_\omega(t)$:</p>
$$
c(\omega) = \langle f(t), \, b_\omega(t) \rangle.
$$<p>It is not difficult to show algebraically that the Fourier transform $\mathcal{F} : L^2(\mathbb{R}) \to L^2(\mathbb{R})$ is a linear operator over this Hilbert space. (So if it were countably-infinite dimensional, it would have a matrix representation.)</p>
</div>
</div>
<h4 id="overview-1">Overview</h4>
<p>We have expanded a finite-dimensional view of vector spaces to potentially allow those with countably- or uncountably-infinite dimensions. To do so, we had to slowly relax our concept of a linear combination from $(1)$ (which already spanned certain infinite-dimensional spaces like $\mathbb{F}[x]$), to $(2)$ (which was able to span a more countably-infinite-dimensional spaces like $\ell^p$), and finally $(5)$ (which can span uncountably-infinite dimensional spaces like $L^2(\mathbb{R})$). We have also encountered the conepts of Banach and Hilbert spaces.</p>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="david-hilbert.jpg"
         alt="David Hilbert (January 23, 1862 – February 14, 1943)" width="256"/> <figcaption>
            <p>David Hilbert (January 23, 1862 – February 14, 1943)</p>
        </figcaption>
</figure>

</div>

<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.7. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>In fact, $(1)$ is a special case of $(2)$, which is a special case of $(5)$. Therefore, it is truly a relaxation of the linear combination; at no point did we lock ourselves out of any vector spaces we could already span. Namely, in the case of $(5)$ and $(2)$ for a given vector space $V$,</p>
$$
\Omega = \mathbb{N} \iff \forall v \in V, \; v = \int_{\Omega} c(\omega)b(\omega) \, d{\mu}(\omega) = \sum_{i \, = \, 0}^{\infty} c_i b_i.
$$<p>In the case of $(2)$ and $(5)$, when all vectors in $V$ are a linear combination of a finite number of basis elements (as is the case for $\mathbb{F}[x]$),</p>
$$
\forall v \in V, \; v = \sum_{i \, = \, 0}^{\infty} c_i b_i = \sum_{i \, = \, 0}^{k} c_i b_i \;\; \text{s.t.} \;\; k \in \mathbb{N}.
$$</div>
</div>
<h3 id="abstract-tensor-spaces">Abstract Tensor Spaces</h3>
<p>Let us revisit the opening scene of <em>Linear Algebra Done Right</em>:</p>
<blockquote>
<p>Linear algebra is the study of linear maps on finite-dimensional vector spaces.</p></blockquote>
<p>Another thing to note (apart from the restriction to finite dimensions) is the lack of matrices and vectors in this description. This section will live up to this witholding; we will first look for a linear-map-centric view of the objects in linear algebra and, afterwards, gain an understanding of tensor spaces.</p>
<h4 id="vectors-and-matrices">Vectors and Matrices</h4>
<p>A regrettable aspect of a typical introduction to linear algebra is the marriage of syntax to abstract objects. Most of us were told in a first impression that a vector $v$ and a matrix $M$ may look something like this:</p>
$$
\begin{equation}
v = 
\begin{bmatrix}
1 \\
2
\end{bmatrix}, \quad 
M = 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}.
\end{equation}
$$<p>I assert that $v$ and $M$ are <em>both</em> matrices, each of which simultaneously identifies a vector and linear map. For richer support, I will establish three resources below and explain their relationship afterward.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.8. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The set of linear maps from a vector space $V$ over the field $\mathbb{F}$ to another vector space $W$ (over the same field) forms a vector space over $\mathbb{F}$. That is,</p>
$$
\mathcal{L}(V, W) = \left\{ \, T : V \to W \; | \; T \text{ is linear} \right\}
$$<p>is a vector space over $\mathbb{F}$. We denote the case of linear operators on $V$, which is simply $\mathcal{L}(V, V)$, as $\mathcal{L}(V)$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.9. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>There is a bijection between $\mathcal{L}(V, W)$ and $\mathbb{F}^{(\dim V) \times (\dim W)}$, where $V$ and $W$ are vector spaces on the same field $\mathbb{F}$ and are finite-dimensional. In other words, for each linear map $T$ from a vector space of dimension $n$ to another of dimension $m$, there is exactly one $m$-by-$n$ matrix with entries in $\mathbb{F}$. When bases are fixed, $T$ and its corresponding matrix can be translated in a standard way.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.10. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>A vector $v$ in a space $V$ over $\mathbb{F}$ can be regarded as a linear map from the space $\mathbb{F}^1$ into $V$, via</p>
$$
\psi_v : \mathbb{F}^1 \to V, \;\; \psi_v(\lambda) = v \lambda.
$$<p>When a basis for $V$ is fixed, the map $\psi_v$ is represented by an $n \times 1$ matrix (as an instance of theorem 3.9) &ndash; the familiar column of &ldquo;coordinates&rdquo; of $v$. In particular, observe that scalar multiplication can be seen as matrix multiplication with a single-dimensional vector,</p>
$$
\psi_v(\lambda) = 
\begin{bmatrix}
v_1 \\
\vdots \\
v_{(\dim V)}
\end{bmatrix}
\begin{bmatrix}
\lambda_1
\end{bmatrix} =
\begin{bmatrix}
\lambda_1 v_1 \\
\vdots \\
\lambda_1 v_{(\dim V)}
\end{bmatrix} = 
\lambda_1 v.
$$</div>
</div>
<p>This sets up a clear organization of vectors, matrices, and linear maps. With fixed bases, we see that the set of linear maps from $V$ to $W$ is already a vector space (via 3.8) and also that all vectors in a space $U$ over a field $\mathbb{F}$ are canonically a linear map from $\mathbb{F}^1$ into $U$ (via 3.10), hence</p>
$$
\text{Linear maps} \cong \text{Vectors}.
$$<p>Also, we see that there is a one-to-one correspondence between linear maps and matrices (via 3.9) by the bijection $\mathcal{L}(V, W) \leftrightarrow \mathbb{F}^{(\dim W) \times (\dim V)}$, so in finite dimensions ($\cong^!$)</p>
$$
\text{Matrices} \cong^! \text{Linear maps}.
$$<p>Indeed, a vector can be turned into a matrix solely through its identifiability as a linear map, turning the linear map into the central object of our understanding of linear algebra. In summary,</p>
$$
\begin{equation}
    \text{Matrices} \cong^! \text{Linear maps} \cong \text{Vectors}.
\end{equation}
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.11. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>We have started to use the symbol $\cong$. In this context, stating $A \cong B$ implies that there is a linear <a href="https://en.wikipedia.org/wiki/Isomorphism">isomorphism</a> between $A$ and $B$. Exactly, this means that there exists some linear map $\phi : A \to B$ that is a bijection.</p>
<p>This is complete as a definition of the symbol $\cong$. However, its use in the rest of this piece will often reference a choice of $\phi$ that is <a href="https://en.wikipedia.org/wiki/Canonical_map">canonical</a>. This means that if you see $A \cong B$, there is probably an &ldquo;standard&rdquo; way to obtain a $b \in B$ from one unique $a \in A$ (and vice versa) &ndash; here, we can informally say that if $\phi(a) = b$ then $a$ &ldquo;is&rdquo; $b$, but it is more precise to say that $a$ &ldquo;identifies&rdquo; $b$. Oftentimes, $\phi$ will not be made explicit.</p>
<p>For example, choosing a basis $\mathcal{B}$ for a space $V$ gives the canonical isomorphism $\mathbb{F}^{(\dim V)\times(\dim V)} \cong \mathcal{L}(V)$. In this case, it makes sense to say that &ldquo;a matrix is a linear transformation.&rdquo; But without a choice of basis there is no &ldquo;standard&rdquo; bijection $\Phi_\mathcal{B} : \mathcal{L}(V) \to \mathbb{F}^{(\dim V) \times (\dim V)}$ (no way to bijectively identify maps from matrices).</p>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-untitled-square-1974.png"
         alt="Vera Molnár, Untitled (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, Untitled (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="vector-translation">Vector Translation</h4>
<p>The statement of 3.10 is neuanced. Consider the case of a linear map $T \in \mathcal{L}(V, W)$ represented by a matrix $M$ (under fixed bases). According to 3.8 that map is a vector, but according to 3.10 it is identified by some <em>other</em> linear map $\psi_v$, which is identified by some <em>other</em> column matrix $M^\prime$,</p>
$$
M 
\xrightarrow{\displaystyle\Phi_\mathcal{B}^{-1}} T 
\xrightarrow{\displaystyle\Psi_{\mathcal{L}(V, W)}} \psi_v 
\xrightarrow{\displaystyle\Phi_\mathcal{B}} M^\prime.
$$<p>In this diagram, the basis-conscious bijection $\Phi_\mathcal{B} : \mathcal{L}(V, W) \to \mathbb{F}^{(\dim W) \times (\dim V)}$ lives up to 3.9, but we silently adopted the canonical translation $\Psi_U$ of arbitrary vectors into linear maps in 3.10,</p>
$$
\Psi_U : U \to \mathcal{L}(\mathbb{F}^1, U) \;\; \text{s.t.} \;\; \Psi_U(u) = \psi_u \; \forall \, u \in U.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.12. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>This makes sense when the vector space $U$ is finite-dimensional, as is the case whenever $U = \mathcal{L}(V, W)$ for finite-dimensional $V$ and $W$; the fact that we always interpret finite-dimensional vectors as column matrices is what makes this case of $\Psi_U$ &ldquo;canonical.&rdquo; In other cases where matrix representations make no sense (e.g. the linear map of the Fourier transform $\mathcal{F}$ from 3.6), the choice of $\Psi_U$ will have to be more conscientious.</p>
</div>
</div>
<h4 id="linear-forms">Linear Forms</h4>
<p>In a way that is &ldquo;dual&rdquo; to the statement 3.10, one could look at another representation which, while not as standard as mapping onto linear maps of column-matrix form (and hence non-canonical), can be seen as equally valid. Namely, one could map vectors to linear maps of row-matrix form,</p>
$$
\Psi_U^* : U \to \mathcal{L}(U, \mathbb{F}^1) \;\; \text{s.t.} \;\; \Psi_U^*(u) = \varphi_u \; \forall \, u \in U.
$$<p>That is, for each vector $u \in U$, we can choose to represent it as a linear map $\varphi_u : U \to \mathbb{F}^1$. Back to the line of thought in 3.10, we see that when a basis is fixed, any such $\varphi_u$ can be identified by a $1 \times n$ matrix (again by 3.8). This can be illustrated quite simply for finite dimensions:</p>
$$
\varphi_u(v) = 
\begin{bmatrix}
u_1 \quad
u_2 \quad
\dots \quad
u_{(\dim U)}
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{(\dim U)}
\end{bmatrix} =
\begin{bmatrix}
\sum_{i \, = \, 1}^{\dim U} u_i v_i
\end{bmatrix}.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.13. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Just as stated in 3.12 for $\Psi_U$, motivating the form $U \to \mathbb{F}^1$ from row matrices in the case of vector spaces $U$ of infinite dimension does not always work, as matrix representations sometimes make no sense there. Hence, the concrete choice of $\Psi_U^*$ may require deeper consideration (e.g. in uncountably-infinite dimensions).</p>
</div>
</div>
<h4 id="dual-spaces">Dual Spaces</h4>
<p>All linear maps $\varphi_u : U \to \mathbb{F}^1$ receive the name of <a href="https://en.wikipedia.org/wiki/Linear_form">linear forms</a> on the space $U$. After 3.8, this is no more than the vector space</p>
$$
 U^* = \{ \, \varphi : U \to \mathbb{F}^1 \; | \; \varphi \text{ is linear} \}.
$$<p>This is called the <a href="https://en.wikipedia.org/wiki/Dual_space">dual vector space</a> of $U$, which receives the special notation $U^*$ due to how naturally it arises. Much like elements of $U$ can be represented by column matrices, the elements of $U^*$ (which are called covectors) can be represented by row matrices (wherever matrices make sense).</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.14. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>In any Hilbert space $H$, for every continuous linear form $\varphi \in H^*$, there is a unique vector $u \in H$ with</p>
$$
 \forall \, v \in H, \; \varphi(v) = \langle u, v \rangle.
$$<p>Furthermore, $\| u \| = \| \varphi \|$. This gives a natural identification between elements of $H$ and $H^*$ by the canonical bijection $J : u \mapsto \varphi$. Here, we say &ldquo;canonical&rdquo; because it is provided uniquely by the inner product. This is the statement of the <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representation theorem</a>.</p>
</div>
</div>
<p>It is not difficult to see that for any vector space $V$ over a field $\mathbb{F}$, in fact, $\Psi_V$ and $\Psi_V^*$ are both bijections. Since they are canonical (in the sense that they are naturally defined), it is only right to assert that</p>
$$
V \cong \mathcal{L}(\mathbb{F}^1, V) \;\; \text{and} \;\; V^* \cong \mathcal{L}(V, \mathbb{F}^1).
$$<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="frigyes-riesz.jpg"
         alt="Frigyes Riesz (January 22, 1880 – February 28, 1956)" width="256"/> <figcaption>
            <p>Frigyes Riesz (January 22, 1880 – February 28, 1956)</p>
        </figcaption>
</figure>

</div>

<h4 id="multilinearity">Multilinearity</h4>
<p>We can continue talking about linearity in maps even when they have multiple arguments. For a map $T$ from multiple vector spaces $V_i$ into another $W$ (all over some field $\mathbb{F}$) where</p>
$$
T : V_1 \times \dots \times V_n \to W \;\; \text{s.t.} \;\; T(v_1, \, \ldots, \, v_n) = w,
$$<p>we say that $T$ is linear in an argument $v_i$ if, for all other arguments $v_j \neq v_i$, fixing $v_j$ makes the altered map $T^\prime : V_i \to W$ linear. If such a map $T$ is linear in all of its $n$ arguments it is called $n$-linear, and all maps like this are called <a href="https://en.wikipedia.org/wiki/Multilinear_map">multilinear maps</a>. The set of multilinear maps of this form is denoted</p>
$$
\mathcal{L}(V_1, \, \ldots, \, V_n; \, W) = \{\, T : V_1 \times \dots \times V_n \to W \; | \; \text{ T is linear} \}.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.15. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Matrix-vector multiplication can be seen as a bilinear ($2$-linear) map,</p>
$$
B : M_{m \times n}(\mathbb{F}) \times \mathbb{F}^n \to \mathbb{F}^m.
$$<p>This is with the understanding that $m$-by-$n$ matrices with entries in $\mathbb{F}$, in other words $M_{m \times n}(\mathbb{F})$, indeed form a vector space over the same field $\mathbb{F}$. One can verify that fixing either argument (the vector or the matrix) forms a linear map with the other argument.</p>
</div>
</div>
<p>We may also extend linear forms with the same treatment that led us to multilinear maps. In particular, if an $n$-linear map over a field $\mathbb{F}$ has the codomain of $\mathbb{F}$ itself, it is called an $n$-linear form (where all maps like this are called <a href="https://en.wikipedia.org/wiki/Multilinear_form">multilinear forms</a>).</p>
<h4 id="tensor-product">Tensor Product</h4>
<p>If one has two vector spaces $V$ and $W$ over the same field, one can naturally talk about their cartesian product $V \times W$ (as we have been doing in the case of multilinear maps). But instead of doing that, one can talk about a third vector space $V \otimes W$ (called the <a href="https://en.wikipedia.org/wiki/Tensor_product">tensor product</a> of $V$ and $W$) which, while having just as much expressiveness as $V \times W$, of course has the added benefit of being a vector space itself.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.16. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let $\varphi : V \times W \to V \otimes W$ be a bilinear map. Then, for each bilinear map $h : V \times W \to Z$ (into another vector space $Z$), there is a unique linear map $\tilde h : V \otimes W \to Z$ such that $h = \tilde h \circ \varphi$. This is referred to as the <a href="https://en.wikipedia.org/wiki/Universal_property">universal property</a> of the tensor product, which justifies the phrase &ldquo;just as much expressiveness.&rdquo;</p>
</div>
</div>
<p>Every tensor product $V \otimes W$ is equipped with such a bilinear map $\varphi : V \times W \to V \otimes W$ that allows the construction of vectors in $V \otimes W$. The <a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a> is an example of this in finite dimensions, but in other cases one must be more creative. Confusingly, this map $\varphi$ is also called a tensor product, and $\otimes$ is predominantly used instead of $\varphi$ in notation. Summarizing,</p>
$$
\forall (v, w) \in V \times W, \; \varphi(v, w) = v \otimes w \quad \text{where} \quad v \otimes w \in V \otimes W.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.17. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Even if the tensor product among vectors is not strictly commutative, there are canonical isomorphisms among permutations of tensor products of vector spaces. That is, for any permutation $\sigma$,</p>
$$
V_1 \otimes \cdots \otimes V_n \;\cong\; V_{\sigma(1)} \otimes \cdots \otimes V_{\sigma(n)}.
$$<p>Due to this symmetry, we often write tensor products as if they were commutative without loss of generality. But when one talks about actual computation or representations, order will probably matter.</p>
</div>
</div>
<p>Totally, through 3.16 and 3.17, the tensor product is precisely designed to &ldquo;linearize&rdquo; multilinear maps. To elaborate, for any multilinear map $h \in \mathcal{L}(V_1, \, \ldots, \, V_n; \, W)$, there exists a unique linear map<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
$$
\tilde h : \bigotimes_i V_i \to W \;\; \text{s.t.} \;\; \tilde h(v_1 \otimes \cdots \otimes v_n) = h(v_1, \ldots, v_n).
$$<h4 id="more-matrices">More Matrices</h4>
<p>Being now able to identify every multilinear map with a unique linear map over a tensor product space, it is possible to assert that 3.8, 3.9, and 3.10 also apply to tensor product spaces. I will reiterate the notes, dressing them up specifically for the case of tensor product spaces.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.18. Specialization of 3.8</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The set of linear maps from a tensor product space $\bigotimes_i V_i$ over the field $\mathbb{F}$ to another vector space $W$ over $\mathbb{F}$ forms a vector space over $\mathbb{F}$. Symbolically,</p>
$$
\mathcal{L}({\textstyle\bigotimes}_i V_i, W) = \left\{ \, T : \bigotimes_i V_i \to W \;\; \bigg| \;\; T \text{ is linear}  \right\}
$$<p>is a vector space over $\mathbb{F}$. We denote the operator case as $\mathcal{L}(\bigotimes_i V_i) = \mathcal{L}(\bigotimes_i V_i, \, \bigotimes_i V_i)$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.19. Specialization of 3.9</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The basis-induced isomorphism $\mathcal{L}(\bigotimes_i V_i, W) \cong \mathbb{F}^{(\times_i \dim V_i) \times (\dim W)}$ (where $V$ and $W$ are finite-dimensional over $\mathbb{F}$) exists, but is canonical only through a generalization of matrix multiplication (seen in 3.41). To be clear, 3.9 is still true in that $\mathcal{L}(\bigotimes_i V_i, W) \cong \mathbb{F}^{(\Pi_i \dim V_i) \times (\dim W)}$, but it is not canonical for tensor product spaces.</p>
</div>
</div>
<p>Often, $(\dim V_1, \, \ldots, \, \dim V_n, \, \dim W)$ from 3.19 is referred to as the shape of the matrix. Each entry of the shape tuple can be viewed as the sidelength of a pictographical embedding of the matrix in $\mathbb{R}^{n + 1}$. For example, the matrix in $\mathbb{R}^{2 \times 3}$</p>
$$
M = 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}
$$<p>is said to have shape $(2, 3)$ &ndash; once it is &ldquo;drawn on paper,&rdquo; its &ldquo;sidelengths&rdquo; are $2$ and $3$. Keeping the spirit of &ldquo;pictographical&rdquo; representation, people often call this a $2$-dimensional array, as it can be neatly &ldquo;drawn&rdquo; on two dimensions. But concretely, this matrix corresponds to a $6$-dimensional linear map. This ambiguity is often resolved by calling each shape entry an &ldquo;axis&rdquo; instead of a dimension, understanding that it refers to the visual axis of $\mathbb{R}^n$ where we would pictographically embed it.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.20. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Software libraries that represent matrices often make the choice of calling them either $n$-dimensional arrays or simply tensors in an effort to maintain generality. Arguably, these terms are both misnomers. Personally, I think they should have just called them all matrices.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.21. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Appending a trailing \(1\) to the shape of a matrix does not change the underlying object. Concretely, a matrix of shape \((\alpha_1, \, \ldots, \, \alpha_n)\) can be naturally identified with one of shape \((\beta_1, \, \ldots, \, \beta_n)\) when $\prod_i \alpha_i = \prod_i \beta_i$. This is a result of the (finite-dimensional) isomorphisms included in the scope of</p>
$$
\prod_i \dim \Alpha_i = \prod_i \dim \Beta_i \iff \bigotimes_i \Alpha_i \;\cong\; \bigotimes_i \Beta_i \, .
$$</div>
</div>
<p>Some bijections of the form $f : \bigotimes_i \Alpha_i \to \bigotimes_i \Beta_i$ are often referred to as &ldquo;reshapings.&rdquo; This concept is hard to formalize, but pictographically graspable. For example, this is one way to reshape $M$:</p>
$$
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}
\xrightarrow{f}
\begin{bmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
\end{bmatrix}
\xrightarrow{g}
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
\end{bmatrix}.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.22. Specialization of 3.10</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Every vector $v$ in a tensor product space $\bigotimes_i V_i$ can be seen as a linear map from the sapce $\mathbb{F}^1$ into $\bigotimes_i V_i$ through the definition</p>
$$
\psi_v : \mathbb{F}^1 \to \bigotimes_i V_i \, , \;\; \psi_v(\lambda) = v \lambda.
$$<p>With a basis for $\bigotimes_i V_i$ fixed, the map $\psi_v$ can be represented by a matrix of shape $(1, \, \Pi_i V_i)$ (invoking 3.19). But for tensor product spaces, it is canonical to represent $\psi_v$ using a matrix of shape $(\dim V_1, \, \ldots, \, \dim V_n)$ (invoking 3.21). We will see why in 3.41.</p>
</div>
</div>
<h4 id="homogeneous-tensors">Homogeneous Tensors</h4>
<p>Many people refer to vectors in tensor product spaces as tensors, especially in computationally-oriented scientific disciplines. This population has recently gained numerosity (and maybe even majority) thanks to the increasing availability of efficient computers and their applications. Prior to taking that perspective, we will understand <a href="https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)">tensors</a> as linear maps associated with a single vector space $V$ over $\mathbb{F}$ of the form</p>
$$
\begin{equation}
 T_{n}^{\, m} : (\times^m \, V^*) \times (\times^n \, V) \to \mathbb{F}.
\end{equation}
$$<p>Here, $(m, \, n)$ is named the &ldquo;type&rdquo; of the tensor $T$, where $m + n$ is referred to as $\text{rank}(T)$. This makes a $2$-linear map like $m : \mathbb{R}^2 \times \mathbb{R}^4 \to \mathbb{R}$ not (yet) identifiable as a tensor, since it is not of tensor form and there is no canonical isomorphism to help us coerce its form into $(8)$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.23. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The tensor space of type $(m, \, n)$ defined over a vector space $V$ is denoted<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
$$
T_{n}^{\, m}(V) = \mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, \mathbb{F}).
$$</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.24. Examples</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let us observe the types of some known tensors of form $T_{n}^{, 0} : (\times^n \, V) \to \mathbb{F}$.</p>
<ol>
<li>Any scalar $\lambda : a \mapsto \lambda a$ is a tensor of type $(0, \, 0)$.</li>
<li>Any linear form $\varphi : v \mapsto \varphi(v)$ is a tensor of type $(0, \, 1)$.</li>
<li>Any quadratic form $q : (v, w) \mapsto v^\top A w$ is a tensor of type $(0, \, 2)$.</li>
<li>Any inner product $\langle \cdot, \cdot \rangle : (v, w) \mapsto \mathbb{F}$ is a tensor of type $(0, \, 2)$.</li>
</ol>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-structures-of-squares-1974.png"
         alt="Vera Molnár, &lsquo;Structures of Squares&rsquo; (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, &lsquo;Structures of Squares&rsquo; (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="tensor-identification">Tensor Identification</h4>
<p>When the definition of a multilinear map involves many vector spaces, there will continue to be a lack of such a canonical isomorphism (at least using the insights we currently have). But if we consider arbitrary multilinear maps defined over a single vector space $V$ over a field $\mathbb{F}$ (even those without a codomain $\mathbb{F}$ ), we will see that canonically</p>
$$
\begin{equation}
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* ))
\cong
\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right),
\end{equation}
$$<p>where we always interpret $\times^0 \, V = \mathbb{F}$ and $\otimes^0 \, V = \mathbb{F}^1$ (as always, with $\mathbb{F} \cong \mathbb{F}^1$). This essentially claims we can uniquely identify vector-valued multilinear maps defined over a single vector space $V$ with vectors in a tensor product space defined only over $V$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.25. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>As a result of 3.16, for all vector spaces $V$, $W$, and $Z$, the tensor product canonicalizes</p>
$$
\mathcal{L}(V \otimes W, \, Z) \cong\ \mathcal{L}(V, \, W; \, Z).
$$</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.26. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Due to 3.14, any Hilbert space is canonically isomorphic to its own dual by the bijection</p>
$$
\Phi : V \to V^* \;\; \text{s.t.} \;\; \Phi[v](w) = \langle v, w \rangle_V
$$<p>where $u, w \in V$. Above, $\Phi$ is exactly $J$ fom 3.14 and is canonical by convention in abstract contexts. Outside of an entirely abstract context, one only needs to &ldquo;manually&rdquo; specify a canonical $\Phi$ to make $V \cong V^*$ canonical (this is a subtle point which will be important for 3.36).</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.27. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>For any Hilbert space $V$, we have that $V^* \otimes V^* \cong (V \otimes V)^*$ canonically. This is supported by the following standard choice of bijection</p>
$$
\Phi : V^* \otimes V^* \to (V \otimes V)^* \;\; \text{s.t.} \;\; \Phi[\psi \otimes \varphi](v \otimes w) \mapsto \psi(v) \varphi(w),
$$<p>where $\psi, \varphi \in V^*$ and $v, w \in V$. To be clear, $\Phi[\psi \otimes \varphi]$ is in $(V \otimes V)^*$ and has the form $V \otimes V \to \mathbb{F}$.</p>
</div>
</div>
<p>To show $(9)$, our strategy will be to consider an arbitrary multilinear map $T$ in the set</p>
$$
\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V) = 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* )).
$$<p>We will introduce a bijection $\tilde \Gamma$ that identifies a unique tensor of type $(m + a, \, n + b)$ for each $T$. Then, we will introduce another bijection $\hat \Gamma$ to show that $T_{n + b}^{\, m + a}(V)$ is isomorphic to $\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right)$. Summarizing, we will construct an invertible trip</p>
$$
\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V) 
\xrightarrow{\displaystyle \tilde \Gamma} 
T_{n + b}^{\, m + a}(V) 
\xrightarrow{\displaystyle \hat \Gamma} 
\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right).
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.28. Demonstration</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let us first consider $\tilde \Gamma$. Given a $T \in \mathcal{L}_{(n, \, m)}^{\, (a, \, b)}(V)$, define $\tilde T^\prime$ where</p>
$$
\begin{align*}
\tilde T^\prime : \; & (\times^m \, V^*) \times (\times^n \, V) \times (\left( \otimes^a \, V \right) \otimes \left( \otimes^b \, V^* \right))^* \to \mathbb{F} \\
& \text{s.t.} \;\; \tilde T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n,  \, \varphi) = \varphi(T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n)),
\end{align*}
$$<p>where $v_i \in V^*$, $w_i \in V$, and $\varphi \in (\left( \otimes^c \, V \right) \otimes \left( \otimes^d \, V^* \right))^*$ is the covector of $T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n)$ (which can be canonically determined by 3.26). Then, observe that</p>
$$
\begin{align*}
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, & \,\, V_{(1)}, \, \dots, \, V_{(n)}, \, (( \otimes^a \, V ) \otimes ( \otimes^b \, V^* ))^*; \, \mathbb{F}) \\
&\overset{\text{3.27}}{\cong} 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \,\, V_{(1)}, \, \dots, \, V_{(n)}, \, ( \otimes^a \, V^* ) \otimes ( \otimes^b \, V ); \, \mathbb{F}) \\
&\overset{\text{3.25}}{\cong} 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m + a)}^*, \,\, V_{(1)}, \, \dots, \, V_{(n + b)}; \, \mathbb{F}).
\end{align*}
$$<p>Using this, a map $\tilde T$ can be uniquely constructed from $\tilde T^\prime$ (invoking 3.25 and 3.17 for argument order), where</p>
$$
\tilde T : (\times^{m + a} \, V^*) \times (\times^{n + b} \, V)\to \mathbb{F}.
$$<p>This finalizes the definition of $\tilde \Gamma : T \mapsto \tilde T$. Each step of this process is bijective, therefore $\tilde \Gamma$ is bijective.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.29. Demonstration</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Now we can consider $\hat \Gamma$. Define the tensor $T \in T_{n + b}^{, m + a}(V)$ such that by definition</p>
$$
T : (\times^{m + a} \, V^*) \times (\times^{n + b} \, V) \to \mathbb{F}.
$$<p>By repeated application of 3.25, we can always use $T$ to construct a unique</p>
$$
\hat T^\prime : (\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V) \to \mathbb{F}.
$$<p>This is clearly a linear form. In other words, $\hat T^\prime \in ((\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V))^*$. But by 3.27, there is a unique</p>
$$
\hat T \in (\otimes^{m + a} \, V) \otimes (\otimes^{n + b} \, V^*)
$$<p>for each $\hat T^\prime$ we could construct. This finalizes the definition of $\hat \Gamma : T \mapsto \hat T$. Each step above is bijective, so $\hat \Gamma$ is itself a bijection.</p>
</div>
</div>
<p>We have shown that vector-valued multilinear maps defined on a single vector space (such as operators) do identify tensors uniquely, despite not being of tensor form. Also, we have shown how tensors uniquely identify elements of tensor product spaces defined on a single vector space. This is why it is normal refer to all of these objects as tensors.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.30. Examples</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Equation $(9)$ provides a formula for identifying the tensor type of elements in $\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V)$. Indeed, tensor type acts like an algebraic signature for these objects.</p>
<ol>
<li>Linear operators $T : V \to V$ identify type $(1, 1)$ tensors.</li>
<li>The tensor product $\otimes : V \times V \to V \otimes V$ identifies a type $(2, 2)$ tensor.</li>
</ol>
<p>For linear operators (represented by square matrices), we can apply $(9)$ and observe they identify elements of $V \otimes V^*$ as they are of type $(1, 1)$. Then, square matrix multiplication represents the map</p>
$$
T : (V \otimes V^*) \times (V \otimes V^*) \to (V \otimes V^*) \;\; \text{s.t.} \;\; T(A, \, B) = AB,
$$<p>where $A, B \in \mathcal{L}(V)$. But due to 3.25, we can reform $T : (\times^2 \, V^*) \times (\times^2 \, V) \to V \otimes V^*$. Applying $(9)$ one more time, we observe that this is a type $(3, 3)$ tensor. This illustrates how the tensor type of a multilinear map defined over other tensor product spaces can be computed.</p>
</div>
</div>
<h4 id="heterogeneous-tensors">Heterogeneous Tensors</h4>
<p>We have used algebraic pathways to extend the idea of a tensor beyond scalar-valued multilinear maps. Our primary tool was the canonical isomorphism, which allowed us to ignore many specifics in all cases. Now, we will widen the concept of a tensor to involve many vector spaces with linear maps of form</p>
$$
\begin{equation}
T_{B}^{\, A} : (\times_A \, V_i^*) \times (\times_B \, V_i) \to \mathbb{F}.
\end{equation}
$$<p>Here, we reference two indexed collections of vector spaces (over the same field $\mathbb{F}$), $\langle V_i \rangle_A$ and $\langle V_i \rangle_B$. We differentiate these tensors by calling them heterogeneous, since they involve different vector spaces. They share much theory with the homogeneous case, resulting in the isomorphism</p>
$$
\begin{equation}
\mathcal{L}(\langle V_i^* \rangle_{A}, \, \langle V_i \rangle_{B}; \, ( \otimes_{C} \, V_i^* ) \otimes ( \otimes_{D} \, V_i ))
\cong
\left( \otimes_{A \cup D} \, V_i \right) \otimes \left( \otimes_{B \cup C} \, V_i^* \right).
\end{equation}
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.31. Explanation</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>For the sake of brevity, this will be hand-wavy. The main insight is to quotient $\mathcal{V} = A \cup B \cup C \cup D$ by simple equality. For each partition in $\mathcal{V}_k \in (\mathcal{V}/=)$ of vector spaces associated with a map</p>
$$
T \in \mathcal{L}(\langle V_i^* \rangle_{A}, \, \langle V_i \rangle_{B}; \, ( \otimes_{C} \, V_i ) \otimes ( \otimes_{D} \, V_i )),
$$<p>fix an argument for all vector spaces in $\mathcal{V} \setminus \mathcal{V}_k$, creating a new multilinear map</p>
$$
\tilde T_k \in \mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* )),
$$<p>where $\mathcal{V}_k = [V]$ in $(\mathcal{V} / =)$ and $|\mathcal{V}_k| = a + b + m + n$ (skipping additional bookeeping). By 3.28 and 3.29, $\tilde T_k$ identifies a homogeneous tensor. Since all $\tilde T_k$ can be uniquely transformed this way, we can define a new $\bar T$ that has the cartesian product of their domains as its own domain with codomain $\mathbb{F}$ (by multilinearity). Finally, 3.25 gives the desired form (the RHS of $(11)$). These steps were through isomorphisms, hence $(11)$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.32. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Just like $(9)$ relies on a canonical isomorphism $V \cong V^*$ (see 3.26), $(11)$ requires an equivalent restriction for the vector spaces involved.</p>
</div>
</div>
<p>It is still possible to consider tensor type in the heterogeneous case &ndash; one simply has to keep track of one type tuple per $[V] \in (\mathcal{V}/=)$ (see 3.31). This embodies a perspective where heterogeneous tensors are informally &ldquo;superpositions&rdquo; of homogeneous tensors (via the tensor product). Observing this, we can write the type of a heterogeneous tensor $T$ as</p>
$$
\langle (m_V, \, n_V) \rangle_{[V] \, \in \, (\mathcal{V} /= )} \;\; \text{s.t.} \;\; \text{rank}(T) = \langle \, \text{rank}_V(T) \, \rangle_{[V] \, \in \, (\mathcal{V} /= )} \;\; \big( \, \text{rank}_V(T) = m_V + n_V \, \big).
$$<p>With heterogeneous tensors, one must also carry a mapping of type index to corresponding vector space. Without this, we would not know which vector space each type tuple $(m_i, \, n_i)$ corresponds to. So when it is not clear to use the above syntax, we just write the type as $(A, \, B)$ as implied from $(10)$.</p>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-untitled-1974.png"
         alt="Vera Molnár, Untitled (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, Untitled (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="tensor-contractions">Tensor Contractions</h4>
<p>The statements of $(9)$ and $(11)$ may initially seem like a cryptic justification of our choice of vocabulary; they justify why we use the word &ldquo;tensor&rdquo; so liberally, with the most general use being in reference to an element of a heterogeneous tensor product space (up to isomorphism).</p>
<p>But beyond justifying use of language, $(9)$ and $(11)$ also provide a clear perspective on computation with tensors. These isomorphisms specify an &ldquo;exchange rate&rdquo; between inputs and outputs of homogeneus and heterogeneous tensors &ndash; using 3.26, one can algebraically &ldquo;trade&rdquo; a tensor input in $V$ for a tensor product evaluation with a canonical element of $V^*$ in the output as many times as desired while maintaining type.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.33. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Consider a vector $v \in V$. Earlier, 3.10 showed that this can be seen as a linear map $\psi_v : \mathbb{F} \to V$. Indeed, we can say that $v$ is a vector of type $(1, 0)$ by application of $(9)$ (which helps identify $\psi_v$ with a map in the form of $(8)$, providing its unique tensor type). Informally, we traded an application of $\cdot \otimes V$ in the codomain $V$ (turning it into $\mathbb{F}$ via $\otimes^0 \, V \cong \mathbb{F}$) for an argument in $V^*$ to the domain $\mathbb{F}$ (recall $\times^0 \, V \cong \mathbb{F}$) to finally identify</p>
$$
\hat \psi_v : V^* \to \mathbb{F} \;\; \left( \, \text{s.t.} \;\; \hat \psi_v :  \times^{(0 \, + \, 1)} \, V^* \to \otimes^{(1 \, - \, 1)} \, V \, \right).
$$</div>
</div>
<p>This becomes especially powerful in the context of composition. Indeed, we can legally do &ldquo;trades&rdquo; of this kind (even disregarding argument order by 3.17) to reorganize and compose tensors as needed. In other words, we may be able to compose the same two tensors in surprisingly many different ways after we use these &ldquo;trades&rdquo; to view each of them as one of many different linear maps they can represent.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.34. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Consider two linear operators $f, \, g \in \mathcal{L}(V)$. They are of form $V \to V$ and have type $(1, 1)$. Without loss of generality, apply $(9)$ to $f$ and $g$ to identify</p>
$$
\hat g : \mathbb{F} \to V^* \otimes V \;\; \text{and} \;\; \hat f : V^* \otimes V \to \mathbb{F}.
$$<p>(Recall that 3.25 allows us to identify $\hat f$ from the form $V^* \times V \to \mathbb{F}$.) Then, we may compose $\hat f \circ \hat g$, which is a tensor of type $(0, 0)$ (the type of a scalar). But we could have just as easily identified</p>
$$
\tilde g : V^* \otimes V \to \mathbb{F} \;\; \text{and} \;\; \tilde f : \mathbb{F} \to V^* \otimes V,
$$<p>in which case $\tilde f \circ \tilde g$ would be a tensor of type $(2, 2)$. As a final case, we could compose $f$ and $g$ as defined to obtain $f \circ g$, a tensor of type $(1, 1)$. Hence, can obtain tensors of type $(0, 0)$, $(1, 1)$, and $(2, 2)$ from $f$ and $g$ via canonical identification and simple composition. We can even continue doing &ldquo;trades&rdquo; in these three tensors (without composition), allowing us to reach the types $(a, b)$ where $a + b \in \{0, \, 2, \, 4 \}$.</p>
</div>
</div>
<p>In the example of 3.34, the map $\otimes : (f, \, g) \mapsto \tilde f \circ \tilde g$ receives the special name of tensor outer product. It is defined for any two tensors, just as the tensor product (which it is a special case of) is defined on any two tensor product spaces. Taking the outer product of two tensors of type $(a, b)$ and $(c, d)$ results in one more of type $(a + c, \, b + d)$.</p>
<p>Likewise, the map $\langle \cdot, \, \cdot \rangle : (f, \, g) \mapsto \hat f \circ \hat g$ is simply a special case of an inner product. As such, it only made sense in 3.34 as it admitted two tensors that live in the same tensor product space. In such cases, the inner product of two tensors of (necessarily equal) type $(a, b)$ is a scalar of type $(0, 0)$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.35. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let us take inspiration in the extreme effect that the outer and inner products have in the types of their outputs, using the same tensors $f$ and $g$ as in the example of 3.34. We know from $(9)$ that we can identify</p>
$$
T_g, \, T_f \in V \otimes V^*
$$<p>from $f$ and $g$ (canonically). We will see that we can obtain the types $(2, 2)$, $(1, 1)$, and $(0, 0)$ without invoking composition, giving us a new perspective on tensor operations. First, the outer product identifies $\hat f \circ \hat g$ with the tensor of type $(2, 2)$ obtained by $T_f \otimes T_g \in V \otimes V^* \otimes V \otimes V^*$.</p>
<p>The next key concept is the evaluation map, which is made canonical by convention. It is defined as the tensor $\text{ev}_U : U^* \otimes U \to \mathbb{F}$ of type $(1, 1)$ such that $\text{ev}_U(\varphi \otimes u) = \varphi(u)$. We can use it to obtain another map</p>
$$
\begin{align*}
(\text{id}_V \otimes \text{ev}_V \otimes \text{id}_{V^*}) & : V \otimes V^* \otimes V \otimes V^* \to V^* \otimes V \\
& \text{s.t.} \;\; (\text{id}_V \otimes \text{ev}_V \otimes \text{id}_{V^*})(v \otimes \varphi \otimes w \otimes \phi) = \varphi(w) (v \otimes \phi),
\end{align*}
$$<p>where the tensor $(\text{id}_U \otimes \text{ev}_U \otimes \text{id}_U)(T_f \otimes T_g)$ corresponds exactly to $f \circ g$ and is of type $(1, 1)$. Similarly, applying the evaluation map a second time decreases tensor type uniformly, where we can use</p>
$$
\begin{align*}
(\text{ev}_V \otimes \text{ev}_V) : V^* & \otimes V \otimes V^* \otimes V \to \mathbb{F} \\
& \text{s.t.} \;\; (\text{ev}_V \otimes \text{ev}_V)(v \otimes \varphi \otimes w \otimes \phi) = \varphi(w) \phi(v)
\end{align*}
$$<p>with 3.17 (to disregard argument order) to get $(\text{ev}_V \otimes \text{ev}_V)(T_f \otimes T_g)$, corresponding exactly to $\tilde f \circ \tilde g$, whose type is $(0, 0)$ (a scalar). The pattern is becomes clear &ndash; the evaluation map provides a canonical way to obtain a tensor of type $(a - 1, \, b - 1)$ from another of type $(a, \, b)$, annihilating one vector-covector argument pair of our choosing (when the input tensor is viewed as a multilinear map). After, we may still perform &ldquo;trades&rdquo; on the resulting tensors (independently of any idea of composition, as described in 3.33).</p>
</div>
</div>
<p>The process of &ldquo;evaluation&rdquo; (perhaps done over many vector-covector argument pairs simultaneously) as described in 3.35 is known as a <a href="https://en.wikipedia.org/wiki/Tensor_contraction">tensor contraction</a>. Note that it can involve any number of tensors, as the atomic step is the evaluation of $\text{ev}_V$ with respect to a single vector-covector pair of arguments involved in the group of tensors. The collection of tensors involved in a contraction is referred to as a <a href="https://en.wikipedia.org/wiki/Tensor_network">tensor network</a>. The result of a contraction is of course a single tensor, which can be seen to &ldquo;compose&rdquo; the tensors in the network in arbitrarily complex ways (through the perspective in 3.34).</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.36. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Remember that $(9)$ and $(11)$ rely on the assumption that $V \cong V^*$ exists and is canonical. This allowed us to obtain tensor forms of &ldquo;redistributed&rdquo; type by &ldquo;trading,&rdquo;</p>
$$
(a, b) \xrightarrow{c \text{ trades}} (a + c, b - c).
$$<p>Recall 3.26, and that the canonicity of $V \cong V^*$ can be &ldquo;manually&rdquo; induced by providing a canonical bijection to underlie it. Many applications specify tensors which enable &ldquo;trades&rdquo; via contractions with them for this purpose. This is often done by replacing the standard inner product in a set with another bilinear map (see 3.40).</p>
</div>
</div>
<h4 id="heterogeneous-contractions">Heterogeneous Contractions</h4>
<p>We have mostly ignored heterogeneous tensor spaces. Now, the framework of contractions offers a great opportunity to pull them back onto our train of thought. We have implied two stages for finding the type of a tensor contraction. First, consider the tensor product of all the spaces involved. Then, repeatedly utilize the canonical map $\text{ev}_V$, once per pair of dual spaces involved in the contraction.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.37. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>For a homogeneous tensor network involving tensors $T_1, \, \ldots, \, T_n$ with $T_i$ of shape $(a_i, b_i)$, the tensor product of all the tensors in the network is</p>
$$
\otimes_i \, T_i \in \otimes_i \, ((\otimes^{a_i} V) \otimes (\otimes^{b_i} V^*)) \;\; \text{s.t.} \;\; T_i \in T_{b_i}^{\, a_i}(V)
$$<p>thanks to $(9)$. Applying 3.17 to reorganize, we see that $\otimes_i \, T_i$ is of type $(\Sigma_i a_i , \, \Sigma_i b_i)$. We then continue to use 3.17 and compose $\text{ev}_V$ with $\text{id}_V$ to construct mappings that perform arbitrary contractions just as done in 3.35, where together with &ldquo;trades,&rdquo; we may achieve a contraction with any type</p>
$$
(a, b) \;\; \text{s.t.} \;\; a + b = \text{rank(T)} - 2k, \;\; 0 \leq k \leq \lfloor \text{rank}(T) \, / \, 2 \rfloor.
$$</div>
</div>
<p>Here, we notice that we can use specialized maps $\text{ev}_X$ for appearances of each different vector space $X$ in a heterogeneous tensor contraction. While I will not formulate a heterogeneous equivalent of 3.37 (as it would be exceedingly verbose), we can see that for a tensor of type</p>
$$
(\{A_{(1)}^*, \, \ldots, \, B_{(1)}^*, \ldots \}, \{A_{(1)}, \, \ldots, \, B_{(1)}, \ldots \}),
$$<p>a contraction is determined by a collection of pairs $\{ (X_{(i)}^*, \, X_{(j)}), \, \ldots \}$ where maps $\text{ev}_X$ are to be used in the manner of 3.35. It requires a significant amount of bookeeping, but its soundness is visible through an argument completely analogous to 3.31. Likewise, we can apply &ldquo;trading&rdquo; as in 3.33 for &ldquo;like terms&rdquo; in their domain and codomain (pairs of vector spaces dual to each other among their factors).</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.38. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>One subtlety of the heterogeneous tensor contraction is that different vector spaces in a heterogeneous tensor network may have different canonical bijections through which &ldquo;trades&rdquo; are done, making bookeeping harder.</p>
</div>
</div>
<h4 id="syntax-and-applications">Syntax and Applications</h4>
<p>An appealing model of tensor operations was offered by <a href="https://en.wikipedia.org/wiki/Roger_Penrose">Sir Roger Penrose</a> in 1971 within the illustrated writeup <a href="https://www.mscs.dal.ca/%7Eselinger/papers/graphical-bib/public/Penrose-applications-of-negative-dimensional-tensors.pdf">Applications of Negative-Dimensional Tensors</a>. There, he provided a first theory of abstract tensor networks which he called Abstract Tensor Systems (ATS), which came with a coordinate-free system for representing homogeneous tensors and contractions. This system became known as <a href="https://en.wikipedia.org/wiki/Penrose_graphical_notation">Penrose graphical notation</a>. It provides its users with a feeling of intuitive dominance on tensor algebra.</p>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="roger-penrose.png"
         alt="Sir Roger Penrose (born August 8, 1931)" width="256"/> <figcaption>
            <p>Sir Roger Penrose (born August 8, 1931)</p>
        </figcaption>
</figure>

</div>

<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.39. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Many recent online sources give loose overviews of graphical notataion, for example <a href="https://tensornetwork.org/diagrams/">here</a> and <a href="https://www.tensors.net/intro">here</a>. However, they miss out on subtle details that this section has set the stage for. The <a href="https://www.mscs.dal.ca/%7Eselinger/papers/graphical-bib/public/Penrose-applications-of-negative-dimensional-tensors.pdf">original monograph</a> does include the aforementioned details, so I encourage you to read it after 3.42 (a notational prerequisite). Note the connection between what Penrose calls a &ldquo;Cartesian ATS&rdquo; and the availability of a canonical $V \cong V^*$ in 3.36, showing the way in which &ldquo;trades&rdquo; as we understand them are done in graphical notation.</p>
</div>
</div>
<p>By ignoring any details needed to determine tensor instances, Penrose notation enjoys better ergonomics for working with abstract tensors. But before Penrose (and the study of abstract tensors spaces itself), the early appearances of these objects first appeared in service of fields like <a href="https://en.wikipedia.org/wiki/Differential_geometry">differential geometry</a>, only being baptized as &ldquo;tensors&rdquo; at a later time by physicists.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.40. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Recall 3.36, which remarked that some applications would introduce a special tensor as a canonical bijection in the stead of a canonical link $V \cong V^*$ to help manage &ldquo;trades.&rdquo; An early example is the <a href="https://en.wikipedia.org/wiki/Metric_tensor">metric tensor</a> defined in the field of differential (Riemannian) geometry. Consider a sphere embedded in $\mathbb{R}^3$, which is the set</p>
$$
\mathcal{E}(S^2) = \left\{ p \in \mathbb{R}^3 : \|p\|_2 = r \right\}.
$$<p>Here, $r$ is the radius of the sphere and $\| \cdot \|_2$ is the $\ell^2$-norm. Above, $S^2$ is a <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>, and not a vector space &ndash; this is a completely different mathematical environment. It would be difficult to attempt a complete explanation of manifolds now, so I will summarize. A manifold $\mathcal{M}$ is in aggregate</p>
<ul>
<li>an underlying set $M$ with a topology $\tau$ (so it is also a <a href="https://en.wikipedia.org/wiki/Topological_space">topological space</a>),</li>
<li>an index $\mathcal{A} = \langle \, \varphi_\alpha : U_\alpha \to \mathbb{R}^n \, \rangle_{\alpha \, \in \, A}$ of <a href="https://en.wikipedia.org/wiki/Homeomorphism">homeomorphisms</a> with $\cup_\alpha \, U_\alpha = M$,</li>
<li>and compatibility and smoothness guarantees ensuring coherence.</li>
</ul>
<p>Above, $\mathcal{A}$ is called an atlas. This name is very accurate &ndash; just as a <a href="https://en.wikipedia.org/wiki/Atlas">real-world atlas</a> is a collection of charts that represent patches of big places in a flat page until the entire place is mapped, $\mathcal{A}$ also contains <a href="https://en.wikipedia.org/wiki/Atlas_(topology)">charts</a> $\varphi_\alpha$ that represent local patches $U_\alpha$ of $\Mu$ in a &ldquo;flat&rdquo; space $\mathbb{R}^n$. The key insight is that we can &ldquo;traverse the entire world&rdquo; of $M$ by moving between patches, allowing us to describe $M$&rsquo;s local geometry in Euclidean terms, with global consistency ensured by the compatibility of the charts.</p>
<p>If we can &ldquo;move between&rdquo; the patches of $M$ smoothly using atlas charts, we call $\mathcal{M}$ a <a href="https://en.wikipedia.org/wiki/Differentiable_manifold">differentiable manifold</a>. This lets us do calculus on $M$, describing characteristics (like curvature) that would be otherwise inaccessible. Usually, manifolds are defined in an ambient space which they are a subset of. For example, $S^2 \subset \mathbb{R^3}$. But an <a href="https://en.wikipedia.org/wiki/Manifold#:~:text=Intrinsic%20and%20extrinsic%20view%5Bedit%5D">intrinsic</a> treatment of manifolds is also possible, such that $S^2$ exists independently of $\mathbb{R}^3$. Here,</p>
$$
\mathcal{E} : S^2 \hookrightarrow \mathbb{R}^3 \;\; \text{s.t.} \;\; \mathcal{E}(p) = [x, \, y, \, z]^\top
$$<p>is an <a href="https://en.wikipedia.org/wiki/Embedding">embedding</a> mapping a point $p$ in $S^2$ to another point in $\mathbb{R}^3$, decoupling the concepts. This comes with the added benefit of being able to identify points in $S^2$ with more ergonomic &ldquo;coordinates.&rdquo; In particular, we can do a conversion to polar coordinates $P = \{ x \in [0, \pi] \times [0, 2 \pi ) \}$ where, even though there is no smooth global map $S^2 \to P$ (you cannot continuously deform a sphere into a plane), each chart we define over $P$ is smooth. This is the benefit of our &ldquo;patchwork&rdquo; strategy, so we define all $\varphi_\alpha(p)$ as restrictions $\mathcal{E} |_{U_\alpha}(p)$ of</p>
$$
\mathcal{E} : S^2 \hookrightarrow \mathbb{R}^3 \;\; \text{s.t.} \;\; \mathcal{E}(p) = \Phi(\Psi(p)) = \Phi(\theta, \, \phi) = r
\begin{bmatrix}
\sin\theta \cos\phi \\
\sin\theta \sin\phi \\
\cos\theta
\end{bmatrix}
$$<p>where $\Psi : S^2 \to P$ is a bijection between $S^2$ and the set of polar coordinates $P \in \mathbb{R}^2$. Now, we can analyze how distances in $P$ (which have an easy relationship to distances in $S^2$ by design) translate to distances in the ambient space $\mathbb{R}^3$. In doing so, we will &ldquo;pull back&rdquo; the Euclidean metric of $\mathbb{R}^3$ to get another that describes $S^2$, while our charts work entirely in the background as theoretical aid (while we use $\mathcal{E} = \Phi \circ \Psi$). We call</p>
$$
\mathcal{J} \mathcal{\Phi} : T_{(\theta, \, \phi)}P \to T_{\Phi(\theta, \, \phi)}\mathbb{R}^3 \;\; \text{s.t.} \;\; 
\mathcal{J} \mathcal{\Phi} = r \begin{bmatrix}
\cos\theta \cos\phi & -\sin\theta \sin\phi \\
\cos\theta \sin\phi & \sin\theta \cos\phi \\
-\sin\theta & 0
\end{bmatrix}
$$<p>(which is just the Jacobian of $\Phi$) the <a href="https://en.wikipedia.org/wiki/Pushforward_(differential)">pushforward</a> differential (where we denote the vector space tangent to $p$ in the manifold $M$ with $T_p M$). This map tells us how a tiny differential change from one point in $P$ is translated to another tiny differential change around its image in $\mathbb{R}^3$ under our embedding of $S^2$, which appears linear. Note that $\mathcal{J} \mathcal{\Phi}$ gives a rank $2$ matrix, since you can only &ldquo;move&rdquo; in two directions on the surface $S^2$. Finally,</p>
$$
\langle \mathcal{J} \mathcal{\Phi}(v), \, \mathcal{J} \mathcal{\Phi}(w) \rangle_{\mathbb{R}^3} 
= v^\top (\mathcal{J} \mathcal{\Phi})^\top (\mathcal{J} \mathcal{\Phi}) \, w
= \begin{bmatrix}
v_\phi & v_\theta
\end{bmatrix}
\begin{bmatrix}
r^2 & 0 \\
0 & r^2 \sin^2 \theta
\end{bmatrix}
\begin{bmatrix}
w_\phi \\
w_\theta
\end{bmatrix}
$$<p>provides a tensor $g_c : (v, w) \mapsto \langle \mathcal{J} \mathcal{\Phi}(v), \, \mathcal{J} \mathcal{\Phi}(w) \rangle_{\mathbb{R}^3}$ which, for two points differential to a point $c = (\phi, \, \theta)$ in $P$ (and hence in $S^2$ via $\Psi$), determines their geometric relationship (as it is an inner product). This defines a <a href="https://en.wikipedia.org/wiki/Tensor_field">tensor field</a> $G$ that assigns one such tensor $g_c = G(c)$ of type $(0, 2)$ per $T_c P$ which fixes the geometry of $\mathcal{M}$ independently of the ambient space $\mathbb{R}^3$. Here, $g_c$ is the (pointwise) <a href="https://en.wikipedia.org/wiki/Pullback_(differential_geometry)">pullback</a> of the $\mathbb{R}^3$ metric (given by $\langle \cdot, \cdot \rangle_{\mathbb{R}^3}$) to $S^2$ and we say that $\mathbb{R}^3$ &ldquo;induced&rdquo; a metric on $S^2$.</p>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="mark-wilson-1e90-1990.png"
         alt="Mark Wilson, &#39;1e90&#39; (1990)" width="512"/> <figcaption>
            <p>Mark Wilson, '1e90' (1990)</p>
        </figcaption>
</figure>

</div>

<p>In 3.40, the spaces $T_p M$ have no abstract canonical isomorphism to their own dual, as the inner product (which is precisely the tensor $g_p$ as described) depends on the geometry of $\mathcal{M}$. This is an example of the statement of 3.36, showing that a purely abstract treatment of tensor spaces is not always productive. But before people approached tensors abstractly, coordinate-based approaches were the norm.</p>
<p>The dominant model for tensor operations in coordinates is indisputably <a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein notation</a>. Coincidentally, Penrose introduced ATS in Einstein notation. It is a data-oriented system where indices corresponding to each argument in $V$ and $V^*$ of a tensor in scalar-valued map form (see $(8)$ and $(10)$) are tracked. Credit for its creation is given to differential geometer <a href="https://en.wikipedia.org/wiki/Gregorio_Ricci-Curbastro">Ricci-Curbastro</a>, but it was made popular by <a href="https://en.wikipedia.org/wiki/Albert_Einstein">Einstein</a> when he published a novel use of it in physics with his <a href="https://en.wikipedia.org/wiki/Einstein_field_equations">field equations</a> in 1915.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.41. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Einstein notation is very well-motivated. It will be easier to see how we could come up with it ourselves. Let us consider a tensor $m$ of type $(1, 1)$ in multilinear map form by attempting to represent it as a matrix,</p>
$$
m : V \to V \;\; \text{s.t.} \;\; M_\mathcal{B}(m) = 
\begin{bmatrix}
 m_{1,1} & m_{1,2} & m_{1,3} \\
 m_{2,1} & m_{2,2} & m_{2,3} \\
 m_{3,1} & m_{3,2} & m_{3,3}
\end{bmatrix}.
$$<p>Here, $M_\mathcal{B}(m)$ is simply the matrix representation of $m$ under the basis $\mathcal{B}$. Through 3.19, we observe that we can do this precisely because there is a function from entry indices to values which makes matrix multiplication work precisely as we expect it to, which is</p>
$$
\text{entry}_m(i, j) = m_{i,j}.
$$<p>From a computational perspective, this is a unique proxy for $m$ when considering maps in $\mathcal{L}(V, W)$ (via 3.9). For example, the <a href="https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm">Frobenius norm</a> $\| m \|_F^2 = \sum_{(i, j)} |m_{i,j}|^2$ makes use of this notation. While this made sense for a linear map of form $V \to W$, issues arise for tensors of other forms. Consider $g_c$ from 3.40,</p>
$$
g_{(\theta, \phi)}(v, w) = 
\begin{bmatrix}
v_\phi & v_\theta
\end{bmatrix}
\begin{bmatrix}
r^2 & 0 \\
0 & r^2 \sin^2 \theta
\end{bmatrix}
\begin{bmatrix}
w_\phi \\
w_\theta
\end{bmatrix}
\;\; \text{s.t.} \;\;
M_\mathcal{B}(g_{({\theta}, {\phi})}) = 
\begin{bmatrix}
r^2 & 0 \\
0 & r^2 \sin^2 \theta
\end{bmatrix}.
$$<p>If given only the matrix $M_\mathcal{B}(g_{({\theta}, {\phi})})$, there would be no way of knowing it represents the map of (inner-product) form $g_{(\theta, \phi)} : P \times P \to \mathbb{R}$ (and not an operator in $\mathcal{L}(V, V)$) precisely because it is canonical to interpret it as a map in $\mathcal{L}(V, V)$. This interpretation is only canonical because matrix-vector multiplication is defined with</p>
$$
(Av)_i = \sum_{j} A_{ij} v_j.
$$<p>Hence, one must revisit matrix multiplication to have a basis-induced canonical isomorphism between tensors and matrices (see 3.19). To this end, we will adopt the tensor contraction in coordinates as matrix multiplication for tensors. A rank-$n$ tensor will be represented as a matrix with $n$ axes (such that $n$ indices identify an entry), such that for a tensor $t$ of type $(a, b)$,</p>
$$
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_a}}
$$<p>denotes an entry of its matrix representation. Taking the tensor product of $t$ with a new tensor $g$ of type $(c, d)$ is very simple, as each entry of the product (by the same logic) requires $a + c$ upper and $b + d$ lower indices to be found. So we simply denote their tensor product</p>
$$
t \otimes g = 
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_a} \, g_{\delta_1 \, \cdots \, \delta_d}^{\gamma \, \cdots \, \gamma_c}}.
$$<p>To perform contractions as specified in 3.35, one does a weighted sum across a pair of indices, one upper and one lower (which may belong to different tensors in a tensor product). For example, one could contract $\alpha_a$ with the index $\delta_1$ in the product $t \otimes g$ to obtain</p>
$$
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a - 1}} \, 
g_{\delta_2 \, \cdots \, \delta_d}^{\gamma \, \cdots \, \gamma_c}} =
\sum_k \,
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a - 1} \, k} \,
g_{k \, \delta_2 \, \cdots \, \delta_d}^{\gamma \, \cdots \, \gamma_c}}.
$$<p>Note that in the homogeneous case, one can always do this for any upper-lower index pair. The heterogeneous case just requires knowing which upper indices can be contracted with which lower indices (which can be done by tracking which index pairs correspond to duals of the same vector space). Finally, tensors of equal type may be summed entrywise,</p>
$$
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a}}} +
{\large q_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a}}} = 
{\large h_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a}}},
$$<p>with the understanding that the tensor product is distributive over tensor sums. This completes the system that Ricci-Curbastro outlined, allowing an axiomatic treatment for tensors in coordinates. The convention to eschew summation notation in contractions (only implying it when a symbol appears twice as an index) was introduced and popularized by Einstein (effectively his only contribution to <a href="https://en.wikipedia.org/wiki/Ricci_calculus">Ricci calculus</a>):</p>
$$
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a - 1} \, k} \,
g_{k \, \delta_2 \, \cdots \, \delta_d}^{\gamma \, \cdots \, \gamma_c}} =
\sum_k \,
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_{a - 1} \, k} \,
g_{k \, \delta_2 \, \cdots \, \delta_d}^{\gamma \, \cdots \, \gamma_c}}.
$$<p>Note that at any point we can find an entry in the matrix representation by replacing all indices with values. This means that Einstein notation is simply element-wise treatment of tensor matrices, with careful consideration of contraction compatibility by use of upper and lower indices. Here, &ldquo;trades&rdquo; translate to index raising or lowering, where (depending on context) one may be allowed to</p>
$$
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_a}} \xrightarrow{\text{raise } \beta_b}
{\large t_{\beta_1 \, \cdots \, \beta_{b - 1}}^{\alpha_1 \, \cdots \, \alpha_a \, \beta_b}} \xrightarrow{\text{lower } \beta_b}
{\large t_{\beta_1 \, \cdots \, \beta_b}^{\alpha_1 \, \cdots \, \alpha_a}}.
$$<p>If allowed to raise or lower indices (which is context-dependent as seen in 3.36 and 3.40), a contraction can be done on two lower or upper indices. So summation (and legality of index lowering or raising) remains implied in cases where $V \cong V^*$ canonically (which Penrose called &ldquo;cartesian&rdquo; in ATS). For example,</p>
$$
{\large t_{\beta_1 \, \cdots \, \beta_b \, k}^{\alpha_1 \, \cdots \, \alpha_{a}} \,
g_{\delta_1 \, \cdots \, \delta_d \, k}^{\gamma \, \cdots \, \gamma_c }} =
\sum_k \,
{\large t_{\beta_1 \, \cdots \, \beta_b \, k}^{\alpha_1 \, \cdots \, \alpha_{a}} \,
g_{\delta_1 \, \cdots \, \delta_d \, k}^{\gamma \, \cdots \, \gamma_c }}.
$$</div>
</div>
<p>Notice that the &ldquo;reduction&rdquo; done by way of summation in a tensor contraction can use any operation with the effect of <a href="https://en.wikipedia.org/wiki/Fold_(higher-order_function)">folding</a> a collection (returning a single value from multiple values of the same type), although this may break any and all algebraic invariants (depending on context). This could look like</p>
$$
w_{abc}^{ik} = \text{reduce}_j \left( v_{a b c}^{i j k} \right).
$$<p>Since the user is giving up all algebraic gurantees, they may annihilate any index (upper or lower). This is simply treating the tensor matrix as data. But it shows that Einstein notation maintains its richness beyond pure tensor algebra, and is even generalizable to infinite-dimensional tensors via wise choices of $\text{reduce}$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.42. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The cornerstone of the <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">transformer architecture</a> is generally regarded to be the <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention mechanism</a>. This is no more than a tensor-valued function $\mathbf{A} : \mathbb{R}^{d \times n} \to \mathbb{R}^{d_v \times n}$ with tunable parameters, generally defined as</p>
$$
\mathbf{A}(X) = \text{softmax}\left(\frac{(KX)^\top (QX)}{\sqrt{d_k}}\right) VX.
$$<p>Here, $X \in \mathbb{R}^{d \times n}$, and the matrices $Q \in \mathbb{R}^{d_k \times d}$ (query projection matrix), $K \in \mathbb{R}^{d_k \times d}$ (key projection matrix), and $V \in \mathbb{R}^{d_v \times d}$ (value projection matrix) are optimized. You rarely see it in Einstein notation as an equation of heterogeneous tensors, which helps when $X$ has many channels to mix (indexed by $c$),</p>
$$
\mathbf{A}(X)_{t}^{ic} = \text{softmax} \left(\frac{g_{jm} K_p^j X_s^{pc} Q_q^m X_t^{qc}}{\sqrt{d_k}}\right) V_k^i X_s^{kc}.
$$<p>I replaced the row-wise dot product application $(KX)^\top (QX)$ with an explicit metric tensor $g_{jm}$, which brings a subtle geometric interpretation to the foreground just by way of notation. Observing this could have motivated us to add another metric tensor $m_{c_1 c_2}$ to capture channel-wise relationships while mixing,</p>
$$
\mathbf{A}(X)_{t}^{ic} = \text{softmax} \left(\frac{g_{jm} m_{c_1 c_2} K_p^j X_s^{pc_1} Q_q^m X_t^{qc_2}}{\sqrt{d_k}}\right) V_k^i X_s^{kc}.
$$<p>With the <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>einsum</code> NumPy API</a>, we can write the above in Python still using Einstein notation. For simplicity, only a single-channel $X$ will be considered. Below, the strategy for determining the tensor <code>g</code> is left unspecified.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># einsum ignores whether indices are upper/lower </span>
</span></span><span class="line"><span class="cl"><span class="c1"># i.e. assumes raising/lowering is generally OK</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">KX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;jk,ks-&gt;js&#39;</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># keys</span>
</span></span><span class="line"><span class="cl"><span class="n">QX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ml,lt-&gt;mt&#39;</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># queries</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># attention scores (similarity under g)</span>
</span></span><span class="line"><span class="cl"><span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;jm,js,mt-&gt;st&#39;</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">KX</span><span class="p">,</span> <span class="n">QX</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">scores</span> <span class="o">=</span> <span class="n">sim</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">VX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,jt-&gt;it&#39;</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># values</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;is,st-&gt;it&#39;</span><span class="p">,</span> <span class="n">VX</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div>
</div>
</div>
<h4 id="overview-2">Overview</h4>
<p>We began this section by organizing the concepts of matrices, linear maps, and vectors. We then took a look at linear forms and dual spaces, being careful of infinite-dimensional cases. Later, we expanded the concept of linearity to multilinear maps, and showed how the tensor product &ldquo;linearizes&rdquo; them. That way, we saw how the way we organized matrices, vectors, and linear maps early on applied to tensor product spaces in very natural ways. Finally, tensors allowed us to speak about these objects as unified under a single umbrella woven by tight isomorphisms.</p>
<p>Although this trajectory was predominantly abstract, we were also exposed to practical notation and uses of tensors in coordinates, with examples from the early history of tensors in differential geometry and from modern applications in deep learning. These examples showed that tensors can be found at many points of the spectrum between raw data and abstract algebraic objects, and that their theory is robust to a wide range of uses &ndash; some more disrespectful than others, but never enough to erase their basic qualities.</p>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="mark-wilson-e67109-2012.png"
         alt="Mark Wilson, &#39;e67109&#39; (2012)" width="512"/> <figcaption>
            <p>Mark Wilson, 'e67109' (2012)</p>
        </figcaption>
</figure>

</div>

<h3 id="signals-and-systems">Signals and Systems</h3>
<h3 id="kernel-methods">Kernel Methods</h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>These two &ldquo;kernels&rdquo; received their names for a superficial reason &ndash; because the symbols that represent them show up inside other symbols. One could imagine that people started calling them &ldquo;kernel&rdquo; independently just to avoid saying the phrase &ldquo;that term in in the middle&rdquo; while pointing at a blackboard. As such, the connecting view of the convolution and reproducing kernels in this piece does not &ldquo;generalize&rdquo; to kernels in other contexts (many of which received their names the same non-profound reason).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>When considering infinite-dimensional vector spaces, this statement is true if and only if one admits the axiom of choice. Perhaps this was another motivation of Axler&rsquo;s restriction to finite-dimensional vector spaces.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Non-parenthesized sub-indices imply that the item is part of an indexed set &ndash; $a_i$ may not be equal to $a_j$.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Parenthesized sub-indices are only used to indicate argument index. That is, $a_{(i)} = a_{(j)}$ for all $i$ and $j$.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</article>

        </main><footer id="footer">
    Copyright © 2024 Max Fierro
</footer>
</body>
</html>
