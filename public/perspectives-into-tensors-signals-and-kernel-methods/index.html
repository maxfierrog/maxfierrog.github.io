<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
    <meta name="description" content="Max&#39;s personal site">
    
     
    <link rel="icon" type="image/x-icon" href="/favicon.ico" media="(prefers-color-scheme: light)">
    <link rel="icon" type="image/x-icon" href="/favicon-dark.ico" media="(prefers-color-scheme: dark)"> 
     
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


    
    <link rel="stylesheet" href="/css/style.min.css">

    <link rel="canonical" href="http://localhost:1313/perspectives-into-tensors-signals-and-kernel-methods/" />
    <title>Perspectives into Tensors, Signals, and Kernel Methods</title>
</head>
<body><header id="banner">
    <a href="http://localhost:1313/"
        ><img src="/logo.svg" alt="Logo" class="site-logo"
    /></a>
    <h2><a href="http://localhost:1313/">Max Fierro</a></h2>
    <nav>
        <ul>
            <li>
                <a href="/about/" title="about"
                    >about</a
                >
            </li><li>
                <a href="/resume/" title="resume"
                    >resume</a
                >
            </li><li>
                <a href="/index.xml" title=""
                    >rss</a
                >
            </li>
        </ul>
    </nav>
</header>
<main id="content">
<article>
    <header id="post-header">
        <h1>Perspectives into Tensors, Signals, and Kernel Methods</h1>
        <div>
                <time>September 8, 2025</time>
            </div>
    </header><aside id="toc">
    <details>
        <summary>&nbsp;<strong> Table of contents</strong></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#vector-spaces">Vector Spaces</a></li>
        <li><a href="#tensor-spaces">Tensor Spaces</a></li>
        <li><a href="#signals-and-systems">Signals and Systems</a></li>
        <li><a href="#kernel-methods">Kernel Methods</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </details>
</aside>

<h2 id="abstract">Abstract</h2>
<p>Linear algebra, signal processing, and machine learning methods are (one of many groups of) topics that enjoy beautiful relationships enclosed in a dense shell of mathematics. Within the shell, one finds a kernel of surprisingly diverse perspectives which are simply a pleasure to entertain. This article hopes to give the reader a romantic and thematic glimpse of the truths in this particular group of topics.</p>
<hr>
<h2 id="overview">Overview</h2>
<p>I follow an essay-like structure including an introduction, body, and conclusion. The introduction sets the stage building from &ldquo;class-style&rdquo; knowledge, assuming a first course in linear algebra, signal processing, and machine learning. For readers lacking this background, I leave pointers to free resources.</p>
<p>The introduction arrives at the <a href="https://en.wikipedia.org/wiki/Linear_map">linear operator</a> perspective of <a href="https://en.wikipedia.org/wiki/Multidimensional_system">systems of many dimensions</a>, defining system properties like <a href="https://en.wikipedia.org/wiki/Time-invariant_system">time-invariance</a> and <a href="https://en.wikipedia.org/wiki/Causal_system">causality</a> in terms of <a href="https://en.wikipedia.org/wiki/Tensor">tensor</a> representations. This results in the marriage of many linear algebra and signal processing concepts. These perspectives then help the body, where I present a cross-cutting perspective of the <a href="https://en.wikipedia.org/wiki/Convolution">convolution kernel</a> and the <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#:~:text=then%20called%20the-,reproducing%20kernel,-%2C%20and%20it%20reproduces">reproducing kernel</a>.</p>
<p>Finally, the conclusion provides some subjective thematization to the concepts emphasized in the rest of the piece, summarizing mathematical details and indulging in a little bit of sensationalism. An attempt is made of providing pointers to further reading on adjacent concepts.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >2.1. Clarification</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The word &ldquo;kernel&rdquo; is criminally polysemous in mathematics, and it will be used a lot in this piece. I mostly use the word under two semantics: The convolution kernel from signal processing and the reproducing kernel from machine learning. All other instances of the word should simply refer to its english meaning.</p>
<p>However, a primary objective of this piece is to reach a perspective of the convolution and reproducing kernels that allows them to be seen through a common lense. So an acute reader may interpret this as an explanation of why they are both called &ldquo;kernel,&rdquo; assuming that they were both named that way because they represent the same kind of mathematical object in some profound way.</p>
<p>What is interesting is that they both received their names for a superficial reason &ndash; because the symbols that represent them show up inside other symbols. One could imagine that people started calling them &ldquo;kernel&rdquo; independently just to avoid saying the phrase &ldquo;that term in in the middle&rdquo; while pointing at a blackboard.</p>
<p>As such, the connecting view of the convolution and reproducing kernels that we will work towards only applies to these two kernels (convolution and reproducing). If these were the only two kernels in mathematics, maybe one could now say that they are named the same for a profound reason. But there are numerous other kinds of kernels for which the constructions in this piece simply do not apply.</p>
</div>
</div>
<hr>
<h2 id="introduction">Introduction</h2>
<p>This section provides not much more than a definition-based refresher on select topics from first courses in linear algebra, signal processing, and machine learning. For readers in need of comprehensive review or first-time coverage, I leave these free resources on said topics:</p>
<ul>
<li><strong>Kernel methods.</strong> <a href="https://cs.nyu.edu/~mohri/mlbook/"><em>Foundations of Machine Learning</em></a> by Mohri, Rostamizadeh, and Talwalkar.</li>
<li><strong>Signals and Systems.</strong> <a href="https://ss2-2e.eecs.umich.edu/"><em>Signals &amp; Systems: Theory and Applications</em></a> by Ulaby and Yagle.</li>
<li><strong>Linear algebra.</strong> <a href="https://linear.axler.net/"><em>Linear Algebra Done Right</em></a> by Axler.</li>
</ul>
<h3 id="vector-spaces">Vector Spaces</h3>
<p>In what is nowadays close to being a canon of linear algebra education, Sheldon Axler opens with the statement below to set the stage for the rest of <em>Linear Algebra Done Right</em>:</p>
<blockquote>
<p>Linear algebra is the study of linear maps on finite-dimensional vector spaces.</p></blockquote>
<p>Here, the restriction of vector spaces to the finite-dimensional case was one the most mathematically respectful ways to negotiate generality with practical pedagogy. However, the spirit of linear algebra is alive way beyond the finite-dimensional case.</p>
<h4 id="hamel-bases">Hamel Bases</h4>
<p>Most engineers are familiar with the concept of a (Hamel) basis of a vector space. If we have a vector space $V$ over a field $\mathbb{F}$ and a Hamel basis $\mathcal{B}$, then &ldquo;$\mathcal{B}$ spans $V$&rdquo; translates to</p>
$$
\begin{equation}
    \forall v \in V, \; v = \sum_{i \, = \, 0}^{k} c_i b_i \;\; \text{s.t.} \;\; c_i \in \mathbb{F}, \, b_i \in \mathcal{B}, \, k \in \mathbb{N}.
\end{equation}
$$<p>Importantly, for $B$ to be a Hamel basis, the sum in $(1)$ must have finite terms. Note that this is allowed even in cases where $\mathcal{B}$ is infinite (or in other words, where $V$ is infinite-dimensional), as one does not necessarily assign nonzero coefficients $c_i$ to each element of $\mathcal{B}$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.1. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The vector space of polynomials (of finite terms) with coefficients in a field $\mathbb{F}$,</p>
$$
\mathbb{F}[x] = \left\{ \sum_{i \, = \, 0}^n a_i x^i \;\Big|\; n \in \mathbb{N},\ a_i \in \mathbb{F} \right\},
$$<p>has the infinite basis $\mathcal{B}_{\mathbb{F}[x]} = \{1, x, x^2, x^3, \dots \}$. Each of its elements, however, is the linear combination of a finite number of basis elements. For example, the polynomial</p>
$$ 
p(x) = 3 + 4x^2 + x^3
$$<p>can be expressed as a (finite) linear combination of basis elements,</p>
$$
p(x) = 3
\begin{bmatrix}
 1 \\
 0 \\
 0 \\
 0 \\
 \vdots
\end{bmatrix} + 4  
\begin{bmatrix}
 0 \\
 0 \\
 1 \\
 0 \\
 \vdots
\end{bmatrix} + 1 
\begin{bmatrix}
 0 \\
 0 \\
 0 \\
 1 \\
 \vdots
\end{bmatrix}.
$$<p>Here we imposed a (canonical) representation such that, for example, $x^2 = \left[ 0, \, 0, \, 1, \, 0, \, {\dots} \right]^\top$. We see that, despite each basis vector being infinite-dimensional, all polynomials are determined by a finite number of them.</p>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-molndrian-1974.png"
         alt="Vera Molnár, &lsquo;Molndrian&rsquo; (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, &lsquo;Molndrian&rsquo; (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="schauder-bases">Schauder Bases</h4>
<p>Interpreting Axler strictly, $\mathbb{F}[x]$ is already beyond linear algebra because it is of <a href="https://en.wikipedia.org/wiki/Semi-infinite">semi-infinite</a> dimension. But definitionally, it is a perfectly valid vector space. Just as finite dimensionality is not necessary in order to access the theorems of linear algebra, having a countable Hamel basis is also not necessary; all vector spaces do have a Hamel basis<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, but not all of them have a countable one.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.2. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Countable bases are desireable not for being countable per se, but rather that, in most cases where a vector space does not have a countable Hamel basis, the uncountable Hamel basis is unconstructive and unutterable. Put another way, the most useful fact about an uncountable Hamel basis, in many cases, is that it exists.</p>
</div>
</div>
<p>For some vector spaces that do not have a countable Hamel basis, one can relax the definition of a basis itself to obtain one that is countable. Specifically, we redefine the phrase &ldquo;$\mathcal{B}$ spans $V$&rdquo; to</p>
$$
\begin{equation}
    \forall v \in V, \; v = \lim_{ n \to \infty } \, \sum_{i \, = \, 0}^{n} c_i b_i \;\; \text{s.t.} \;\; c_i \in \mathbb{F}, \, b_i \in \mathcal{B}.
\end{equation}
$$<p>If the above is true for a vector space $V$ over $\mathbb{F}$, then $\mathcal{B}$ is a Schauder basis of said space. The critical difference to a Hamel basis is of course the generality afforded by the possiblity of infinite terms for the sum in $(2)$, giving us a new countably-infinite flavor of linear combination.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.3. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The vector space of square-summable sequences of real numbers,</p>
$$
\ell^2 = \left\{ (x_1, x_2, x_3, \dots) \;:\; \sum_{n \, = \, 1}^\infty |x_n|^2 < \infty \right\},
$$<p>has no Hamel basis because, no matter how you define one, you can come up with an element of $\ell^2$ which requires a decomposition into an infinite number of basis elements (which is not allowed). However, it does have the countably-infinite Schauder basis</p>
$$
\mathcal{B}_{\ell^2} = \left\{ 
\left(
 1, \,
 0, \,
 0, \,
 \dots
\right), \,
\left(
 0, \,
 1, \,
 0, \,
 \dots
\right), \,
\left(
 0, \,
 0, \,
 1, \,
 \dots
\right), \,
{\dots}
\right\}.
$$</div>
</div>
<h4 id="taxonomy-of-spaces">Taxonomy of Spaces</h4>
<p>Hidden in $(2)$ is the requirement that all such sums over basis elements converge. But the definition of a vector space does not include any operation that computes the &ldquo;closeness&rdquo; of two vectors, so additional concepts are needed to make sense of convergence. Abstractly, one needs to equip the vector space of interest with a <a href="https://en.wikipedia.org/wiki/Topological_space#topology">topology</a>. The way of doing so that we will consider is by assuming a <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> over the space, such that we can concretely declare the definition of an infinite series</p>
$$
\begin{equation}
    \lim_{n \to \infty} \left\| x - \sum_{k \, = \, 1}^n a_k \right\| = 0 \iff x = \sum_{k \, = \, 1}^{\infty} a_k.
\end{equation}
$$<p>Vector spaces that have a norm are called normed vector spaces. If a normed vector space is <a href="https://en.wikipedia.org/wiki/Complete_metric_space">complete</a> under the norm-induced <a href="https://en.wikipedia.org/wiki/Metric_space">metric</a> $d : (x_1, \, x_2) \mapsto ||x_1 - x_2||$, then it is also called a <a href="https://en.wikipedia.org/wiki/Banach_space">Banach space</a>.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.4. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Banach spaces do not necessarily have a Schauder basis. The reason for this is technical and out of scope. Additionally, not all normed vector spaces that have a Schauder basis are Banach spaces, because they may not be complete. But for the remainder of this piece, completeness can be comfortably assumed. Indeed, most of the time anyone talks about a Schauder basis in a practical context, it spans a complete space (such as $\ell^2$).</p>
</div>
</div>
<p>If a Banach space is also equipped with an <a href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a> in such a way that $\langle x, \, x \rangle = ||x||^2$, then it is also called a <a href="https://en.wikipedia.org/wiki/Inner_product_space">Hilbert space</a>. With the understanding that a vector space with an inner product defined is called an inner-product space, we arrive at the following:</p>
$$
\begin{equation}
\begin{aligned}
\text{Vector} & \text{ spaces} \\[0.2em]
&\supset
\Bigg\{
  \begin{aligned}
  & \text{Normed vector spaces} \;\supset\; \text{Banach spaces} \\
  & \text{Inner-product vector spaces}
  \end{aligned}\\[1.2em]
&\supset \text{Hilbert spaces} = (\text{Banach spaces} \, \cap \, \text{Inner-product spaces}).
\end{aligned}
\end{equation}
$$<h4 id="continuous-bases">Continuous Bases</h4>
<p>The last thing we will consider are vector spaces of uncountably-infinite dimension. So far, we have been comfortable in using syntax such as &ldquo;$x = [ 1, \, 2, \, \ldots ]^\top$&rdquo; to refer to vectors of countably-infinite dimension. This will no longer be possible with uncountably-infinite dimensions, so we must revisit our notation.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.5. Reminder</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>A vector $v$ is an abstract object, and is independent of a choice of basis. To write $v$ as a tuple $[c_1, \dots, c_n]$, one must choose a basis ${b_1, \, \dots, \, b_n}$ and expand (in the finite-dimensional case)</p>
$$
v = \sum_{i \, = \, 1}^n c_i b_i.
$$<p>Hence, $[1, \, 2, \, 3]^\top$ means “the coefficients of $v$ in this basis.”
Changing the basis changes the coefficients, but not the vector itself. This also applies to infinite-dimensional cases.</p>
</div>
</div>
<p>Observing that, once a basis is chosen, a vector in a vector space $V$ over a field $\mathbb{F}$ is determined by the coefficients in its representation as a linear combination of basis vectors, we can introduce a new type of linear combination to evolve the translation of &ldquo;$\mathcal{B}$ spans $V$&rdquo; to involve a <a href="https://en.wikipedia.org/wiki/Lebesgue_integral">Lebesgue integral</a>,</p>
$$
\begin{equation}
    \forall v \in V, \; v = \int_{\mathcal{\Omega}} c(\omega) b(\omega) \, d\mu(\omega) \;\; \text{s.t.} \;\; c : \Omega \to \mathbb{F}, \, b : \Omega \to \mathcal{B},
\end{equation}
$$<p>where a measure ${\mu}$ over $\Omega$ is provided. Here, the index $\omega$ intuitively replaces the index $i$ from $(2)$, where there is a map $c$ &ldquo;choosing&rdquo; a coefficient $c(\omega)$ and a map $b$ &ldquo;choosing&rdquo; a basis element $b(\omega)$ per index $\omega$. This way, once a map $b : \Omega \to \mathcal{B}$ and a measure $\mu$ over $\Omega$ have been agreed upon, a vector can still be represented by &ldquo;its coefficients,&rdquo; which is just the map $c$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.6. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>We can consider the Hilbert space of all square-integrable functions</p>
$$
L^2(\mathbb{R}) = 
\Bigl\{\, f:\mathbb{R} \to \mathbb{C} \; \big| \; \int_{-\infty}^{\infty} |f(t)|^2 \, dt < \infty \,\Bigr\},
$$<p>with the inner product $\langle f_1(t), \, f_2(t) \rangle = \int_{\mathbb{R}} f_1(t) \overline{f_2(t)} \, dt$. A natural choice of &ldquo;continuous basis&rdquo; for $L^2(\mathbb{R})$ is the family of complex exponentials indexed by frequency, $b_\omega(t) = e^{2 \pi i \omega t}$ with $\omega \in \mathbb{R}$. For all $f \in L^2(\mathbb{R})$,</p>
$$
f(t) = \int_{\mathbb{R}} c(\omega)b(\omega) \, d\mu(\omega) = \int_{-\infty}^{\infty} c(\omega)\, b_\omega(t)\, d\omega.
$$<p>The measure $d\omega$ is the typical Lebesgue measure on $\mathbb{R}$. Here, we see that any $f \in L^2(\mathbb{R})$ can be expressed as a continuous linear combination of basis elements in the form $e^{2 \pi i \omega t}$ which, to reiterate, are other functions parameterized by $t$ and indexed by $\omega \in \mathbb{R}$. In this case, the <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> of $f(t)$ provides $c(\omega)$:</p>
$$
c(\omega) = \int_{-\infty}^{\infty} f(t) \, \overline{b_\omega(t)} \, dt
= \int_{-\infty}^{\infty} f(t)\, e^{-2 \pi i \omega t} \, dt.
$$<p>In a finite-dimensional case, we would compute $\langle v, \, e_n \rangle$ to observe the &ldquo;contribution&rdquo; of the basis element $e_n$ in the vector $v$, obtaining the coefficient it would be assigned in its decomposition as a linear combination of basis elements. The Fourier transform does exactly the same thing per $\omega$, where $v = f(t)$ and $e_n = b_\omega(t)$:</p>
$$
c(\omega) = \langle f(t), \, b_\omega(t) \rangle.
$$<p>It is not difficult to show algebraically that the Fourier transform $\mathcal{F} : L^2(\mathbb{R}) \to L^2(\mathbb{R})$ is a linear operator over this Hilbert space. (So if it were countably-infinite dimensional, it would have a matrix representation.)</p>
</div>
</div>
<h4 id="overview-1">Overview</h4>
<p>We have expanded a finite-dimensional view of vector spaces to potentially allow those with countably- or uncountably-infinite dimensions. To do so, we had to slowly relax our concept of a linear combination from $(1)$ (which already spanned certain infinite-dimensional spaces like $\mathbb{F}[x]$), to $(2)$ (which was able to span a more countably-infinite-dimensional spaces like $\ell^2$), and finally $(5)$ (which can span uncountably-infinite dimensional spaces like $L^2(\mathbb{R})$). We have also encountered the conepts of Banach and Hilbert spaces.</p>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="david-hilbert.jpg"
         alt="David Hilbert (January 23, 1862 – February 14, 1943)" width="256"/> <figcaption>
            <p>David Hilbert (January 23, 1862 – February 14, 1943)</p>
        </figcaption>
</figure>

</div>

<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.7. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>In fact, $(1)$ is a special case of $(2)$, which is a special case of $(5)$. Therefore, it is truly a relaxation of the linear combination; at no point did we lock ourselves out of any vector spaces we could already span. Namely, in the case of $(5)$ and $(2)$ for a given vector space $V$,</p>
$$
\Omega = \mathbb{N} \iff \forall v \in V, \; v = \int_{\Omega} c(\omega)b(\omega) \, d{\mu}(\omega) = \sum_{i \, = \, 0}^{\infty} c_i b_i.
$$<p>In the case of $(2)$ and $(5)$, when all vectors in $V$ are a linear combination of a finite number of basis elements (as is the case for $\mathbb{F}[x]$),</p>
$$
\forall v \in V, \; v = \sum_{i \, = \, 0}^{\infty} c_i b_i = \sum_{i \, = \, 0}^{k} c_i b_i \;\; \text{s.t.} \;\; k \in \mathbb{N}.
$$</div>
</div>
<h3 id="tensor-spaces">Tensor Spaces</h3>
<p>Let us revisit the opening scene of <em>Linear Algebra Done Right</em>:</p>
<blockquote>
<p>Linear algebra is the study of linear maps on finite-dimensional vector spaces.</p></blockquote>
<p>Another thing to note (apart from the restriction to finite dimensions) is the lack of matrices and vectors in this description. This section will live up to this witholding; we will first look for a linear-map-centric view of the objects in linear algebra and, afterwards, gain an understanding of tensor spaces.</p>
<h4 id="vectors-and-matrices">Vectors and Matrices</h4>
<p>A regrettable aspect of a typical introduction to linear algebra is the marriage of syntax to abstract objects. Most of us were told in a first impression that a vector $v$ and a matrix $M$ may look something like this:</p>
$$
\begin{equation}
v = 
\begin{bmatrix}
1 \\
2
\end{bmatrix}, \quad 
M = 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}.
\end{equation}
$$<p>I assert that $v$ and $M$ are <em>both</em> matrices, each of which simultaneously identifies a vector and linear map. For richer support, I will establish three resources below and explain their relationship afterward.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.8. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The set of linear maps from a vector space $V$ over the field $\mathbb{F}$ to another vector space $W$ (over the same field) forms a vector space over $\mathbb{F}$. That is,</p>
$$
\mathcal{L}(V, W) = \left\{ \, T : V \to W \; | \; T \text{ is linear} \right\}
$$<p>is a vector space over $\mathbb{F}$. We denote the case of linear operators on $V$, which is simply $\mathcal{L}(V, V)$, as $\mathcal{L}(V)$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.9. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>There is a bijection between $\mathcal{L}(V, W)$ and $\mathbb{F}^{(\dim V) \times (\dim W)}$, where $V$ and $W$ are vector spaces on the same field $\mathbb{F}$ and are finite-dimensional. In other words, for each linear map $T$ from a vector space of dimension $n$ to another of dimension $m$, there is exactly one $m$-by-$n$ matrix with entries in $\mathbb{F}$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.10. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>A vector $v$ in a space $V$ over $\mathbb{F}$ can be regarded as a linear map from the space $\mathbb{F}^1$ into $V$, via</p>
$$
\psi_v : \mathbb{F}^1 \to V, \;\; \psi_v(\lambda) = v \lambda.
$$<p>When a basis for $V$ is fixed, the map $\psi_v$ is represented by an $n \times 1$ matrix (as an instance of theorem 3.9). This matrix is the familiar column of &ldquo;coordinates&rdquo; of $v$. In particular, observe that scalar multiplication can be seen as matrix multiplication with a single-dimensional vector,</p>
$$
\psi_v(\lambda) = 
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{(\dim V)}
\end{bmatrix}
\begin{bmatrix}
\lambda_1
\end{bmatrix} =
\begin{bmatrix}
\lambda_1 v_1 \\
\lambda_1 v_2 \\
\vdots \\
\lambda_1 v_{(\dim V)}
\end{bmatrix} = 
\lambda_1 v.
$$</div>
</div>
<p>This sets up a clear organization of vectors, matrices, and linear maps. With fixed bases, we see that the set of linear maps from $V$ to $W$ is already a vector space (via 3.8) and also that all vectors in a space $U$ over a field $\mathbb{F}$ are canonically a linear map from $\mathbb{F}^1$ into $U$ (via 3.10), hence</p>
$$
\text{Linear maps} \cong \text{Vectors}.
$$<p>Also, we see that there is a one-to-one correspondence between linear maps and matrices (via 3.9) by the bijection $\mathcal{L}(V, W) \leftrightarrow \mathbb{F}^{(\dim W) \times (\dim V)}$, so in finite dimensions ($\cong^!$)</p>
$$
\text{Matrices} \cong^! \text{Linear maps}.
$$<p>Indeed, a vector can be turned into a matrix solely through its identifiability as a linear map, turning the linear map into the central object of our understanding of linear algebra. In summary,</p>
$$
\begin{equation}
    \text{Matrices} \cong^! \text{Linear maps} \cong \text{Vectors}.
\end{equation}
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.11. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>We have started to use the symbol $\cong$. In this context, stating $A \cong B$ implies that there is a linear <a href="https://en.wikipedia.org/wiki/Isomorphism">isomorphism</a> between $A$ and $B$. Exactly, this means that there exists some linear map $\phi : A \to B$ that is a bijection.</p>
<p>This is complete as a definition of the symbol $\cong$. However, its use in the rest of this piece will often reference a choice of $\phi$ that is <a href="https://en.wikipedia.org/wiki/Canonical_map">canonical</a>. This means that if you see $A \cong B$, there is probably an &ldquo;standard&rdquo; way to obtain a $b \in B$ from one unique $a \in A$ (and vice versa) &ndash; here, we can informally say that if $\phi(a) = b$ then $a$ &ldquo;is&rdquo; $b$, but it is more precise to say that $a$ &ldquo;identifies&rdquo; $b$. Oftentimes, $\phi$ will not be made explicit.</p>
<p>For example, choosing a basis $\mathcal{B}$ for a space $V$ gives the canonical isomorphism $\mathbb{F}^{(\dim V)\times(\dim V)} \cong \mathcal{L}(V)$. In this case, it makes sense to say that &ldquo;a matrix is a linear transformation.&rdquo; But without a choice of basis there is no &ldquo;standard&rdquo; bijection $\Phi_\mathcal{B} : \mathcal{L}(V) \to \mathbb{F}^{(\dim V) \times (\dim V)}$ (no way to bijectively identify maps from matrices).</p>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-untitled-square-1974.png"
         alt="Vera Molnár, Untitled (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, Untitled (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="vector-translation">Vector Translation</h4>
<p>The statement of 3.10 is neuanced. Consider the case of a linear map $T \in \mathcal{L}(V, W)$ represented by a matrix $M$ (under fixed bases). According to 3.8 that map is a vector, but according to 3.10 it is identified by some <em>other</em> linear map $\psi_v$, which is identified by some <em>other</em> column matrix $M^\prime$,</p>
$$
M 
\xrightarrow{\displaystyle\Phi_\mathcal{B}^{-1}} T 
\xrightarrow{\displaystyle\Psi_{\mathcal{L}(V, W)}} \psi_v 
\xrightarrow{\displaystyle\Phi_\mathcal{B}} M^\prime.
$$<p>In this diagram, the basis-conscious bijection $\Phi_\mathcal{B} : \mathcal{L}(V, W) \to \mathbb{F}^{(\dim W) \times (\dim V)}$ lives up to 3.9, but we silently adopted the canonical translation $\Psi_U$ of arbitrary vectors into linear maps in 3.10,</p>
$$
\Psi_U : U \to \mathcal{L}(\mathbb{F}^1, U) \;\; \text{s.t.} \;\; \Psi_U(u) = \psi_u \; \forall \, u \in U.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.12. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>This makes sense when the vector space $U$ is finite-dimensional, as is the case whenever $U = \mathcal{L}(V, W)$ for finite-dimensional $V$ and $W$; the fact that we always interpret finite-dimensional vectors as column matrices is what makes this case of $\Psi_U$ &ldquo;canonical.&rdquo; In other cases where matrix representations make no sense (e.g. the linear map of the Fourier transform $\mathcal{F}$ from 3.6), the choice of $\Psi_U$ will have to be more conscientious.</p>
</div>
</div>
<h4 id="linear-forms">Linear Forms</h4>
<p>In a way that is &ldquo;dual&rdquo; to the statement 3.10, one could look at another representation which, while not as standard as mapping onto linear maps of column-matrix form (and hence non-canonical), can be seen as equally valid. Namely, one could map vectors to linear maps of row-matrix form,</p>
$$
\Psi_U^* : U \to \mathcal{L}(U, \mathbb{F}^1) \;\; \text{s.t.} \;\; \Psi_U^*(u) = \varphi_u \; \forall \, u \in U.
$$<p>That is, for each vector $u \in U$, we can choose to represent it as a linear map $\varphi_u : U \to \mathbb{F}^1$. Back to the line of thought in 3.10, we see that when a basis is fixed, any such $\varphi_u$ can be identified by a $1 \times n$ matrix (again by 3.8). This can be illustrated quite simply for finite dimensions:</p>
$$
\varphi_u(v) = 
\begin{bmatrix}
u_1 \quad
u_2 \quad
\dots \quad
u_{(\dim U)}
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{(\dim U)}
\end{bmatrix} =
\begin{bmatrix}
\sum_{i \, = \, 1}^{\dim U} u_i v_i
\end{bmatrix}.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.13. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Just as stated in 3.12 for $\Psi_U$, motivating the form $U \to \mathbb{F}^1$ from row matrices in the case of vector spaces $U$ of infinite dimension does not always work, as matrix representations sometimes make no sense there. Hence, the concrete choice of $\Psi_U^*$ may require deeper consideration (e.g. in uncountably-infinite dimensions).</p>
</div>
</div>
<h4 id="dual-spaces">Dual Spaces</h4>
<p>All linear maps $\varphi_u : U \to \mathbb{F}^1$ receive the name of <a href="https://en.wikipedia.org/wiki/Linear_form">linear forms</a> on the space $U$. After 3.8, this is no more than the vector space</p>
$$
 U^* = \{ \, \varphi : U \to \mathbb{F}^1 \; | \; \varphi \text{ is linear} \}.
$$<p>This is called the <a href="https://en.wikipedia.org/wiki/Dual_space">dual vector space</a> of $U$, which receives the special notation $U^*$ due to how naturally it arises. Much like elements of $U$ can be represented by column matrices, the elements of $U^*$ (which are called covectors) can be represented by row matrices (wherever matrices make sense).</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.14. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>In any Hilbert space $H$, for every continuous linear form $\varphi \in H^*$, there is a unique vector $u \in H$ with</p>
$$
 \forall \, v \in H, \; \varphi(v) = \langle u, v \rangle.
$$<p>Furthermore, $||u|| = ||\varphi||$. This gives a natural identification between elements of $H$ and $H^*$ by the canonical bijection $J : u \mapsto \varphi$. Here, we say &ldquo;canonical&rdquo; because it is provided uniquely by the inner product. This is the statement of the <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representation theorem</a>.</p>
</div>
</div>
<p>It is not difficult to see that for any vector space $V$ over a field $\mathbb{F}$, in fact, $\Psi_V$ and $\Psi_V^*$ are both bijections. Since they are canonical (in the sense that they are naturally defined), it is only right to assert that</p>
$$
V \cong \mathcal{L}(\mathbb{F}^1, V) \;\; \text{and} \;\; V^* \cong \mathcal{L}(V, \mathbb{F}^1).
$$<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="frigyes-riesz.jpg"
         alt="Frigyes Riesz (January 22, 1880 – February 28, 1956)" width="256"/> <figcaption>
            <p>Frigyes Riesz (January 22, 1880 – February 28, 1956)</p>
        </figcaption>
</figure>

</div>

<h4 id="multilinearity">Multilinearity</h4>
<p>We can continue talking about linearity in maps even when they have multiple arguments. For a map $T$ from multiple vector spaces $V_i$ into another $W$ (all over some field $\mathbb{F}$) where</p>
$$
T : V_1 \times V_2 \times \dots \times V_n \to W \;\; \text{s.t.} \;\; T(v_1, \, v_2, \, \ldots, \, v_n) = w,
$$<p>we say that $T$ is linear in an argument $v_i$ if, for all other arguments $v_j \neq v_i$, fixing $v_j$ makes the altered map $T^\prime : V_i \to W$ linear. If such a map $T$ is linear in all of its $n$ arguments it is called $n$-linear, and all maps like this are called <a href="https://en.wikipedia.org/wiki/Multilinear_map">multilinear maps</a>. The set of multilinear maps of this form is denoted</p>
$$
\mathcal{L}(V_1, \, \ldots, \, V_n; \, W) = \{\, T : V_1 \times \dots \times V_n \to W \; | \; \text{ T is linear} \}.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.15. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Matrix-vector multiplication can be seen as a bilinear ($2$-linear) map,</p>
$$
B : M_{m \times n}(\mathbb{F}) \times \mathbb{F}^n \to \mathbb{F}^m.
$$<p>This is with the understanding that $m$-by-$n$ matrices with entries in $\mathbb{F}$, in other words $M_{m \times n}(\mathbb{F})$, indeed form a vector space over the same field $\mathbb{F}$. One can verify that fixing either argument (the vector or the matrix) forms a linear map with the other argument.</p>
</div>
</div>
<p>We may also extend linear forms with the same treatment that led us to multilinear maps. In particular, if an $n$-linear map over a field $\mathbb{F}$ has the codomain of $\mathbb{F}$ itself, it is called an $n$-linear form (where all maps like this are called <a href="https://en.wikipedia.org/wiki/Multilinear_form">multilinear forms</a>).</p>
<h4 id="tensor-product">Tensor Product</h4>
<p>If one has two vector spaces $V$ and $W$ over the same field, one can naturally talk about their cartesian product $V \times W$ (as we have been doing in the case of multilinear maps). But instead of doing that, one can talk about a third vector space $V \otimes W$ (called the <a href="https://en.wikipedia.org/wiki/Tensor_product">tensor product</a> of $V$ and $W$) which, while having just as much expressiveness as $V \times W$, of course has the added benefit of being a vector space itself.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.16. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let $\varphi : V \times W \to V \otimes W$ be a bilinear map. Then, for each bilinear map $h : V \times W \to Z$ (into another vector space $Z$), there is a unique linear map $\tilde h : V \otimes W \to Z$ such that $h = \tilde h \circ \varphi$. This is referred to as the <a href="https://en.wikipedia.org/wiki/Universal_property">universal property</a> of the tensor product, which justifies the phrase &ldquo;just as much expressiveness.&rdquo;</p>
</div>
</div>
<p>Every tensor product $V \otimes W$ is equipped with such a bilinear map $\varphi : V \times W \to V \otimes W$ that allows the construction of vectors in $V \otimes W$. The <a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a> is an example of this in finite dimensions, but in other cases one must be more creative. Confusingly, this map $\varphi$ is also called a tensor product, and $\otimes$ is predominantly used instead of $\varphi$ in notation. Summarizing,</p>
$$
\forall (v, w) \in V \times W, \; \varphi(v, w) = v \otimes w \quad \text{where} \quad v \otimes w \in V \otimes W.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.17. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Even if the tensor product among vectors is not strictly commutative, there are canonical isomorphisms among permutations of tensor products of vector spaces. That is, for any permutation $\sigma$,</p>
$$
V_1 \otimes \cdots \otimes V_n \;\cong\; V_{\sigma(1)} \otimes \cdots \otimes V_{\sigma(n)}.
$$<p>Due to this symmetry, we often write tensor products as if they were commutative without loss of generality. But when one talks about actual computation or representations, order will probably matter.</p>
</div>
</div>
<p>Totally, through 3.16 and 3.17, the tensor product is precisely designed to &ldquo;linearize&rdquo; multilinear maps. To elaborate, for any multilinear map $h \in \mathcal{L}(V_1, \, \ldots, \, V_n; \, W)$, there exists a unique linear map<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
$$
\tilde h : \bigotimes_i V_i \to W \;\; \text{s.t.} \;\; \tilde h(v_1 \otimes \cdots \otimes v_n) = h(v_1, \ldots, v_n).
$$<h4 id="more-matrices">More Matrices</h4>
<p>Being now able to identify every multilinear map with a unique linear map over a tensor product space, it is possible to assert that 3.8, 3.9, and 3.10 also apply to tensor product spaces. I will reiterate the notes, dressing them up specifically for the case of tensor product spaces.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.18. Specialization of 3.8</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The set of linear maps from a tensor product space $\bigotimes_i V_i$ over the field $\mathbb{F}$ to another vector space $W$ over $\mathbb{F}$ forms a vector space over $\mathbb{F}$. Symbolically,</p>
$$
\mathcal{L}({\textstyle\bigotimes}_i V_i, W) = \left\{ \, T : \bigotimes_i V_i \to W \;\; \bigg| \;\; T \text{ is linear}  \right\}
$$<p>is a vector space over $\mathbb{F}$. We denote the operator case as $\mathcal{L}(\bigotimes_i V_i) = \mathcal{L}(\bigotimes_i V_i, \, \bigotimes_i V_i)$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.19. Specialization of 3.9</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>There is a bijection between $\mathcal{L}(\bigotimes_i V_i, W)$ and $\mathbb{F}^{\times_i (\dim V_i)} \times \mathbb{F}^{(\dim W)}$ when $V$ and $W$ are finite-dimensional vector spaces over $\mathbb{F}$. That is, for each linear map from a tensor product over spaces $V_1, \, \ldots, \, V_n$ and into $W$ (all over a field $\mathbb{F}$), there is one matrix with axis lengths $(\dim V_1, \, \ldots, \, \dim V_n, \, \dim W)$ with entries in $\mathbb{F}$.</p>
</div>
</div>
<p>Often, $(\dim V_1, \, \ldots, \, \dim V_n, \, \dim W)$ from 3.19 is referred to as the shape of the matrix. Each entry of the shape tuple can be viewed as the sidelength of a pictographical embedding of the matrix in $\mathbb{R}^{n + 1}$. For example, the matrix in $\mathbb{R}^{2 \times 3}$</p>
$$
M = 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}
$$<p>is said to have shape $(2, 3)$ &ndash; once it is &ldquo;drawn on paper,&rdquo; its &ldquo;sidelengths&rdquo; are $2$ and $3$. Keeping the spirit of &ldquo;pictographical&rdquo; representation, people often call this a $2$-dimensional array, as it can be neatly &ldquo;drawn&rdquo; on two dimensions. But concretely, this matrix corresponds to a $6$-dimensional linear map. This ambiguity is often resolved by calling each shape entry an &ldquo;axis&rdquo; instead of a dimension, understanding that it refers to the visual axis of $\mathbb{R}^n$ where we would pictographically embed it.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.20. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Software libraries that represent matrices often make the choice of calling them either $n$-dimensional arrays or simply tensors in an effort to maintain generality. Arguably, these terms are both misnomers. Personally, I think they should have just called them all matrices.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.21. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Appending a trailing \(1\) to the shape of a matrix does not change the underlying object. Concretely, a matrix of shape \((\alpha_1, \, \ldots, \, \alpha_n)\) can be naturally identified with one of shape \((\beta_1, \, \ldots, \, \beta_n)\) when $\prod_i \alpha_i = \prod_i \beta_i$. This is a result of the isomorphisms included in the scope of</p>
$$
\prod_i \dim \Alpha_i = \prod_i \dim \Beta_i \iff \bigotimes_i \Alpha_i \;\cong\; \bigotimes_i \Beta_i \, .
$$</div>
</div>
<p>Some bijections of the form $f : \bigotimes_i \Alpha_i \to \bigotimes_i \Beta_i$ are often referred to as &ldquo;reshapings.&rdquo; This concept is hard to formalize, but pictographically graspable. For example, this is one way to reshape $M$:</p>
$$
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}
\xrightarrow{f}
\begin{bmatrix}
1 & 2 & 3 & 4 & 5 & 6 \\
\end{bmatrix}
\xrightarrow{g}
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
\end{bmatrix}.
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.22. Specialization of 3.10</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Every vector $v$ in a tensor product space $\bigotimes_i V_i$ can be seen as a linear map from the sapce $\mathbb{F}^1$ into $\bigotimes_i V_i$ through the definition</p>
$$
\psi_v : \mathbb{F}^1 \to \bigotimes_i V_i \, , \;\; \psi_v(\lambda) = v \lambda.
$$<p>With a basis for $\bigotimes_i V_i$ fixed, the map $\psi_v$ can be represented by a matrix of shape $(1, \, \bigotimes_i V_i)$ (as an instance of note 3.19). However, in the context of tensor product spaces, it is common to represent $\psi_v$ using a matrix of shape $(\dim V_1, \, \ldots, \, \dim V_n)$ (invoking 3.21). This is done to facilitate descriptions of computations involving the vector in question.</p>
</div>
</div>
<h4 id="homogeneous-tensors">Homogeneous Tensors</h4>
<p>Many people refer to vectors in tensor product spaces as tensors, especially in computationally-oriented scientific disciplines. This population has recently gained numerosity (and maybe even majority) thanks to the increasing availability of efficient computers and their applications. But traditionally, a <a href="https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)">tensor</a> is a linear map associated with a single vector space $V$ over $\mathbb{F}$ of the form</p>
$$
\begin{equation}
 T_{n}^{\, m} : (\times^m \, V^*) \times (\times^n \, V) \to \mathbb{F}.
\end{equation}
$$<p>Here, $(m, \, n)$ is called the &ldquo;type&rdquo; of the tensor $T$. This makes a map like $m : \mathbb{R}^2 \times \mathbb{R}^4 \to \mathbb{R}$ strictly not interpretable as a tensor, as $m$ itself is not of tensor form, and there is no standard bijection that can help us view it as a tensor. That is, we cannot always identify a tensor with a multilinear map.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.23. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>The tensor space of type $(m, \, n)$ defined over a vector space $V$ is denoted<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
$$
T_{n}^{\, m}(V) = \mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, \mathbb{F}).
$$</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.24. Examples</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let us observe the types of some known tensors of form $T_{n}^{, 0} : (\times^n \, V) \to \mathbb{F}$.</p>
<ol>
<li>Any scalar $\lambda : a \mapsto \lambda a$ is a tensor of type $(0, \, 0)$.</li>
<li>Any linear form $\varphi : v \mapsto \varphi(v)$ is a tensor of type $(0, \, 1)$.</li>
<li>Any quadratic form $q : (v, w) \mapsto v^\top A w$ is a tensor of type $(0, \, 2)$.</li>
<li>Any inner product $\langle \cdot, \cdot \rangle : (v, w) \mapsto \mathbb{F}$ is a tensor of type $(0, \, 2)$.</li>
</ol>
</div>
</div>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-structures-of-squares-1974.png"
         alt="Vera Molnár, &lsquo;Structures of Squares&rsquo; (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, &lsquo;Structures of Squares&rsquo; (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="tensor-identification">Tensor Identification</h4>
<p>When the definition of a multilinear map involves many vector spaces, there will continue to be a lack of such a canonical isomorphism. But if we consider arbitrary multilinear maps defined over a single vector space $V$ over a field $\mathbb{F}$ (even those without a codomain $\mathbb{F}$ ), we will see that canonically</p>
$$
\begin{equation}
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* ))
\cong
\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right),
\end{equation}
$$<p>where we always interpret $\times^0 \, V = \mathbb{F}$ and $\otimes^0 \, V = \mathbb{F}^1$ (as always, with $\mathbb{F} \cong \mathbb{F}^1$). This essentially claims we can uniquely identify vector-valued multilinear maps defined over a single vector space $V$ with vectors in a tensor product space defined only over $V$.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.25. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>As a result of 3.16, for all vector spaces $V$, $W$, and $Z$,</p>
$$
\mathcal{L}(V \otimes W, \, Z) \cong\ \mathcal{L}(V, \, W; \, Z).
$$</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.26. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Any finite-dimensional vector space is canonically isomorphic to its own dual due the standard bijection</p>
$$
\Phi : V \to V^* \;\; \text{s.t.} \;\; \Phi[v](w) = \langle v, w \rangle,
$$<p>where $u, w \in V$ (when $V$ is not an inner-product space one defaults to the standard dot product). However, many infinite-dimensional inner-product spaces have duals which cannot be spanned using this strategy. The exception is Hilbert spaces, where 3.14 provides the bijection $J$ (still through the inner product).</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.27. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>For any Hilbert space or finite-dimensional vector space $V$, we have that $V^* \otimes V^* \cong (V \otimes V)^*$ canonically. This is supported by the following standard choice of bijection</p>
$$
\Phi : V^* \otimes V^* \to (V \otimes V)^* \;\; \text{s.t.} \;\; \Phi[\psi \otimes \varphi](v \otimes w) \mapsto \psi(v) \varphi(w),
$$<p>where $\psi, \varphi \in V^*$ and $v, w \in V$. To be clear, $\Phi[\psi \otimes \varphi]$ is in $(V \otimes V)^*$ and has the form $V \otimes V \to \mathbb{F}$.</p>
</div>
</div>
<p>To show $(9)$, our strategy will be to consider an arbitrary multilinear map $T$ in the set</p>
$$
\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V) = 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* )).
$$<p>We will introduce a bijection $\tilde \Gamma$ that identifies a unique tensor of type $(m + a, \, n + b)$ for each $T$. Then, we will introduce another bijection $\hat \Gamma$ to show that $T_{n + b}^{\, m + a}(V)$ is isomorphic to $\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right)$. Summarizing, we will construct an invertible trip</p>
$$
\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V) 
\xrightarrow{\displaystyle \tilde \Gamma} 
T_{n + b}^{\, m + a}(V) 
\xrightarrow{\displaystyle \hat \Gamma} 
\left( \otimes^{m + a} \, V \right) \otimes \left( \otimes^{n + b} \, V^* \right).
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.28. Demonstration</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let us first consider $\tilde \Gamma$. Given a $T \in \mathcal{L}_{(n, \, m)}^{\, (a, \, b)}(V)$, define $\tilde T^\prime$ where</p>
$$
\begin{align*}
\tilde T^\prime : \; & (\times^m \, V^*) \times (\times^n \, V) \times (\left( \otimes^a \, V \right) \otimes \left( \otimes^b \, V^* \right))^* \to \mathbb{F} \\
& \text{s.t.} \;\; \tilde T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n,  \, \varphi) = \varphi(T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n)),
\end{align*}
$$<p>where $v_i \in V^*$, $w_i \in V$, and $\varphi \in (\left( \otimes^c \, V \right) \otimes \left( \otimes^d \, V^* \right))^*$ is the covector of $T(v_1, \, \ldots, \, v_m, \, w_1, \, \ldots, \, w_n)$ (which can be canonically determined by 3.14). Then, observe that</p>
$$
\begin{align*}
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, & \,\, V_{(1)}, \, \dots, \, V_{(n)}, \, (( \otimes^a \, V ) \otimes ( \otimes^b \, V^* ))^*; \, \mathbb{F}) \\
&\overset{\text{3.27}}{\cong} 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \,\, V_{(1)}, \, \dots, \, V_{(n)}, \, ( \otimes^a \, V^* ) \otimes ( \otimes^b \, V ); \, \mathbb{F}) \\
&\overset{\text{3.25}}{\cong} 
\mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m + a)}^*, \,\, V_{(1)}, \, \dots, \, V_{(n + b)}; \, \mathbb{F}).
\end{align*}
$$<p>Using this, a map $\tilde T$ can be uniquely constructed from $\tilde T^\prime$ (invoking 3.25 and 3.17 for argument order), where</p>
$$
\tilde T : (\times^{m + a} \, V^*) \times (\times^{n + b} \, V)\to \mathbb{F}.
$$<p>This finalizes the definition of $\tilde \Gamma : T \mapsto \tilde T$. Each step of this process is bijective, therefore $\tilde \Gamma$ is bijective.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.29. Demonstration</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Now we can consider $\hat \Gamma$. Define the tensor $T \in T_{n + b}^{, m + a}(V)$ such that by definition</p>
$$
T : (\times^{m + a} \, V^*) \times (\times^{n + b} \, V) \to \mathbb{F}.
$$<p>By repeated application of 3.25, we can always use $T$ to construct a unique</p>
$$
\hat T^\prime : (\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V) \to \mathbb{F}.
$$<p>This is clearly a linear form. In other words, $\hat T^\prime \in ((\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V))^*$. But by 3.27, there is a unique</p>
$$
\hat T \in (\otimes^{m + a} \, V^*) \otimes (\otimes^{n + b} \, V)
$$<p>for each $\hat T^\prime$ we could construct. This finalizes the definition of $\hat \Gamma : T \mapsto \hat T$. Each step above is bijective, so $\hat \Gamma$ is itself a bijection.</p>
</div>
</div>
<p>We have shown that vector-valued multilinear maps defined on a single vector space (such as operators) do identify tensors uniquely, despite not being of tensor form. Also, we have shown how tensors uniquely identify elements of tensor product spaces defined on a single vector space. This is why it is normal refer to all of these objects as tensors.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.30. Examples</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Equation $(9)$ provides a formula for identifying the tensor type of elements in $\mathcal{L}_{(m, \, n)}^{\, (a, \, b)}(V)$. Indeed, tensor type acts like an algebraic signature for these objects.</p>
<ol>
<li>Linear operators $T : V \to V$ identify type $(1, 1)$ tensors.</li>
<li>The tensor product $\otimes : V \times V \to V \otimes V$ identifies a type $(2, 2)$ tensor.</li>
</ol>
<p>For linear operators (represented by square matrices), we can apply $(9)$ and observe they identify elements of $V \otimes V^*$ as they are of type $(1, 1)$. Then, square matrix multiplication represents the map</p>
$$
T : (V \otimes V^*) \times (V \otimes V^*) \to (V \otimes V^*) \;\; \text{s.t.} \;\; T(A, \, B) = AB,
$$<p>where $A, B \in \mathcal{L}(V)$. But due to 3.25, we can reform $T : (\times^2 \, V^*) \times (\times^2 \, V) \to V \otimes V^*$. Applying $(9)$ one more time, we observe that this is a type $(3, 3)$ tensor. This illustrates how the tensor type of a multilinear map defined over other tensor product spaces can be computed.</p>
</div>
</div>
<h4 id="heterogeneous-tensors">Heterogeneous Tensors</h4>
<p>We have used algebraic pathways to extend the idea of a tensor beyond scalar-valued multilinear maps. Our primary tool was the canonical isomorphism, which allowed us to ignore many specifics in all cases. Now, we will widen the concept of a tensor to involve many vector spaces with linear maps of form</p>
$$
\begin{equation}
T_{B}^{\, A} : (\times_A \, V_i^*) \times (\times_B \, V_i) \to \mathbb{F}.
\end{equation}
$$<p>Here, we reference two indexed collections of vector spaces (over the same field $\mathbb{F}$), $\langle V_i \rangle_A$ and $\langle V_i \rangle_B$. We differentiate these tensors by calling them heterogeneous, since they involve different vector spaces. They share much theory with the homogeneous case, resulting in the isomorphism</p>
$$
\begin{equation}
\mathcal{L}(\langle V_i^* \rangle_{A}, \, \langle V_i \rangle_{B}; \, ( \otimes_{C} \, V_i^* ) \otimes ( \otimes_{D} \, V_i ))
\cong
\left( \otimes_{A \cup D} \, V_i \right) \otimes \left( \otimes_{B \cup C} \, V_i^* \right).
\end{equation}
$$<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.31. Explanation</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>For the sake of brevity, this will be hand-wavy. The main insight is to quotient $\mathcal{V} = A \cup B \cup C \cup D$ by simple equality. For each partition in $\mathcal{V}_k \in (\mathcal{V}/=)$ of vector spaces associated with a map</p>
$$
T \in \mathcal{L}(\langle V_i^* \rangle_{A}, \, \langle V_i \rangle_{B}; \, ( \otimes_{C} \, V_i ) \otimes ( \otimes_{D} \, V_i )),
$$<p>fix an argument for all other vector spaces $\mathcal{V} \setminus \mathcal{V}_k$, creating a new multilinear map</p>
$$
\tilde T_k \in \mathcal{L}(V_{(1)}^*, \, \dots, \, V_{(m)}^*, \, V_{(1)}, \, \dots, \, V_{(n)}; \, ( \otimes^a \, V ) \otimes ( \otimes^b \, V^* )),
$$<p>where $\mathcal{V}_k = [V]$ in $(\mathcal{V} / =)$ and $|\mathcal{V}_k| = a + b + m + n$ (skipping additional bookeeping). By 3.28 and 3.29, $\tilde T_k$ identifies a homogeneous tensor. Since all $\tilde T_k$ can be uniquely transformed this way, we can define a new $\bar T$ that has the cartesian product of their domains as its own domain with codomain $\mathbb{F}$ (by multilinearity). Finally, 3.25 gives the desired form (the RHS of $(11)$). These steps were through isomorphisms, hence $(11)$.</p>
</div>
</div>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.32. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Much like $(9)$ relies on finite-dimensionality of the vector space in question for homogeneous tensors (or that it be a Hilbert space), the same restriction is needed in $(11)$ for all vector spaces in heterogeneous tensor forms.</p>
</div>
</div>
<p>It is still possible to consider tensor type in the heterogeneous case &ndash; one simply has to keep track of one type tuple per $[V] \in (\mathcal{V}/=)$ (see 3.31). If there are $K$ such equivalence classes, a tensor type may be</p>
$$
(m_1, \, \ldots, \, m_K, \, n_1, \, \ldots, \, n_K).
$$<p>With heterogeneous tensors, one must also carry a mapping of type index to corresponding vector space. Without this, we would not know which vector space each type tuple $(m_i, \, n_i)$ corresponds to. So when it is not clear from context, we simply say that the type is $(A, \, B)$ as implied from the syntax of $(10)$.</p>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="vera-molnar-untitled-1974.png"
         alt="Vera Molnár, Untitled (1974)" width="512"/> <figcaption>
            <p>Vera Molnár, Untitled (1974)</p>
        </figcaption>
</figure>

</div>

<h4 id="tensor-contractions">Tensor Contractions</h4>
<p>The statements of $(9)$ and $(11)$ may initially seem like a cryptic justification of our choice of vocabulary; they justify why we use the word &ldquo;tensor&rdquo; so liberally, with the most general use being in reference to an element of a heterogeneous tensor product space (up to isomorphism).</p>
<p>But beyond justifying use of language, $(9)$ and $(11)$ also provide a clear perspective on computation with tensors. They imply that all tensors can be &ldquo;used&rdquo; both as vectors and as multilinear maps &ndash; they are both multi-argument functions and possible inputs to other multi-argument functions. To better understand this, we will take a look at <a href="https://en.wikipedia.org/wiki/Partial_application">partial application</a> in this context.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.33. Example</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Consider the quadratic form $q : (v, w) \mapsto v^\top A w$, which from 3.24 is a (homogeneous) tensor of type $(0, 2)$. It is a multilinear map of the form $q : V \times V \to \mathbb{F}$. If we fix the argument $v$, we can obtain $\hat q : w \mapsto v^\top A w$, which is a $1$-linear map of form $\hat q : V \to \mathbb{F}$ and a tensor of type $(0, 1)$.</p>
</div>
</div>
<p>In this example, we combined a multilinear map and a vector to obtain another multilinear map via partial application. Taking note that all the objects involved in this process are tensors, we can study how partial application is related to the type of the tensors involved.</p>
<style>
    .box-body > :last-child {
        margin-bottom: 0 !important;
    }

    .box-body > :first-child {
        margin-top: 0 !important;
    }
</style>
<div
    class="hint-box"
    style="
        border: 1px solid #000000;
        padding: 10px;
        border-radius: 5px;
        margin: 25px 0;
        background-color: rgba(0, 0, 0, 0.05);
    "
>
    <strong style="display: block; margin-bottom: 5px"
        >3.34. Note</strong
    >
    <hr
        style="
            border: none;
            border-top: 1px solid #000000;
            margin: 10px 0;
            width: calc(100%);
        "
    />
    <div style="font-size: 0.92em" class="box-body">
<p>Let $T$ be a homogeneous tensor of type $(m, n)$ on a vector space $V$. Partial application of $k$ of its arguments in $V$ and $h$ of its arguments in $V^*$ will result in a new tensor $\hat T$ of type $(m - h, \, n - k)$. Further, observe that by 3.25 one can construct a unique bilinear form $\tilde T$ from $T$ where an equivalent partial application can be done in a single argument, such that for a unique $z \in (\otimes^h \, V^*) \otimes (\otimes^k \, V)$,</p>
$$
\begin{align*}
\tilde T : (\otimes^{m - h} \, V^*) \otimes (\otimes^{n - k} \, V) & \times (\otimes^h \, V^*) \otimes (\otimes^k \, V) \to \mathbb{F} \;\; \text{s.t.}\\
 \;\; \hat T(v_1, \, \ldots, \, v_{m - h}, \, w_1, \, \ldots, \, w_{n - k}) &= \tilde T(v_1 \otimes \ldots \otimes v_{m - h} \otimes w_1 \otimes \ldots \otimes w_{n - k}, \, z).
\end{align*}
$$<p>Above, $z$ is exactly the tensor product of the vectors that were used as arguments during partial application on$T$ in order to obtain $\hat T$. Note that $z$, by statement $(9)$, identifies another tensor of type $(h, k)$.</p>
</div>
</div>
<p>The note above explains why (in the homogeneus case) partial application of multiple tensor arguments is in fact partial application of another tensor as an arguent on a uniquely associated bilinear map. This view shows how natural it is to think of partial application as a process that transforms two tensors into a third.</p>
<ol start="0">
<li>Tensor contraction</li>
<li>Einstein notation (mention einsum)</li>
<li>Penrose diagrams</li>
</ol>
<style>
    .halign-container {
        display: flex;
        width: 100%;
        justify-content: center;  
    }
</style>
<div class="halign-container">
<figure>
    <img loading="lazy" src="roger-penrose.png"
         alt="Sir Roger Penrose (born August 8, 1931)" width="256"/> <figcaption>
            <p>Sir Roger Penrose (born August 8, 1931)</p>
        </figcaption>
</figure>

</div>

<h4 id="overview-2">Overview</h4>
<h3 id="signals-and-systems">Signals and Systems</h3>
<h3 id="kernel-methods">Kernel Methods</h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>When considering infinite-dimensional vector spaces, this statement is true if and only if one admits the axiom of choice. Perhaps this was another motivation of Axler&rsquo;s restriction to finite-dimensional vector spaces.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Non-parenthesized sub-indices imply that the item is part of an indexed set &ndash; $a_i$ may not be equal to $a_j$.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Parenthesized sub-indices are only used to indicate argument index. That is, $a_{(i)} = a_{(j)}$ for all $i$ and $j$.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</article>

        </main><footer id="footer">
    Copyright © 2024 Max Fierro
</footer>
</body>
</html>
